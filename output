2018-04-26 09:38:29 hwc hwc[3075] DEBUG Checking section `cluster/slurm` ...
2018-04-26 09:38:29 hwc hwc[3075] DEBUG Checking section `login/centos` ...
2018-04-26 09:38:29 hwc hwc[3075] DEBUG Checking section `setup/ansible-slurm` ...
2018-04-26 09:38:29 hwc hwc[3075] DEBUG Checking section `cloud/catalyst` ...
2018-04-26 09:38:29 hwc hwc[3075] DEBUG Using class <class 'elasticluster.providers.openstack.OpenStackCloudProvider'> from module <module 'elasticluster.providers.openstack' from '/usr/local/lib/python2.7/site-packages/elasticluster-1.3.dev1-py2.7.egg/elasticluster/providers/openstack.pyc'> to instanciate provider 'openstack'
2018-04-26 09:38:29 hwc hwc[3075] DEBUG OpenStack auth URL taken from env variable OS_AUTH_URL
2018-04-26 09:38:29 hwc hwc[3075] DEBUG OpenStack user name taken from env variable OS_USERNAME
2018-04-26 09:38:29 hwc hwc[3075] DEBUG OpenStack user domain name taken from env variable OS_USER_DOMAIN_NAME
2018-04-26 09:38:29 hwc hwc[3075] DEBUG OpenStack password taken from env variable OS_PASSWORD
2018-04-26 09:38:29 hwc hwc[3075] DEBUG OpenStack project name taken from env variable OS_PROJECT_NAME
2018-04-26 09:38:29 hwc hwc[3075] DEBUG OpenStack project domain name taken from env variable OS_PROJECT_DOMAIN_NAME
2018-04-26 09:38:29 hwc hwc[3075] DEBUG Using class <class 'elasticluster.providers.ansible_provider.AnsibleSetupProvider'> from module <module 'elasticluster.providers.ansible_provider' from '/usr/local/lib/python2.7/site-packages/elasticluster-1.3.dev1-py2.7.egg/elasticluster/providers/ansible_provider.pyc'> to instanciate provider 'ansible'
2018-04-26 09:38:29 hwc hwc[3075] DEBUG setting variable user_obs_url=http://obs.cn-north-1.myhwclouds.com for node kind compute
2018-04-26 09:38:29 hwc hwc[3075] DEBUG setting variable upgrade_packages=no for node kind compute
2018-04-26 09:38:29 hwc hwc[3075] DEBUG setting variable user_obs_endpoint=cn-north-1 for node kind compute
2018-04-26 09:38:29 hwc hwc[3075] DEBUG setting variable user_obs_name=hwc-obs for node kind compute
2018-04-26 09:38:29 hwc hwc[3075] DEBUG setting variable user_client_ip=192.168.0.188 for node kind compute
2018-04-26 09:38:29 hwc hwc[3075] DEBUG setting variable user_sfs_url=sfs-nas1.cn-north-1.myhuaweicloud.com:/share-480fca1b for node kind compute
2018-04-26 09:38:29 hwc hwc[3075] DEBUG setting variable user_ak=CNYVGDKK2PPBQEEMUYQG for node kind compute
2018-04-26 09:38:29 hwc hwc[3075] DEBUG setting variable user_sk=6imAmmCVVzMqR1dm5x0AKJcL1BILA7TlOsSMhu4G for node kind compute
2018-04-26 09:38:29 hwc hwc[3075] DEBUG setting variable user_obs_url=http://obs.cn-north-1.myhwclouds.com for node kind frontend
2018-04-26 09:38:29 hwc hwc[3075] DEBUG setting variable upgrade_packages=no for node kind frontend
2018-04-26 09:38:29 hwc hwc[3075] DEBUG setting variable user_obs_endpoint=cn-north-1 for node kind frontend
2018-04-26 09:38:29 hwc hwc[3075] DEBUG setting variable user_obs_name=hwc-obs for node kind frontend
2018-04-26 09:38:29 hwc hwc[3075] DEBUG setting variable user_client_ip=192.168.0.188 for node kind frontend
2018-04-26 09:38:29 hwc hwc[3075] DEBUG setting variable user_sfs_url=sfs-nas1.cn-north-1.myhuaweicloud.com:/share-480fca1b for node kind frontend
2018-04-26 09:38:29 hwc hwc[3075] DEBUG setting variable user_ak=CNYVGDKK2PPBQEEMUYQG for node kind frontend
2018-04-26 09:38:29 hwc hwc[3075] DEBUG setting variable user_sk=6imAmmCVVzMqR1dm5x0AKJcL1BILA7TlOsSMhu4G for node kind frontend
2018-04-26 09:38:29 hwc hwc[3075] INFO Starting cluster nodes (timeout: 600 seconds) ...
2018-04-26 09:38:29 hwc hwc[3075] DEBUG Note: starting 3 nodes concurrently.
2018-04-26 09:38:29 hwc hwc[3075] DEBUG _start_node: working on node `frontend001`
2018-04-26 09:38:29 hwc hwc[3075] INFO Starting node `frontend001` from image `befbd7ca-dd7a-4737-bc60-33a1cdf6ea8b` with flavor c1.medium ...
2018-04-26 09:38:29 hwc hwc[3075] DEBUG Initializing OpenStack API clients: OS_AUTH_URL='https://iam.cn-north-1.myhwclouds.com/v3' OS_USERNAME='mschyj' OS_USER_DOMAIN_NAME='mschyj' OS_PROJECT_NAME='cn-north-1' OS_PROJECT_DOMAIN_NAME='mschyj' OS_REGION_NAME='cn-north-1'
2018-04-26 09:38:29 hwc hwc[3075] INFO Using Keystone API v3 session to authenticate to OpenStack
2018-04-26 09:38:29 hwc hwc[3075] DEBUG Creating OpenStack Compute API (Nova) v2 client ...
2018-04-26 09:38:29 hwc hwc[3075] DEBUG _start_node: working on node `compute001`
2018-04-26 09:38:29 hwc hwc[3075] INFO Starting node `compute001` from image `befbd7ca-dd7a-4737-bc60-33a1cdf6ea8b` with flavor c1.medium ...
2018-04-26 09:38:29 hwc hwc[3075] DEBUG Initializing OpenStack API clients: OS_AUTH_URL='https://iam.cn-north-1.myhwclouds.com/v3' OS_USERNAME='mschyj' OS_USER_DOMAIN_NAME='mschyj' OS_PROJECT_NAME='cn-north-1' OS_PROJECT_DOMAIN_NAME='mschyj' OS_REGION_NAME='cn-north-1'
2018-04-26 09:38:29 hwc hwc[3075] DEBUG _start_node: working on node `compute002`
2018-04-26 09:38:29 hwc hwc[3075] INFO Starting node `compute002` from image `befbd7ca-dd7a-4737-bc60-33a1cdf6ea8b` with flavor c1.medium ...
2018-04-26 09:38:29 hwc hwc[3075] DEBUG Initializing OpenStack API clients: OS_AUTH_URL='https://iam.cn-north-1.myhwclouds.com/v3' OS_USERNAME='mschyj' OS_USER_DOMAIN_NAME='mschyj' OS_PROJECT_NAME='cn-north-1' OS_PROJECT_DOMAIN_NAME='mschyj' OS_REGION_NAME='cn-north-1'
2018-04-26 09:38:29 hwc hwc[3075] INFO Using Keystone API v3 session to authenticate to OpenStack
2018-04-26 09:38:29 hwc hwc[3075] DEBUG Creating OpenStack Compute API (Nova) v2 client ...
2018-04-26 09:38:29 hwc hwc[3075] DEBUG Creating OpenStack Network API (Neutron) client ...
2018-04-26 09:38:29 hwc hwc[3075] INFO Using Keystone API v3 session to authenticate to OpenStack
2018-04-26 09:38:29 hwc hwc[3075] DEBUG Creating OpenStack Compute API (Nova) v2 client ...
2018-04-26 09:38:29 hwc hwc[3075] DEBUG Creating OpenStack Network API (Neutron) client ...
2018-04-26 09:38:29 hwc hwc[3075] DEBUG Creating OpenStack Network API (Neutron) client ...
2018-04-26 09:38:29 hwc hwc[3075] DEBUG Creating OpenStack Image API (Glance) v2 client ...
2018-04-26 09:38:29 hwc hwc[3075] DEBUG Creating OpenStack Image API (Glance) v2 client ...
2018-04-26 09:38:29 hwc hwc[3075] DEBUG Creating OpenStack Image API (Glance) v2 client ...
2018-04-26 09:38:29 hwc hwc[3075] DEBUG Creating OpenStack Volume API (Cinder) v2 client ...
2018-04-26 09:38:29 hwc hwc[3075] DEBUG Creating OpenStack Volume API (Cinder) v2 client ...
2018-04-26 09:38:29 hwc hwc[3075] DEBUG Creating OpenStack Volume API (Cinder) v2 client ...
2018-04-26 09:38:29 hwc hwc[3075] DEBUG Checking keypair `hwc-key` ...
2018-04-26 09:38:29 hwc hwc[3075] DEBUG Checking keypair `hwc-key` ...
2018-04-26 09:38:29 hwc hwc[3075] DEBUG Checking keypair `hwc-key` ...
2018-04-26 09:38:30 hwc hwc[3075] DEBUG Checking existence of security group(s) ['security-mschyj'] ...
2018-04-26 09:38:30 hwc hwc[3075] DEBUG Checking existence of security group(s) ['security-mschyj'] ...
2018-04-26 09:38:30 hwc hwc[3075] DEBUG Checking existence of security group(s) ['security-mschyj'] ...
2018-04-26 09:38:42 hwc hwc[3075] DEBUG Specifying networks for node slurm-compute002: 6c7d662f-eb15-4a44-b2ef-2e71ff7fce15
2018-04-26 09:38:42 hwc hwc[3075] DEBUG Specifying networks for node slurm-frontend001: 6c7d662f-eb15-4a44-b2ef-2e71ff7fce15
2018-04-26 09:38:44 hwc hwc[3075] DEBUG Specifying networks for node slurm-compute001: 6c7d662f-eb15-4a44-b2ef-2e71ff7fce15
2018-04-26 09:38:48 hwc hwc[3075] DEBUG Node `frontend001` has instance ID `b34c6343-8d7b-4f99-a869-8aa4dd4df58e`
2018-04-26 09:38:48 hwc hwc[3075] INFO Node `frontend001` has been started.
2018-04-26 09:38:49 hwc hwc[3075] DEBUG Node `compute002` has instance ID `d9aa5ef4-b4cc-45d2-8dfd-845ef432dc78`
2018-04-26 09:38:49 hwc hwc[3075] INFO Node `compute002` has been started.
2018-04-26 09:38:49 hwc hwc[3075] DEBUG Node `compute001` has instance ID `af500973-2935-48a8-b5b9-76fca460134c`
2018-04-26 09:38:49 hwc hwc[3075] INFO Node `compute001` has been started.
2018-04-26 09:38:49 hwc hwc[3075] DEBUG Getting information for instance d9aa5ef4-b4cc-45d2-8dfd-845ef432dc78
2018-04-26 09:38:50 hwc hwc[3075] DEBUG node `compute002` (instance id `d9aa5ef4-b4cc-45d2-8dfd-845ef432dc78`) still building...
2018-04-26 09:38:50 hwc hwc[3075] DEBUG Getting information for instance b34c6343-8d7b-4f99-a869-8aa4dd4df58e
2018-04-26 09:38:51 hwc hwc[3075] DEBUG node `frontend001` (instance id `b34c6343-8d7b-4f99-a869-8aa4dd4df58e`) still building...
2018-04-26 09:38:51 hwc hwc[3075] DEBUG Getting information for instance af500973-2935-48a8-b5b9-76fca460134c
2018-04-26 09:38:52 hwc hwc[3075] DEBUG node `compute001` (instance id `af500973-2935-48a8-b5b9-76fca460134c`) still building...
2018-04-26 09:38:52 hwc hwc[3075] DEBUG Waiting for 3 more nodes to come up ...
2018-04-26 09:39:02 hwc hwc[3075] DEBUG Getting information for instance d9aa5ef4-b4cc-45d2-8dfd-845ef432dc78
2018-04-26 09:39:03 hwc hwc[3075] DEBUG node `compute002` (instance id `d9aa5ef4-b4cc-45d2-8dfd-845ef432dc78`) still building...
2018-04-26 09:39:03 hwc hwc[3075] DEBUG Getting information for instance b34c6343-8d7b-4f99-a869-8aa4dd4df58e
2018-04-26 09:39:03 hwc hwc[3075] DEBUG node `frontend001` (instance id `b34c6343-8d7b-4f99-a869-8aa4dd4df58e`) still building...
2018-04-26 09:39:03 hwc hwc[3075] DEBUG Getting information for instance af500973-2935-48a8-b5b9-76fca460134c
2018-04-26 09:39:04 hwc hwc[3075] DEBUG node `compute001` (instance id `af500973-2935-48a8-b5b9-76fca460134c`) still building...
2018-04-26 09:39:04 hwc hwc[3075] DEBUG Waiting for 3 more nodes to come up ...
2018-04-26 09:39:14 hwc hwc[3075] DEBUG Getting information for instance d9aa5ef4-b4cc-45d2-8dfd-845ef432dc78
2018-04-26 09:39:15 hwc hwc[3075] DEBUG node `compute002` (instance id `d9aa5ef4-b4cc-45d2-8dfd-845ef432dc78`) still building...
2018-04-26 09:39:15 hwc hwc[3075] DEBUG Getting information for instance b34c6343-8d7b-4f99-a869-8aa4dd4df58e
2018-04-26 09:39:15 hwc hwc[3075] DEBUG node `frontend001` (instance id `b34c6343-8d7b-4f99-a869-8aa4dd4df58e`) still building...
2018-04-26 09:39:15 hwc hwc[3075] DEBUG Getting information for instance af500973-2935-48a8-b5b9-76fca460134c
2018-04-26 09:39:16 hwc hwc[3075] DEBUG node `compute001` (instance id `af500973-2935-48a8-b5b9-76fca460134c`) still building...
2018-04-26 09:39:16 hwc hwc[3075] DEBUG Waiting for 3 more nodes to come up ...
2018-04-26 09:39:26 hwc hwc[3075] DEBUG Getting information for instance d9aa5ef4-b4cc-45d2-8dfd-845ef432dc78
2018-04-26 09:39:27 hwc hwc[3075] DEBUG node `compute002` (instance id `d9aa5ef4-b4cc-45d2-8dfd-845ef432dc78`) still building...
2018-04-26 09:39:27 hwc hwc[3075] DEBUG Getting information for instance b34c6343-8d7b-4f99-a869-8aa4dd4df58e
2018-04-26 09:39:28 hwc hwc[3075] DEBUG node `frontend001` (instance id `b34c6343-8d7b-4f99-a869-8aa4dd4df58e`) still building...
2018-04-26 09:39:28 hwc hwc[3075] DEBUG Getting information for instance af500973-2935-48a8-b5b9-76fca460134c
2018-04-26 09:39:28 hwc hwc[3075] DEBUG node `compute001` (instance id `af500973-2935-48a8-b5b9-76fca460134c`) still building...
2018-04-26 09:39:28 hwc hwc[3075] DEBUG Waiting for 3 more nodes to come up ...
2018-04-26 09:39:38 hwc hwc[3075] DEBUG Getting information for instance d9aa5ef4-b4cc-45d2-8dfd-845ef432dc78
2018-04-26 09:39:38 hwc hwc[3075] DEBUG node `compute002` (instance id `d9aa5ef4-b4cc-45d2-8dfd-845ef432dc78`) still building...
2018-04-26 09:39:38 hwc hwc[3075] DEBUG Getting information for instance b34c6343-8d7b-4f99-a869-8aa4dd4df58e
2018-04-26 09:39:39 hwc hwc[3075] DEBUG node `frontend001` (instance id `b34c6343-8d7b-4f99-a869-8aa4dd4df58e`) still building...
2018-04-26 09:39:39 hwc hwc[3075] DEBUG Getting information for instance af500973-2935-48a8-b5b9-76fca460134c
2018-04-26 09:39:40 hwc hwc[3075] DEBUG node `compute001` (instance id `af500973-2935-48a8-b5b9-76fca460134c`) still building...
2018-04-26 09:39:40 hwc hwc[3075] DEBUG Waiting for 3 more nodes to come up ...
2018-04-26 09:39:50 hwc hwc[3075] DEBUG Getting information for instance d9aa5ef4-b4cc-45d2-8dfd-845ef432dc78
2018-04-26 09:39:51 hwc hwc[3075] DEBUG node `compute002` (instance id `d9aa5ef4-b4cc-45d2-8dfd-845ef432dc78`) still building...
2018-04-26 09:39:51 hwc hwc[3075] DEBUG Getting information for instance b34c6343-8d7b-4f99-a869-8aa4dd4df58e
2018-04-26 09:39:51 hwc hwc[3075] DEBUG node `frontend001` (instance id `b34c6343-8d7b-4f99-a869-8aa4dd4df58e`) still building...
2018-04-26 09:39:51 hwc hwc[3075] DEBUG Getting information for instance af500973-2935-48a8-b5b9-76fca460134c
2018-04-26 09:39:52 hwc hwc[3075] DEBUG node `compute001` (instance id `af500973-2935-48a8-b5b9-76fca460134c`) still building...
2018-04-26 09:39:52 hwc hwc[3075] DEBUG Waiting for 3 more nodes to come up ...
2018-04-26 09:40:02 hwc hwc[3075] DEBUG Getting information for instance d9aa5ef4-b4cc-45d2-8dfd-845ef432dc78
2018-04-26 09:40:03 hwc hwc[3075] DEBUG node `compute002` (instance id `d9aa5ef4-b4cc-45d2-8dfd-845ef432dc78`) still building...
2018-04-26 09:40:03 hwc hwc[3075] DEBUG Getting information for instance b34c6343-8d7b-4f99-a869-8aa4dd4df58e
2018-04-26 09:40:03 hwc hwc[3075] DEBUG node `frontend001` (instance id `b34c6343-8d7b-4f99-a869-8aa4dd4df58e`) still building...
2018-04-26 09:40:03 hwc hwc[3075] DEBUG Getting information for instance af500973-2935-48a8-b5b9-76fca460134c
2018-04-26 09:40:04 hwc hwc[3075] DEBUG node `compute001` (instance id `af500973-2935-48a8-b5b9-76fca460134c`) still building...
2018-04-26 09:40:04 hwc hwc[3075] DEBUG Waiting for 3 more nodes to come up ...
2018-04-26 09:40:14 hwc hwc[3075] DEBUG Getting information for instance d9aa5ef4-b4cc-45d2-8dfd-845ef432dc78
2018-04-26 09:40:15 hwc hwc[3075] DEBUG node `compute002` (instance id `d9aa5ef4-b4cc-45d2-8dfd-845ef432dc78`) still building...
2018-04-26 09:40:15 hwc hwc[3075] DEBUG Getting information for instance b34c6343-8d7b-4f99-a869-8aa4dd4df58e
2018-04-26 09:40:15 hwc hwc[3075] DEBUG node `frontend001` (instance id `b34c6343-8d7b-4f99-a869-8aa4dd4df58e`) still building...
2018-04-26 09:40:15 hwc hwc[3075] DEBUG Getting information for instance af500973-2935-48a8-b5b9-76fca460134c
2018-04-26 09:40:16 hwc hwc[3075] DEBUG node `compute001` (instance id `af500973-2935-48a8-b5b9-76fca460134c`) still building...
2018-04-26 09:40:16 hwc hwc[3075] DEBUG Waiting for 3 more nodes to come up ...
2018-04-26 09:40:26 hwc hwc[3075] DEBUG Getting information for instance d9aa5ef4-b4cc-45d2-8dfd-845ef432dc78
2018-04-26 09:40:27 hwc hwc[3075] DEBUG node `compute002` (instance id `d9aa5ef4-b4cc-45d2-8dfd-845ef432dc78`) still building...
2018-04-26 09:40:27 hwc hwc[3075] DEBUG Getting information for instance b34c6343-8d7b-4f99-a869-8aa4dd4df58e
2018-04-26 09:40:28 hwc hwc[3075] DEBUG node `frontend001` (instance id `b34c6343-8d7b-4f99-a869-8aa4dd4df58e`) still building...
2018-04-26 09:40:28 hwc hwc[3075] DEBUG Getting information for instance af500973-2935-48a8-b5b9-76fca460134c
2018-04-26 09:40:28 hwc hwc[3075] DEBUG node `compute001` (instance id `af500973-2935-48a8-b5b9-76fca460134c`) still building...
2018-04-26 09:40:28 hwc hwc[3075] DEBUG Waiting for 3 more nodes to come up ...
2018-04-26 09:40:38 hwc hwc[3075] DEBUG Getting information for instance d9aa5ef4-b4cc-45d2-8dfd-845ef432dc78
2018-04-26 09:40:39 hwc hwc[3075] DEBUG node `compute002` (instance id `d9aa5ef4-b4cc-45d2-8dfd-845ef432dc78`) still building...
2018-04-26 09:40:39 hwc hwc[3075] DEBUG Getting information for instance b34c6343-8d7b-4f99-a869-8aa4dd4df58e
2018-04-26 09:40:40 hwc hwc[3075] DEBUG node `frontend001` (instance id `b34c6343-8d7b-4f99-a869-8aa4dd4df58e`) still building...
2018-04-26 09:40:40 hwc hwc[3075] DEBUG Getting information for instance af500973-2935-48a8-b5b9-76fca460134c
2018-04-26 09:40:40 hwc hwc[3075] DEBUG node `compute001` (instance id `af500973-2935-48a8-b5b9-76fca460134c`) still building...
2018-04-26 09:40:40 hwc hwc[3075] DEBUG Waiting for 3 more nodes to come up ...
2018-04-26 09:40:50 hwc hwc[3075] DEBUG Getting information for instance d9aa5ef4-b4cc-45d2-8dfd-845ef432dc78
2018-04-26 09:40:51 hwc hwc[3075] DEBUG node `compute002` (instance id `d9aa5ef4-b4cc-45d2-8dfd-845ef432dc78`) still building...
2018-04-26 09:40:51 hwc hwc[3075] DEBUG Getting information for instance b34c6343-8d7b-4f99-a869-8aa4dd4df58e
2018-04-26 09:40:52 hwc hwc[3075] DEBUG node `frontend001` (instance id `b34c6343-8d7b-4f99-a869-8aa4dd4df58e`) still building...
2018-04-26 09:40:52 hwc hwc[3075] DEBUG Getting information for instance af500973-2935-48a8-b5b9-76fca460134c
2018-04-26 09:40:52 hwc hwc[3075] DEBUG node `compute001` (instance id `af500973-2935-48a8-b5b9-76fca460134c`) still building...
2018-04-26 09:40:52 hwc hwc[3075] DEBUG Waiting for 3 more nodes to come up ...
2018-04-26 09:41:02 hwc hwc[3075] DEBUG Getting information for instance d9aa5ef4-b4cc-45d2-8dfd-845ef432dc78
2018-04-26 09:41:03 hwc hwc[3075] DEBUG node `compute002` (instance id `d9aa5ef4-b4cc-45d2-8dfd-845ef432dc78`) still building...
2018-04-26 09:41:03 hwc hwc[3075] DEBUG Getting information for instance b34c6343-8d7b-4f99-a869-8aa4dd4df58e
2018-04-26 09:41:03 hwc hwc[3075] DEBUG node `frontend001` (instance id `b34c6343-8d7b-4f99-a869-8aa4dd4df58e`) still building...
2018-04-26 09:41:03 hwc hwc[3075] DEBUG Getting information for instance af500973-2935-48a8-b5b9-76fca460134c
2018-04-26 09:41:04 hwc hwc[3075] DEBUG node `compute001` (instance id `af500973-2935-48a8-b5b9-76fca460134c`) still building...
2018-04-26 09:41:04 hwc hwc[3075] DEBUG Waiting for 3 more nodes to come up ...
2018-04-26 09:41:14 hwc hwc[3075] DEBUG Getting information for instance d9aa5ef4-b4cc-45d2-8dfd-845ef432dc78
2018-04-26 09:41:15 hwc hwc[3075] DEBUG node `compute002` (instance id `d9aa5ef4-b4cc-45d2-8dfd-845ef432dc78`) still building...
2018-04-26 09:41:15 hwc hwc[3075] DEBUG Getting information for instance b34c6343-8d7b-4f99-a869-8aa4dd4df58e
2018-04-26 09:41:16 hwc hwc[3075] DEBUG node `frontend001` (instance id `b34c6343-8d7b-4f99-a869-8aa4dd4df58e`) still building...
2018-04-26 09:41:16 hwc hwc[3075] DEBUG Getting information for instance af500973-2935-48a8-b5b9-76fca460134c
2018-04-26 09:41:17 hwc hwc[3075] DEBUG node `compute001` (instance id `af500973-2935-48a8-b5b9-76fca460134c`) still building...
2018-04-26 09:41:17 hwc hwc[3075] DEBUG Waiting for 3 more nodes to come up ...
2018-04-26 09:41:27 hwc hwc[3075] DEBUG Getting information for instance d9aa5ef4-b4cc-45d2-8dfd-845ef432dc78
2018-04-26 09:41:27 hwc hwc[3075] DEBUG node `compute002` (instance id `d9aa5ef4-b4cc-45d2-8dfd-845ef432dc78`) still building...
2018-04-26 09:41:27 hwc hwc[3075] DEBUG Getting information for instance b34c6343-8d7b-4f99-a869-8aa4dd4df58e
2018-04-26 09:41:28 hwc hwc[3075] DEBUG node `frontend001` (instance id `b34c6343-8d7b-4f99-a869-8aa4dd4df58e`) still building...
2018-04-26 09:41:28 hwc hwc[3075] DEBUG Getting information for instance af500973-2935-48a8-b5b9-76fca460134c
2018-04-26 09:41:29 hwc hwc[3075] DEBUG node `compute001` (instance id `af500973-2935-48a8-b5b9-76fca460134c`) still building...
2018-04-26 09:41:29 hwc hwc[3075] DEBUG Waiting for 3 more nodes to come up ...
2018-04-26 09:41:39 hwc hwc[3075] DEBUG Getting information for instance d9aa5ef4-b4cc-45d2-8dfd-845ef432dc78
2018-04-26 09:41:40 hwc hwc[3075] DEBUG node `compute002` (instance id `d9aa5ef4-b4cc-45d2-8dfd-845ef432dc78`) still building...
2018-04-26 09:41:40 hwc hwc[3075] DEBUG Getting information for instance b34c6343-8d7b-4f99-a869-8aa4dd4df58e
2018-04-26 09:41:42 hwc hwc[3075] DEBUG node `frontend001` (instance id `b34c6343-8d7b-4f99-a869-8aa4dd4df58e`) still building...
2018-04-26 09:41:42 hwc hwc[3075] DEBUG Getting information for instance af500973-2935-48a8-b5b9-76fca460134c
2018-04-26 09:41:42 hwc hwc[3075] DEBUG node `compute001` (instance id `af500973-2935-48a8-b5b9-76fca460134c`) still building...
2018-04-26 09:41:42 hwc hwc[3075] DEBUG Waiting for 3 more nodes to come up ...
2018-04-26 09:41:52 hwc hwc[3075] DEBUG Getting information for instance d9aa5ef4-b4cc-45d2-8dfd-845ef432dc78
2018-04-26 09:41:53 hwc hwc[3075] DEBUG node `compute002` (instance id `d9aa5ef4-b4cc-45d2-8dfd-845ef432dc78`) still building...
2018-04-26 09:41:53 hwc hwc[3075] DEBUG Getting information for instance b34c6343-8d7b-4f99-a869-8aa4dd4df58e
2018-04-26 09:41:54 hwc hwc[3075] DEBUG node `frontend001` (instance id `b34c6343-8d7b-4f99-a869-8aa4dd4df58e`) still building...
2018-04-26 09:41:54 hwc hwc[3075] DEBUG Getting information for instance af500973-2935-48a8-b5b9-76fca460134c
2018-04-26 09:41:54 hwc hwc[3075] DEBUG node `compute001` (instance id `af500973-2935-48a8-b5b9-76fca460134c`) still building...
2018-04-26 09:41:54 hwc hwc[3075] DEBUG Waiting for 3 more nodes to come up ...
2018-04-26 09:42:04 hwc hwc[3075] DEBUG Getting information for instance d9aa5ef4-b4cc-45d2-8dfd-845ef432dc78
2018-04-26 09:42:05 hwc hwc[3075] DEBUG node `compute002` (instance id `d9aa5ef4-b4cc-45d2-8dfd-845ef432dc78`) still building...
2018-04-26 09:42:05 hwc hwc[3075] DEBUG Getting information for instance b34c6343-8d7b-4f99-a869-8aa4dd4df58e
2018-04-26 09:42:06 hwc hwc[3075] DEBUG node `frontend001` (instance id `b34c6343-8d7b-4f99-a869-8aa4dd4df58e`) still building...
2018-04-26 09:42:06 hwc hwc[3075] DEBUG Getting information for instance af500973-2935-48a8-b5b9-76fca460134c
2018-04-26 09:42:07 hwc hwc[3075] DEBUG node `compute001` (instance id `af500973-2935-48a8-b5b9-76fca460134c`) still building...
2018-04-26 09:42:07 hwc hwc[3075] DEBUG Waiting for 3 more nodes to come up ...
2018-04-26 09:42:17 hwc hwc[3075] DEBUG Getting information for instance d9aa5ef4-b4cc-45d2-8dfd-845ef432dc78
2018-04-26 09:42:17 hwc hwc[3075] DEBUG node `compute002` (instance id d9aa5ef4-b4cc-45d2-8dfd-845ef432dc78) is up.
2018-04-26 09:42:18 hwc hwc[3075] DEBUG Getting information for instance b34c6343-8d7b-4f99-a869-8aa4dd4df58e
2018-04-26 09:42:18 hwc hwc[3075] DEBUG node `frontend001` (instance id b34c6343-8d7b-4f99-a869-8aa4dd4df58e) is up.
2018-04-26 09:42:19 hwc hwc[3075] DEBUG Getting information for instance af500973-2935-48a8-b5b9-76fca460134c
2018-04-26 09:42:20 hwc hwc[3075] DEBUG node `compute001` (instance id af500973-2935-48a8-b5b9-76fca460134c) is up.
2018-04-26 09:42:20 hwc hwc[3075] INFO Checking SSH connection to nodes (timeout: 600 seconds) ...
2018-04-26 09:42:20 hwc hwc[3075] DEBUG Trying to connect to host compute002 (192.168.0.20) ...
2018-04-26 09:42:23 hwc hwc[3075] DEBUG Host compute002 (192.168.0.20) not reachable within 5 seconds: [Errno None] Unable to connect to port 22 on 192.168.0.20 -- <class 'paramiko.ssh_exception.NoValidConnectionsError'>
2018-04-26 09:42:23 hwc hwc[3075] DEBUG Trying to connect to host frontend001 (192.168.0.46) ...
2018-04-26 09:42:26 hwc hwc[3075] DEBUG Host frontend001 (192.168.0.46) not reachable within 5 seconds: [Errno None] Unable to connect to port 22 on 192.168.0.46 -- <class 'paramiko.ssh_exception.NoValidConnectionsError'>
2018-04-26 09:42:26 hwc hwc[3075] DEBUG Trying to connect to host compute001 (192.168.0.251) ...
2018-04-26 09:42:31 hwc hwc[3075] DEBUG Host compute001 (192.168.0.251) not reachable within 5 seconds: timed out -- <class 'socket.timeout'>
2018-04-26 09:42:41 hwc hwc[3075] DEBUG Trying to connect to host compute002 (192.168.0.20) ...
2018-04-26 09:42:41 hwc hwc[3075] DEBUG Host compute002 (192.168.0.20) not reachable within 5 seconds: [Errno None] Unable to connect to port 22 on 192.168.0.20 -- <class 'paramiko.ssh_exception.NoValidConnectionsError'>
2018-04-26 09:42:41 hwc hwc[3075] DEBUG Trying to connect to host frontend001 (192.168.0.46) ...
2018-04-26 09:42:41 hwc hwc[3075] DEBUG Host frontend001 (192.168.0.46) not reachable within 5 seconds: [Errno None] Unable to connect to port 22 on 192.168.0.46 -- <class 'paramiko.ssh_exception.NoValidConnectionsError'>
2018-04-26 09:42:41 hwc hwc[3075] DEBUG Trying to connect to host compute001 (192.168.0.251) ...
2018-04-26 09:42:41 hwc hwc[3075] DEBUG Host compute001 (192.168.0.251) not reachable within 5 seconds: [Errno None] Unable to connect to port 22 on 192.168.0.251 -- <class 'paramiko.ssh_exception.NoValidConnectionsError'>
2018-04-26 09:42:51 hwc hwc[3075] DEBUG Trying to connect to host compute002 (192.168.0.20) ...
2018-04-26 09:42:51 hwc hwc[3075] DEBUG Host compute002 (192.168.0.20) not reachable within 5 seconds: [Errno None] Unable to connect to port 22 on 192.168.0.20 -- <class 'paramiko.ssh_exception.NoValidConnectionsError'>
2018-04-26 09:42:51 hwc hwc[3075] DEBUG Trying to connect to host frontend001 (192.168.0.46) ...
2018-04-26 09:42:51 hwc hwc[3075] DEBUG Host frontend001 (192.168.0.46) not reachable within 5 seconds: [Errno None] Unable to connect to port 22 on 192.168.0.46 -- <class 'paramiko.ssh_exception.NoValidConnectionsError'>
2018-04-26 09:42:51 hwc hwc[3075] DEBUG Trying to connect to host compute001 (192.168.0.251) ...
2018-04-26 09:42:51 hwc hwc[3075] DEBUG Host compute001 (192.168.0.251) not reachable within 5 seconds: [Errno None] Unable to connect to port 22 on 192.168.0.251 -- <class 'paramiko.ssh_exception.NoValidConnectionsError'>
2018-04-26 09:43:01 hwc hwc[3075] DEBUG Trying to connect to host compute002 (192.168.0.20) ...
2018-04-26 09:43:01 hwc hwc[3075] DEBUG Connection to 192.168.0.20 succeeded on port 22, will use this IP address for future connections.
2018-04-26 09:43:01 hwc hwc[3075] INFO Connection to node `compute002` successful, using IP address 192.168.0.20 to connect.
2018-04-26 09:43:01 hwc hwc[3075] DEBUG Trying to connect to host frontend001 (192.168.0.46) ...
2018-04-26 09:43:02 hwc hwc[3075] DEBUG Connection to 192.168.0.46 succeeded on port 22, will use this IP address for future connections.
2018-04-26 09:43:02 hwc hwc[3075] INFO Connection to node `frontend001` successful, using IP address 192.168.0.46 to connect.
2018-04-26 09:43:02 hwc hwc[3075] DEBUG Trying to connect to host compute001 (192.168.0.251) ...
2018-04-26 09:43:02 hwc hwc[3075] DEBUG Connection to 192.168.0.251 succeeded on port 22, will use this IP address for future connections.
2018-04-26 09:43:02 hwc hwc[3075] INFO Connection to node `compute001` successful, using IP address 192.168.0.251 to connect.
2018-04-26 09:43:02 hwc hwc[3075] DEBUG Writing Ansible inventory to file `/root/hwc/cfg/storage/slurm.inventory` ...
2018-04-26 09:43:02 hwc hwc[3075] DEBUG Calling `ansible-playbook` with the following environment:
2018-04-26 09:43:02 hwc hwc[3075] DEBUG - ANSIBLE_ANY_ERRORS_FATAL='yes'
2018-04-26 09:43:02 hwc hwc[3075] DEBUG - ANSIBLE_FORKS='10'
2018-04-26 09:43:02 hwc hwc[3075] DEBUG - ANSIBLE_HOST_KEY_CHECKING='no'
2018-04-26 09:43:02 hwc hwc[3075] DEBUG - ANSIBLE_RETRY_FILES_ENABLED='no'
2018-04-26 09:43:02 hwc hwc[3075] DEBUG - ANSIBLE_ROLES_PATH='/root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles:/root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks:/etc/ansible/roles'
2018-04-26 09:43:02 hwc hwc[3075] DEBUG - ANSIBLE_SSH_PIPELINING='yes'
2018-04-26 09:43:02 hwc hwc[3075] DEBUG - ANSIBLE_TIMEOUT='120'
2018-04-26 09:43:02 hwc hwc[3075] DEBUG - CINDER_ENDPOINT_TYPE='publicURL'
2018-04-26 09:43:02 hwc hwc[3075] DEBUG - CLUSTER_HOME='/root/elasticluster'
2018-04-26 09:43:02 hwc hwc[3075] DEBUG - CVS_RSH='ssh'
2018-04-26 09:43:02 hwc hwc[3075] DEBUG - G_BROKEN_FILENAMES='1'
2018-04-26 09:43:02 hwc hwc[3075] DEBUG - HISTCONTROL='ignoredups'
2018-04-26 09:43:02 hwc hwc[3075] DEBUG - HISTSIZE='1000'
2018-04-26 09:43:02 hwc hwc[3075] DEBUG - HISTTIMEFORMAT='%F %T root '
2018-04-26 09:43:02 hwc hwc[3075] DEBUG - HOME='/root'
2018-04-26 09:43:02 hwc hwc[3075] DEBUG - HOSTNAME='hwc'
2018-04-26 09:43:02 hwc hwc[3075] DEBUG - LANG='en_US.UTF-8'
2018-04-26 09:43:02 hwc hwc[3075] DEBUG - LESSOPEN='||/usr/bin/lesspipe.sh %s'
2018-04-26 09:43:02 hwc hwc[3075] DEBUG - LOGNAME='root'
2018-04-26 09:43:02 hwc hwc[3075] DEBUG - LS_COLORS='rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=01;05;37;41:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arj=01;31:*.taz=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.dz=01;31:*.gz=01;31:*.lz=01;31:*.xz=01;31:*.bz2=01;31:*.tbz=01;31:*.tbz2=01;31:*.bz=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.rar=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.jpg=01;35:*.jpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.axv=01;35:*.anx=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=01;36:*.au=01;36:*.flac=01;36:*.mid=01;36:*.midi=01;36:*.mka=01;36:*.mp3=01;36:*.mpc=01;36:*.ogg=01;36:*.ra=01;36:*.wav=01;36:*.axa=01;36:*.oga=01;36:*.spx=01;36:*.xspf=01;36:'
2018-04-26 09:43:02 hwc hwc[3075] DEBUG - MAIL='/var/spool/mail/root'
2018-04-26 09:43:02 hwc hwc[3075] DEBUG - NOVA_ENDPOINT_TYPE='publicURL'
2018-04-26 09:43:02 hwc hwc[3075] DEBUG - OS_AUTH_URL='https://iam.cn-north-1.myhwclouds.com/v3'
2018-04-26 09:43:02 hwc hwc[3075] DEBUG - OS_AVAILABILITY='cn-north-1a'
2018-04-26 09:43:02 hwc hwc[3075] DEBUG - OS_ENDPOINT_TYPE='publicURL'
2018-04-26 09:43:02 hwc hwc[3075] DEBUG - OS_IDENTITY_API_VERSION='3'
2018-04-26 09:43:02 hwc hwc[3075] DEBUG - OS_IMAGE_API_VERSION='2'
2018-04-26 09:43:02 hwc hwc[3075] DEBUG - OS_PASSWORD='1980813c'
2018-04-26 09:43:02 hwc hwc[3075] DEBUG - OS_PROJECT_DOMAIN_NAME='mschyj'
2018-04-26 09:43:02 hwc hwc[3075] DEBUG - OS_PROJECT_NAME='cn-north-1'
2018-04-26 09:43:02 hwc hwc[3075] DEBUG - OS_TENANT_NAME='cn-north-1'
2018-04-26 09:43:02 hwc hwc[3075] DEBUG - OS_USERNAME='mschyj'
2018-04-26 09:43:02 hwc hwc[3075] DEBUG - OS_USER_DOMAIN_NAME='mschyj'
2018-04-26 09:43:02 hwc hwc[3075] DEBUG - OS_VOLUME_API_VERSION='2'
2018-04-26 09:43:02 hwc hwc[3075] DEBUG - PATH='/usr/lib64/qt-3.3/bin:/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin:/root/bin:/root/bin'
2018-04-26 09:43:02 hwc hwc[3075] DEBUG - PWD='/root/hwc'
2018-04-26 09:43:02 hwc hwc[3075] DEBUG - QTDIR='/usr/lib64/qt-3.3'
2018-04-26 09:43:02 hwc hwc[3075] DEBUG - QTINC='/usr/lib64/qt-3.3/include'
2018-04-26 09:43:02 hwc hwc[3075] DEBUG - QTLIB='/usr/lib64/qt-3.3/lib'
2018-04-26 09:43:02 hwc hwc[3075] DEBUG - SHELL='/bin/bash'
2018-04-26 09:43:02 hwc hwc[3075] DEBUG - SHLVL='3'
2018-04-26 09:43:02 hwc hwc[3075] DEBUG - SSH_CLIENT='58.213.108.56 51111 22'
2018-04-26 09:43:02 hwc hwc[3075] DEBUG - SSH_CONNECTION='58.213.108.56 59229 192.168.0.188 22'
2018-04-26 09:43:02 hwc hwc[3075] DEBUG - SSH_TTY='/dev/pts/0'
2018-04-26 09:43:02 hwc hwc[3075] DEBUG - TERM='screen'
2018-04-26 09:43:02 hwc hwc[3075] DEBUG - TMUX='/tmp/tmux-0/default,11426,1'
2018-04-26 09:43:02 hwc hwc[3075] DEBUG - TMUX_PANE='%11'
2018-04-26 09:43:02 hwc hwc[3075] DEBUG - USER='root'
2018-04-26 09:43:02 hwc hwc[3075] DEBUG - _='/usr/local/bin/elasticluster'
2018-04-26 09:43:02 hwc hwc[3075] DEBUG Using playbook file /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/site.yml.
2018-04-26 09:43:02 hwc hwc[3075] DEBUG Running Ansible command `ansible-playbook --private-key=/root/hwc/auth/id_rsa /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/site.yml --inventory=/root/hwc/cfg/storage/slurm.inventory --become --become-user=root -vv -e elasticluster_output_dir=/tmp/elasticluster.p0mXwg.d` ...
No config file found; using defaults
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/hosts.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/hostname.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/netgroup.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/ssh_auth.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/ssh_keysign_rhel7.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/cuda/tasks/_check_nvidia_dev.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/cuda/tasks/_reboot_and_wait.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/cuda/tasks/_check_nvidia_dev.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/ceph/tasks/mon.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/ceph/tasks/mgr.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/ceph/tasks/_create_key.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/ceph/tasks/osd.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/ceph/tasks/mds.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/ceph/tasks/fs.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/ceph/tasks/client.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/glusterfs-common/tasks/debian.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/glusterfs-common/tasks/rhel.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/glusterfs-common/tasks/debian.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/glusterfs-common/tasks/rhel.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/ssh_auth.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/ssh_keysign_rhel7.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/handlers/main.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/handlers/main.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypbind.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/cgroup.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypbind.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypbind.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/lmod/tasks/build.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/lmod/tasks/post-install.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypbind.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/lmod/tasks/build.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/lmod/tasks/post-install.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypbind.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/postgresql/tasks/install.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/postgresql/tasks/install_yum.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/postgresql/tasks/extensions.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/postgresql/tasks/extensions/contrib.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/postgresql/tasks/extensions/dev_headers.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/postgresql/tasks/extensions/postgis.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/postgresql/tasks/configure.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/postgresql/tasks/users.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/postgresql/tasks/databases.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/postgresql/tasks/users_privileges.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/postgresql/tasks/monit.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/lmod/tasks/build.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/lmod/tasks/post-install.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/r/tasks/rmpi.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypbind.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/lmod/tasks/build.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/lmod/tasks/post-install.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/r/tasks/rmpi.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/lmod/tasks/build.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/lmod/tasks/post-install.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypbind.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/handlers/main.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/handlers/main.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/ssh_auth.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/ssh_keysign_rhel7.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/jenkins/tasks/main.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/jenkins/handlers/main.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/kubernetes-common/tasks/debian.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/kubernetes-common/tasks/redhat.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/kubernetes-common/tasks/debian.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/kubernetes-common/tasks/redhat.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypbind.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/lmod/tasks/build.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/lmod/tasks/post-install.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/pbs+maui/tasks/master.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/handlers/main.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/pbs+maui/handlers/main.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/pbs+maui/tasks/maui.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/pbs+maui/handlers/main.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypbind.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/lmod/tasks/build.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/lmod/tasks/post-install.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/pbs+maui/tasks/clients.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/pbs+maui/handlers/main.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypbind.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-master/tasks/db.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-master/tasks/install-slurmdbd.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-master/tasks/install-slurmctld.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypbind.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/cgroup.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypbind.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/lmod/tasks/build.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/lmod/tasks/post-install.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/lmod/tasks/build.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/lmod/tasks/post-install.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/r/tasks/rmpi.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/smb-server/tasks/ctdb.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/lmod/tasks/build.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/lmod/tasks/post-install.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/r/tasks/rmpi.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/jupyter/tasks/bash.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/jupyter/tasks/python.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/jupyter/tasks/python.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/jupyter/tasks/pyspark.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/jupyter/tasks/pyspark.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/jupyter/tasks/irkernel.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/jupyter/tasks/matlab.yml

PLAYBOOK: site.yml *************************************************************
44 plays in /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/site.yml

PLAY [Prepare VM for running Ansible] ******************************************

TASK [Ensure Python is installed] **********************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/site.yml:6
skipping: [compute002] => {"changed": false, "msg": "skipped, since /usr/bin/python exists", "skipped": true}
skipping: [compute001] => {"changed": false, "msg": "skipped, since /usr/bin/python exists", "skipped": true}
skipping: [frontend001] => {"changed": false, "msg": "skipped, since /usr/bin/python exists", "skipped": true}

PLAY [Apply local customizations (before)] *************************************

TASK [setup] *******************************************************************
ok: [compute002]
ok: [frontend001]
ok: [compute001]

PLAY [Common setup for all hosts] **********************************************

TASK [setup] *******************************************************************
ok: [compute002]
ok: [compute001]
ok: [frontend001]

TASK [Ensure apt-daily is *not* running] ***************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/site.yml:20
skipping: [frontend001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [common : Provide workaround for YAML syntax error in lines containing colon+space] ***
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/main.yml:3
ok: [frontend001] => {"ansible_facts": {"__colon__": ":"}, "changed": false}
ok: [compute001] => {"ansible_facts": {"__colon__": ":"}, "changed": false}
ok: [compute002] => {"ansible_facts": {"__colon__": ":"}, "changed": false}

TASK [common : Allow package updates] ******************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/main.yml:9
skipping: [frontend001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [common : Disallow package updates] ***************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/main.yml:14
ok: [frontend001] => {"ansible_facts": {"pkg_install_state": "present"}, "changed": false}
ok: [compute001] => {"ansible_facts": {"pkg_install_state": "present"}, "changed": false}
ok: [compute002] => {"ansible_facts": {"pkg_install_state": "present"}, "changed": false}

TASK [common : Set /etc/hosts from Ansible hostgroups] *************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/hosts.yml:2
changed: [compute001] => {"changed": true, "checksum": "4ba60d84dade03609e5b3bbaf7f9578c869adbf2", "dest": "/etc/hosts", "gid": 0, "group": "root", "md5sum": "5ffd090405f712999747b1d5e2009ef4", "mode": "0644", "owner": "root", "size": 430, "src": "/root/.ansible/tmp/ansible-tmp-1524706988.0-161821645145458/source", "state": "file", "uid": 0}
changed: [frontend001] => {"changed": true, "checksum": "4ba60d84dade03609e5b3bbaf7f9578c869adbf2", "dest": "/etc/hosts", "gid": 0, "group": "root", "md5sum": "5ffd090405f712999747b1d5e2009ef4", "mode": "0644", "owner": "root", "size": 430, "src": "/root/.ansible/tmp/ansible-tmp-1524706987.99-154122850914217/source", "state": "file", "uid": 0}
changed: [compute002] => {"changed": true, "checksum": "4ba60d84dade03609e5b3bbaf7f9578c869adbf2", "dest": "/etc/hosts", "gid": 0, "group": "root", "md5sum": "5ffd090405f712999747b1d5e2009ef4", "mode": "0644", "owner": "root", "size": 430, "src": "/root/.ansible/tmp/ansible-tmp-1524706988.01-123580468575056/source", "state": "file", "uid": 0}

TASK [common : Patch `/etc/redhat-release`] ************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/hostname.yml:8
skipping: [frontend001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [common : Set host name to Ansible "inventory name"] **********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/hostname.yml:16
changed: [compute001] => {"ansible_facts": {"ansible_domain": "", "ansible_fqdn": "compute001", "ansible_hostname": "compute001", "ansible_nodename": "compute001"}, "changed": true, "name": "compute001"}
ok: [frontend001] => {"ansible_facts": {"ansible_domain": "", "ansible_fqdn": "frontend001", "ansible_hostname": "frontend001", "ansible_nodename": "frontend001"}, "changed": false, "name": "frontend001"}
changed: [compute002] => {"ansible_facts": {"ansible_domain": "", "ansible_fqdn": "compute002", "ansible_hostname": "compute002", "ansible_nodename": "compute002"}, "changed": true, "name": "compute002"}

TASK [common : Undo patch to `/etc/redhat-release`] ****************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/hostname.yml:22
skipping: [frontend001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [common : Check for cloud-init conf file] *********************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/hostname.yml:30
ok: [frontend001] => {"changed": false, "stat": {"atime": 1524706979.523, "checksum": "ce75755e6aabf7995730f54951ecb5da27974667", "ctime": 1523322248.64, "dev": 51714, "executable": false, "exists": true, "gid": 0, "gr_name": "root", "inode": 1572909, "isblk": false, "ischr": false, "isdir": false, "isfifo": false, "isgid": false, "islnk": false, "isreg": true, "issock": false, "isuid": false, "md5": "bba22929886688d6488b6b8b85e33d74", "mode": "0664", "mtime": 1523322248.64, "nlink": 1, "path": "/etc/cloud/cloud.cfg", "pw_name": "root", "readable": true, "rgrp": true, "roth": true, "rusr": true, "size": 1627, "uid": 0, "wgrp": true, "woth": false, "writeable": true, "wusr": true, "xgrp": false, "xoth": false, "xusr": false}}
ok: [compute001] => {"changed": false, "stat": {"atime": 1524706980.68, "checksum": "ce75755e6aabf7995730f54951ecb5da27974667", "ctime": 1523322248.64, "dev": 51714, "executable": false, "exists": true, "gid": 0, "gr_name": "root", "inode": 1572909, "isblk": false, "ischr": false, "isdir": false, "isfifo": false, "isgid": false, "islnk": false, "isreg": true, "issock": false, "isuid": false, "md5": "bba22929886688d6488b6b8b85e33d74", "mode": "0664", "mtime": 1523322248.64, "nlink": 1, "path": "/etc/cloud/cloud.cfg", "pw_name": "root", "readable": true, "rgrp": true, "roth": true, "rusr": true, "size": 1627, "uid": 0, "wgrp": true, "woth": false, "writeable": true, "wusr": true, "xgrp": false, "xoth": false, "xusr": false}}
ok: [compute002] => {"changed": false, "stat": {"atime": 1524706977.718, "checksum": "ce75755e6aabf7995730f54951ecb5da27974667", "ctime": 1523322248.64, "dev": 51714, "executable": false, "exists": true, "gid": 0, "gr_name": "root", "inode": 1572909, "isblk": false, "ischr": false, "isdir": false, "isfifo": false, "isgid": false, "islnk": false, "isreg": true, "issock": false, "isuid": false, "md5": "bba22929886688d6488b6b8b85e33d74", "mode": "0664", "mtime": 1523322248.64, "nlink": 1, "path": "/etc/cloud/cloud.cfg", "pw_name": "root", "readable": true, "rgrp": true, "roth": true, "rusr": true, "size": 1627, "uid": 0, "wgrp": true, "woth": false, "writeable": true, "wusr": true, "xgrp": false, "xoth": false, "xusr": false}}

TASK [common : Ensure changes to hostname are not overwritten by cloud-init] ***
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/hostname.yml:41
ok: [compute002] => {"backup": "", "changed": false, "msg": ""}
ok: [compute001] => {"backup": "", "changed": false, "msg": ""}
ok: [frontend001] => {"backup": "", "changed": false, "msg": ""}

TASK [common : Deploy `/etc/netgroup` file.] ***********************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/netgroup.yml:3
changed: [compute002] => {"changed": true, "checksum": "ee2d3dfaf1af7ca9fa96b282c07b66cf57d2ecb6", "dest": "/etc/netgroup", "gid": 0, "group": "root", "md5sum": "5fa85341d712d505a56fb9a43e938e53", "mode": "0444", "owner": "root", "size": 274, "src": "/root/.ansible/tmp/ansible-tmp-1524706990.72-260507133016731/source", "state": "file", "uid": 0}
changed: [frontend001] => {"changed": true, "checksum": "ee2d3dfaf1af7ca9fa96b282c07b66cf57d2ecb6", "dest": "/etc/netgroup", "gid": 0, "group": "root", "md5sum": "5fa85341d712d505a56fb9a43e938e53", "mode": "0444", "owner": "root", "size": 274, "src": "/root/.ansible/tmp/ansible-tmp-1524706990.68-204867459135581/source", "state": "file", "uid": 0}
changed: [compute001] => {"changed": true, "checksum": "ee2d3dfaf1af7ca9fa96b282c07b66cf57d2ecb6", "dest": "/etc/netgroup", "gid": 0, "group": "root", "md5sum": "5fa85341d712d505a56fb9a43e938e53", "mode": "0444", "owner": "root", "size": 274, "src": "/root/.ansible/tmp/ansible-tmp-1524706990.69-56865469462660/source", "state": "file", "uid": 0}

TASK [common : Add `files` databases to `netgroup` service (I)] ****************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/netgroup.yml:21
ok: [compute001] => {"changed": false, "msg": ""}
ok: [frontend001] => {"changed": false, "msg": ""}
ok: [compute002] => {"changed": false, "msg": ""}

TASK [common : Add `files` databases to `netgroup` service (II)] ***************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/netgroup.yml:32
skipping: [frontend001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [common : Load distribution-dependent values (Debian/Ubuntu)] *************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/ssh_auth.yml:6
skipping: [frontend001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [common : Load distribution-dependent values (CentOS/RHEL)] ***************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/ssh_auth.yml:12
ok: [frontend001] => {"ansible_facts": {"ssh_keysign_path": "/usr/libexec/openssh/ssh-keysign"}, "changed": false}
ok: [compute001] => {"ansible_facts": {"ssh_keysign_path": "/usr/libexec/openssh/ssh-keysign"}, "changed": false}
ok: [compute002] => {"ansible_facts": {"ssh_keysign_path": "/usr/libexec/openssh/ssh-keysign"}, "changed": false}

TASK [common : Setup SSH known hosts file] *************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/ssh_auth.yml:18
changed: [compute001] => {"changed": true, "checksum": "c16912b90f9afba6d5883197f680be5e561243a1", "dest": "/etc/ssh/ssh_known_hosts", "gid": 0, "group": "root", "md5sum": "7df704e721531cdfce7b937bef2c63a5", "mode": "0644", "owner": "root", "size": 3058, "src": "/root/.ansible/tmp/ansible-tmp-1524706992.8-213627770505534/source", "state": "file", "uid": 0}
changed: [compute002] => {"changed": true, "checksum": "c16912b90f9afba6d5883197f680be5e561243a1", "dest": "/etc/ssh/ssh_known_hosts", "gid": 0, "group": "root", "md5sum": "7df704e721531cdfce7b937bef2c63a5", "mode": "0644", "owner": "root", "size": 3058, "src": "/root/.ansible/tmp/ansible-tmp-1524706992.82-144845780871719/source", "state": "file", "uid": 0}
changed: [frontend001] => {"changed": true, "checksum": "c16912b90f9afba6d5883197f680be5e561243a1", "dest": "/etc/ssh/ssh_known_hosts", "gid": 0, "group": "root", "md5sum": "7df704e721531cdfce7b937bef2c63a5", "mode": "0644", "owner": "root", "size": 3058, "src": "/root/.ansible/tmp/ansible-tmp-1524706992.79-96536429276/source", "state": "file", "uid": 0}

TASK [common : Setup /etc/ssh/shosts.equiv file] *******************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/ssh_auth.yml:27
changed: [frontend001] => {"changed": true, "checksum": "39e0a8b4db7f45649c4408113447924b0ef0b657", "dest": "/etc/ssh/shosts.equiv", "gid": 0, "group": "root", "md5sum": "5559a292750232936527b76454fbcd54", "mode": "0644", "owner": "root", "size": 74, "src": "/root/.ansible/tmp/ansible-tmp-1524706994.01-58499707554271/source", "state": "file", "uid": 0}
changed: [compute002] => {"changed": true, "checksum": "39e0a8b4db7f45649c4408113447924b0ef0b657", "dest": "/etc/ssh/shosts.equiv", "gid": 0, "group": "root", "md5sum": "5559a292750232936527b76454fbcd54", "mode": "0644", "owner": "root", "size": 74, "src": "/root/.ansible/tmp/ansible-tmp-1524706994.03-262639814222577/source", "state": "file", "uid": 0}
changed: [compute001] => {"changed": true, "checksum": "39e0a8b4db7f45649c4408113447924b0ef0b657", "dest": "/etc/ssh/shosts.equiv", "gid": 0, "group": "root", "md5sum": "5559a292750232936527b76454fbcd54", "mode": "0644", "owner": "root", "size": 74, "src": "/root/.ansible/tmp/ansible-tmp-1524706994.02-141574026664028/source", "state": "file", "uid": 0}

TASK [common : Setup /root/.shosts file] ***************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/ssh_auth.yml:35
changed: [frontend001] => {"changed": true, "checksum": "39e0a8b4db7f45649c4408113447924b0ef0b657", "dest": "/root/.shosts", "gid": 0, "group": "root", "md5sum": "5559a292750232936527b76454fbcd54", "mode": "0644", "owner": "root", "size": 74, "src": "/root/.ansible/tmp/ansible-tmp-1524706995.24-49568941993213/source", "state": "file", "uid": 0}
changed: [compute002] => {"changed": true, "checksum": "39e0a8b4db7f45649c4408113447924b0ef0b657", "dest": "/root/.shosts", "gid": 0, "group": "root", "md5sum": "5559a292750232936527b76454fbcd54", "mode": "0644", "owner": "root", "size": 74, "src": "/root/.ansible/tmp/ansible-tmp-1524706995.26-24579344574429/source", "state": "file", "uid": 0}
changed: [compute001] => {"changed": true, "checksum": "39e0a8b4db7f45649c4408113447924b0ef0b657", "dest": "/root/.shosts", "gid": 0, "group": "root", "md5sum": "5559a292750232936527b76454fbcd54", "mode": "0644", "owner": "root", "size": 74, "src": "/root/.ansible/tmp/ansible-tmp-1524706995.25-218879523827427/source", "state": "file", "uid": 0}

TASK [common : Setup SSH authentication (server configuration file)] ***********
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/ssh_auth.yml:43
ok: [frontend001] => (item={u'value': u'yes', u'key': u'HostbasedAuthentication'}) => {"backup": "", "changed": false, "item": {"key": "HostbasedAuthentication", "value": "yes"}, "msg": ""}
ok: [compute001] => (item={u'value': u'yes', u'key': u'HostbasedAuthentication'}) => {"backup": "", "changed": false, "item": {"key": "HostbasedAuthentication", "value": "yes"}, "msg": ""}
ok: [compute002] => (item={u'value': u'yes', u'key': u'HostbasedAuthentication'}) => {"backup": "", "changed": false, "item": {"key": "HostbasedAuthentication", "value": "yes"}, "msg": ""}
ok: [compute001] => (item={u'value': u'no', u'key': u'IgnoreRhosts'}) => {"backup": "", "changed": false, "item": {"key": "IgnoreRhosts", "value": "no"}, "msg": ""}
ok: [compute002] => (item={u'value': u'no', u'key': u'IgnoreRhosts'}) => {"backup": "", "changed": false, "item": {"key": "IgnoreRhosts", "value": "no"}, "msg": ""}
ok: [frontend001] => (item={u'value': u'no', u'key': u'IgnoreRhosts'}) => {"backup": "", "changed": false, "item": {"key": "IgnoreRhosts", "value": "no"}, "msg": ""}
ok: [compute001] => (item={u'value': u'yes', u'key': u'PasswordAuthentication'}) => {"backup": "", "changed": false, "item": {"key": "PasswordAuthentication", "value": "yes"}, "msg": ""}
ok: [frontend001] => (item={u'value': u'yes', u'key': u'PasswordAuthentication'}) => {"backup": "", "changed": false, "item": {"key": "PasswordAuthentication", "value": "yes"}, "msg": ""}
ok: [compute002] => (item={u'value': u'yes', u'key': u'PasswordAuthentication'}) => {"backup": "", "changed": false, "item": {"key": "PasswordAuthentication", "value": "yes"}, "msg": ""}

TASK [common : Setup SSH authentication (client configuration file)] ***********
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/ssh_auth.yml:55
ok: [compute001] => (item={u'value': u'yes', u'key': u'HostbasedAuthentication'}) => {"backup": "", "changed": false, "item": {"key": "HostbasedAuthentication", "value": "yes"}, "msg": ""}
ok: [frontend001] => (item={u'value': u'yes', u'key': u'HostbasedAuthentication'}) => {"backup": "", "changed": false, "item": {"key": "HostbasedAuthentication", "value": "yes"}, "msg": ""}
ok: [compute002] => (item={u'value': u'yes', u'key': u'HostbasedAuthentication'}) => {"backup": "", "changed": false, "item": {"key": "HostbasedAuthentication", "value": "yes"}, "msg": ""}
ok: [compute002] => (item={u'value': u'yes', u'key': u'EnableSSHKeysign'}) => {"backup": "", "changed": false, "item": {"key": "EnableSSHKeysign", "value": "yes"}, "msg": ""}
ok: [compute001] => (item={u'value': u'yes', u'key': u'EnableSSHKeysign'}) => {"backup": "", "changed": false, "item": {"key": "EnableSSHKeysign", "value": "yes"}, "msg": ""}
ok: [frontend001] => (item={u'value': u'yes', u'key': u'EnableSSHKeysign'}) => {"backup": "", "changed": false, "item": {"key": "EnableSSHKeysign", "value": "yes"}, "msg": ""}

TASK [common : Load info about the `ssh-keysign` executable] *******************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/ssh_keysign_rhel7.yml:9
skipping: [frontend001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [common : List all SSH host keys] *****************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/ssh_keysign_rhel7.yml:15
skipping: [frontend001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [common : Ensure SSH host keys can be read by `ssh-keysign`] **************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/ssh_keysign_rhel7.yml:23
skipping: [frontend001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [iptables : Load distribution-specific data] ******************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/iptables/tasks/main.yml:18
included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/iptables/tasks/init-RedHat.yml for frontend001, compute001, compute002

TASK [iptables : Load configuration and service names (RHEL-compatible)] *******
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/iptables/tasks/init-RedHat.yml:3
ok: [frontend001] => {"ansible_facts": {"configfile": {"etc/iptables/rules.v4": "/etc/sysconfig/iptables", "etc/iptables/rules.v6": "/etc/sysconfig/ip6tables"}, "reload": "reload iptables", "service": {"ip6tables": "ip6tables", "iptables": "iptables"}}, "changed": false}
ok: [compute001] => {"ansible_facts": {"configfile": {"etc/iptables/rules.v4": "/etc/sysconfig/iptables", "etc/iptables/rules.v6": "/etc/sysconfig/ip6tables"}, "reload": "reload iptables", "service": {"ip6tables": "ip6tables", "iptables": "iptables"}}, "changed": false}
ok: [compute002] => {"ansible_facts": {"configfile": {"etc/iptables/rules.v4": "/etc/sysconfig/iptables", "etc/iptables/rules.v6": "/etc/sysconfig/ip6tables"}, "reload": "reload iptables", "service": {"ip6tables": "ip6tables", "iptables": "iptables"}}, "changed": false}

TASK [iptables : Load configuration and service names (RHEL6-compatible)] ******
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/iptables/tasks/init-RedHat.yml:17
ok: [frontend001] => {"ansible_facts": {"packages": ["iptables", "iptables-ipv6"]}, "changed": false}
ok: [compute001] => {"ansible_facts": {"packages": ["iptables", "iptables-ipv6"]}, "changed": false}
ok: [compute002] => {"ansible_facts": {"packages": ["iptables", "iptables-ipv6"]}, "changed": false}

TASK [iptables : Load configuration and service names (RHEL7-compatible)] ******
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/iptables/tasks/init-RedHat.yml:25
skipping: [frontend001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [iptables : Install iptables packages] ************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/iptables/tasks/main.yml:25
ok: [frontend001] => (item=[u'iptables', u'iptables-ipv6']) => {"changed": false, "item": ["iptables", "iptables-ipv6"], "msg": "", "rc": 0, "results": ["iptables-1.4.7-16.el6.x86_64 providing iptables is already installed", "iptables-ipv6-1.4.7-16.el6.x86_64 providing iptables-ipv6 is already installed"]}
ok: [compute002] => (item=[u'iptables', u'iptables-ipv6']) => {"changed": false, "item": ["iptables", "iptables-ipv6"], "msg": "", "rc": 0, "results": ["iptables-1.4.7-16.el6.x86_64 providing iptables is already installed", "iptables-ipv6-1.4.7-16.el6.x86_64 providing iptables-ipv6 is already installed"]}
ok: [compute001] => (item=[u'iptables', u'iptables-ipv6']) => {"changed": false, "item": ["iptables", "iptables-ipv6"], "msg": "", "rc": 0, "results": ["iptables-1.4.7-16.el6.x86_64 providing iptables is already installed", "iptables-ipv6-1.4.7-16.el6.x86_64 providing iptables-ipv6 is already installed"]}

TASK [iptables : Deploy netfilter rules] ***************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/iptables/tasks/main.yml:33
changed: [compute001] => (item=etc/iptables/rules.v4) => {"changed": true, "checksum": "0e06f592e948ed7cf2f6f9f837343ef2ddd83360", "dest": "/etc/sysconfig/iptables", "gid": 0, "group": "root", "item": "etc/iptables/rules.v4", "md5sum": "ce021ee727209d8fcc33ff4bbceeb808", "mode": "0444", "owner": "root", "size": 1114, "src": "/root/.ansible/tmp/ansible-tmp-1524706999.89-204569509154267/source", "state": "file", "uid": 0}
changed: [compute002] => (item=etc/iptables/rules.v4) => {"changed": true, "checksum": "0e06f592e948ed7cf2f6f9f837343ef2ddd83360", "dest": "/etc/sysconfig/iptables", "gid": 0, "group": "root", "item": "etc/iptables/rules.v4", "md5sum": "ce021ee727209d8fcc33ff4bbceeb808", "mode": "0444", "owner": "root", "size": 1114, "src": "/root/.ansible/tmp/ansible-tmp-1524706999.91-203079038028307/source", "state": "file", "uid": 0}
changed: [frontend001] => (item=etc/iptables/rules.v4) => {"changed": true, "checksum": "0e06f592e948ed7cf2f6f9f837343ef2ddd83360", "dest": "/etc/sysconfig/iptables", "gid": 0, "group": "root", "item": "etc/iptables/rules.v4", "md5sum": "ce021ee727209d8fcc33ff4bbceeb808", "mode": "0444", "owner": "root", "size": 1114, "src": "/root/.ansible/tmp/ansible-tmp-1524706999.87-44884448815733/source", "state": "file", "uid": 0}
NOTIFIED HANDLER reload iptables
NOTIFIED HANDLER reload iptables
ok: [frontend001] => (item=etc/iptables/rules.v6) => {"changed": false, "gid": 0, "group": "root", "item": "etc/iptables/rules.v6", "mode": "0444", "owner": "root", "path": "/etc/sysconfig/ip6tables", "size": 1180, "state": "file", "uid": 0}
NOTIFIED HANDLER reload iptables
ok: [compute002] => (item=etc/iptables/rules.v6) => {"changed": false, "gid": 0, "group": "root", "item": "etc/iptables/rules.v6", "mode": "0444", "owner": "root", "path": "/etc/sysconfig/ip6tables", "size": 1180, "state": "file", "uid": 0}
ok: [compute001] => (item=etc/iptables/rules.v6) => {"changed": false, "gid": 0, "group": "root", "item": "etc/iptables/rules.v6", "mode": "0444", "owner": "root", "path": "/etc/sysconfig/ip6tables", "size": 1180, "state": "file", "uid": 0}

TASK [iptables : Ensure netfilter rules are loaded at boot] ********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/iptables/tasks/main.yml:50
ok: [compute002] => (item=iptables) => {"changed": false, "enabled": true, "item": "iptables", "name": "iptables", "state": "started"}
ok: [compute001] => (item=iptables) => {"changed": false, "enabled": true, "item": "iptables", "name": "iptables", "state": "started"}
ok: [frontend001] => (item=iptables) => {"changed": false, "enabled": true, "item": "iptables", "name": "iptables", "state": "started"}
ok: [compute002] => (item=ip6tables) => {"changed": false, "enabled": true, "item": "ip6tables", "name": "ip6tables", "state": "started"}
ok: [compute001] => (item=ip6tables) => {"changed": false, "enabled": true, "item": "ip6tables", "name": "ip6tables", "state": "started"}
ok: [frontend001] => (item=ip6tables) => {"changed": false, "enabled": true, "item": "ip6tables", "name": "ip6tables", "state": "started"}

RUNNING HANDLER [iptables : reload iptables] ***********************************
changed: [compute001] => (item=iptables) => {"changed": true, "item": "iptables", "name": "iptables", "state": "started"}
changed: [frontend001] => (item=iptables) => {"changed": true, "item": "iptables", "name": "iptables", "state": "started"}
changed: [compute002] => (item=iptables) => {"changed": true, "item": "iptables", "name": "iptables", "state": "started"}
changed: [frontend001] => (item=ip6tables) => {"changed": true, "item": "ip6tables", "name": "ip6tables", "state": "started"}
changed: [compute001] => (item=ip6tables) => {"changed": true, "item": "ip6tables", "name": "ip6tables", "state": "started"}
changed: [compute002] => (item=ip6tables) => {"changed": true, "item": "ip6tables", "name": "ip6tables", "state": "started"}

TASK [ntpd : Load distribution-specific parameters] ****************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/ntpd/tasks/main.yml:3
included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/ntpd/tasks/init-RedHat.yml for frontend001, compute001, compute002

TASK [ntpd : Set NTPd common playbook params (RHEL/CentOS)] ********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/ntpd/tasks/init-RedHat.yml:3
ok: [frontend001] => {"ansible_facts": {"ntpd_package": "ntp", "ntpd_service": "ntpd"}, "changed": false}
ok: [compute001] => {"ansible_facts": {"ntpd_package": "ntp", "ntpd_service": "ntpd"}, "changed": false}
ok: [compute002] => {"ansible_facts": {"ntpd_package": "ntp", "ntpd_service": "ntpd"}, "changed": false}

TASK [ntpd : Deploy NTP configuration file] ************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/ntpd/tasks/main.yml:7
NOTIFIED HANDLER restart ntpd
changed: [compute002] => {"changed": true, "checksum": "af804a75af9d7acdc73f35ac807c870bc1c4b119", "dest": "/etc/ntp.conf", "gid": 0, "group": "root", "md5sum": "6de30d188e852b806b3b6fe902673286", "mode": "0444", "owner": "root", "size": 1983, "src": "/root/.ansible/tmp/ansible-tmp-1524707003.32-51373358232573/source", "state": "file", "uid": 0}
NOTIFIED HANDLER restart ntpd
changed: [compute001] => {"changed": true, "checksum": "1cff101d1a86b3b2e39151809d17b232abf8eaa5", "dest": "/etc/ntp.conf", "gid": 0, "group": "root", "md5sum": "508b82e89163e2c216a396161f4e8f23", "mode": "0444", "owner": "root", "size": 1983, "src": "/root/.ansible/tmp/ansible-tmp-1524707003.31-254413105368947/source", "state": "file", "uid": 0}
NOTIFIED HANDLER restart ntpd
changed: [frontend001] => {"changed": true, "checksum": "6a2291b1f1fd168748d1db7953cbe66ed2904151", "dest": "/etc/ntp.conf", "gid": 0, "group": "root", "md5sum": "e7b8052f8f4375213ed726d36b7a96f8", "mode": "0444", "owner": "root", "size": 1981, "src": "/root/.ansible/tmp/ansible-tmp-1524707003.3-224699866756731/source", "state": "file", "uid": 0}

TASK [ntpd : Install NTP packages] *********************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/ntpd/tasks/main.yml:19
ok: [compute001] => {"changed": false, "msg": "", "rc": 0, "results": ["ntp-4.2.6p5-12.el6.centos.2.x86_64 providing ntp is already installed"]}
ok: [frontend001] => {"changed": false, "msg": "", "rc": 0, "results": ["ntp-4.2.6p5-12.el6.centos.2.x86_64 providing ntp is already installed"]}
ok: [compute002] => {"changed": false, "msg": "", "rc": 0, "results": ["ntp-4.2.6p5-12.el6.centos.2.x86_64 providing ntp is already installed"]}

TASK [ntpd : Enable NTP service at boot] ***************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/ntpd/tasks/main.yml:42
ok: [frontend001] => {"changed": false, "enabled": true, "name": "ntpd", "state": "started"}
ok: [compute002] => {"changed": false, "enabled": true, "name": "ntpd", "state": "started"}
ok: [compute001] => {"changed": false, "enabled": true, "name": "ntpd", "state": "started"}

TASK [pdsh : Install pdsh packages (Debian-family)] ****************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/pdsh/tasks/main.yml:3
skipping: [frontend001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [pdsh : Install pdsh packages (RHEL-family)] ******************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/pdsh/tasks/main.yml:11
ok: [frontend001] => (item=[u'pdsh', u'pdsh-rcmd-ssh', u'pdsh-mod-genders']) => {"changed": false, "item": ["pdsh", "pdsh-rcmd-ssh", "pdsh-mod-genders"], "msg": "", "rc": 0, "results": ["pdsh-2.26-4.el6.x86_64 providing pdsh is already installed", "pdsh-rcmd-ssh-2.26-4.el6.x86_64 providing pdsh-rcmd-ssh is already installed", "pdsh-mod-genders-2.26-4.el6.x86_64 providing pdsh-mod-genders is already installed"]}
ok: [compute001] => (item=[u'pdsh', u'pdsh-rcmd-ssh', u'pdsh-mod-genders']) => {"changed": false, "item": ["pdsh", "pdsh-rcmd-ssh", "pdsh-mod-genders"], "msg": "", "rc": 0, "results": ["pdsh-2.26-4.el6.x86_64 providing pdsh is already installed", "pdsh-rcmd-ssh-2.26-4.el6.x86_64 providing pdsh-rcmd-ssh is already installed", "pdsh-mod-genders-2.26-4.el6.x86_64 providing pdsh-mod-genders is already installed"]}
ok: [compute002] => (item=[u'pdsh', u'pdsh-rcmd-ssh', u'pdsh-mod-genders']) => {"changed": false, "item": ["pdsh", "pdsh-rcmd-ssh", "pdsh-mod-genders"], "msg": "", "rc": 0, "results": ["pdsh-2.26-4.el6.x86_64 providing pdsh is already installed", "pdsh-rcmd-ssh-2.26-4.el6.x86_64 providing pdsh-rcmd-ssh is already installed", "pdsh-mod-genders-2.26-4.el6.x86_64 providing pdsh-mod-genders is already installed"]}

TASK [pdsh : Create genders file for PDSH] *************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/pdsh/tasks/main.yml:21
changed: [compute002] => {"changed": true, "checksum": "4d9cb5086244d2a2e7b0cfe22261aeb8b7b88bcc", "dest": "/etc/genders", "gid": 0, "group": "root", "md5sum": "ea66ce19ee3c74de41bcc6d9958081a9", "mode": "0444", "owner": "root", "size": 206, "src": "/root/.ansible/tmp/ansible-tmp-1524707006.31-81492867705107/source", "state": "file", "uid": 0}
changed: [compute001] => {"changed": true, "checksum": "4d9cb5086244d2a2e7b0cfe22261aeb8b7b88bcc", "dest": "/etc/genders", "gid": 0, "group": "root", "md5sum": "ea66ce19ee3c74de41bcc6d9958081a9", "mode": "0444", "owner": "root", "size": 206, "src": "/root/.ansible/tmp/ansible-tmp-1524707006.3-104630114276301/source", "state": "file", "uid": 0}
changed: [frontend001] => {"changed": true, "checksum": "4d9cb5086244d2a2e7b0cfe22261aeb8b7b88bcc", "dest": "/etc/genders", "gid": 0, "group": "root", "md5sum": "ea66ce19ee3c74de41bcc6d9958081a9", "mode": "0444", "owner": "root", "size": 206, "src": "/root/.ansible/tmp/ansible-tmp-1524707006.29-73778413783177/source", "state": "file", "uid": 0}

TASK [pdsh : Make SSH the default exec method for PDSH] ************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/pdsh/tasks/main.yml:29
ok: [frontend001] => {"changed": false, "checksum": "5929e07c9e9a681ab81ae9da0e1a4c17630509ab", "dest": "/etc/profile.d/pdsh.sh", "gid": 0, "group": "root", "mode": "0444", "owner": "root", "path": "/etc/profile.d/pdsh.sh", "size": 278, "state": "file", "uid": 0}
ok: [compute001] => {"changed": false, "checksum": "5929e07c9e9a681ab81ae9da0e1a4c17630509ab", "dest": "/etc/profile.d/pdsh.sh", "gid": 0, "group": "root", "mode": "0444", "owner": "root", "path": "/etc/profile.d/pdsh.sh", "size": 278, "state": "file", "uid": 0}
ok: [compute002] => {"changed": false, "checksum": "5929e07c9e9a681ab81ae9da0e1a4c17630509ab", "dest": "/etc/profile.d/pdsh.sh", "gid": 0, "group": "root", "mode": "0444", "owner": "root", "path": "/etc/profile.d/pdsh.sh", "size": 278, "state": "file", "uid": 0}

RUNNING HANDLER [ntpd : restart ntpd] ******************************************
changed: [compute002] => {"changed": true, "name": "ntpd", "state": "started"}
changed: [compute001] => {"changed": true, "name": "ntpd", "state": "started"}
changed: [frontend001] => {"changed": true, "name": "ntpd", "state": "started"}

PLAY [Slurm worker nodes Playbook] *********************************************

TASK [setup] *******************************************************************
ok: [frontend001]
ok: [compute001]
ok: [compute002]

TASK [nis : Load distribution-specific parameters] *****************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/main.yml:3
included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/init-RedHat.yml for frontend001, compute001, compute002

TASK [nis : Set NIS common playbook params (CentOS/RHEL)] **********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/init-RedHat.yml:3
ok: [frontend001] => {"ansible_facts": {"nis_client_packages": ["ypbind"], "nis_client_services": ["rpcbind", "ypbind"], "nis_common_packages": ["yp-tools"], "nis_master_packages": ["ypserv"], "nis_master_services": ["rpcbind", "yppasswdd", "ypserv"], "nis_securenets_path": "/var/yp/securenets"}, "changed": false}
ok: [compute001] => {"ansible_facts": {"nis_client_packages": ["ypbind"], "nis_client_services": ["rpcbind", "ypbind"], "nis_common_packages": ["yp-tools"], "nis_master_packages": ["ypserv"], "nis_master_services": ["rpcbind", "yppasswdd", "ypserv"], "nis_securenets_path": "/var/yp/securenets"}, "changed": false}
ok: [compute002] => {"ansible_facts": {"nis_client_packages": ["ypbind"], "nis_client_services": ["rpcbind", "ypbind"], "nis_common_packages": ["yp-tools"], "nis_master_packages": ["ypserv"], "nis_master_services": ["rpcbind", "yppasswdd", "ypserv"], "nis_securenets_path": "/var/yp/securenets"}, "changed": false}

TASK [nis : Pre-load debconf answer to questions (Debian/Ubuntu)] **************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/main.yml:7
skipping: [frontend001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [nis : Deploy additional NIS configuration (Debian/Ubuntu)] ***************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/main.yml:18
skipping: [frontend001] => (item=etc/default/nis)  => {"changed": false, "item": "etc/default/nis", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [frontend001] => (item=etc/defaultdomain)  => {"changed": false, "item": "etc/defaultdomain", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => (item=etc/default/nis)  => {"changed": false, "item": "etc/default/nis", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => (item=etc/defaultdomain)  => {"changed": false, "item": "etc/defaultdomain", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item=etc/default/nis)  => {"changed": false, "item": "etc/default/nis", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item=etc/defaultdomain)  => {"changed": false, "item": "etc/defaultdomain", "skip_reason": "Conditional check failed", "skipped": true}

TASK [nis : Set NIS domain (CentOS/RHEL)] **************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/main.yml:29
changed: [compute001] => {"backup": "", "changed": true, "msg": "line added"}
changed: [frontend001] => {"backup": "", "changed": true, "msg": "line added"}
changed: [compute002] => {"backup": "", "changed": true, "msg": "line added"}

TASK [nis : Install NIS common packages] ***************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/main.yml:39
ok: [compute001] => (item=[u'yp-tools']) => {"changed": false, "item": ["yp-tools"], "msg": "", "rc": 0, "results": ["yp-tools-2.9-12.el6.x86_64 providing yp-tools is already installed"]}
ok: [compute002] => (item=[u'yp-tools']) => {"changed": false, "item": ["yp-tools"], "msg": "", "rc": 0, "results": ["yp-tools-2.9-12.el6.x86_64 providing yp-tools is already installed"]}
ok: [frontend001] => (item=[u'yp-tools']) => {"changed": false, "item": ["yp-tools"], "msg": "", "rc": 0, "results": ["yp-tools-2.9-12.el6.x86_64 providing yp-tools is already installed"]}

TASK [nis : Deploy `ypserv` configuration files] *******************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml:3
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
NOTIFIED HANDLER restart NIS master services
changed: [frontend001] => {"changed": true, "checksum": "2ee9eeffb1d8e1ae6ce1efd41f9d93f0b0b1555c", "dest": "/var/yp/securenets", "gid": 0, "group": "root", "md5sum": "c0e3e2a08dec1bfad22ad9fe9ec8148c", "mode": "0400", "owner": "root", "size": 707, "src": "/root/.ansible/tmp/ansible-tmp-1524707010.76-17487257715816/source", "state": "file", "uid": 0}

TASK [nis : Deploy `yppasswdd` configuration file (CentOS/RHEL)] ***************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml:12
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
ok: [frontend001] => {"changed": false, "checksum": "18392b4e3ecc4f9e7832c274663b5f6cee00d3f1", "dest": "/etc/sysconfig/yppasswdd", "gid": 0, "group": "root", "mode": "0444", "owner": "root", "path": "/etc/sysconfig/yppasswdd", "size": 681, "state": "file", "uid": 0}

TASK [nis : Install NIS master server packages] ********************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml:22
skipping: [compute001] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}
ok: [frontend001] => (item=[u'ypserv']) => {"changed": false, "item": ["ypserv"], "msg": "", "rc": 0, "results": ["ypserv-2.19-31.el6.x86_64 providing ypserv is already installed"]}

RUNNING HANDLER [nis : restart NIS master services] ****************************
changed: [frontend001] => (item=rpcbind) => {"changed": true, "item": "rpcbind", "name": "rpcbind", "state": "started"}
changed: [frontend001] => (item=yppasswdd) => {"changed": true, "item": "yppasswdd", "name": "yppasswdd", "state": "started"}
changed: [frontend001] => (item=ypserv) => {"changed": true, "item": "ypserv", "name": "ypserv", "state": "started"}

TASK [nis : Ensure `ypserv` starts at boot] ************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml:33
skipping: [compute001] => (item=rpcbind)  => {"changed": false, "item": "rpcbind", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item=rpcbind)  => {"changed": false, "item": "rpcbind", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => (item=yppasswdd)  => {"changed": false, "item": "yppasswdd", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item=yppasswdd)  => {"changed": false, "item": "yppasswdd", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => (item=ypserv)  => {"changed": false, "item": "ypserv", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item=ypserv)  => {"changed": false, "item": "ypserv", "skip_reason": "Conditional check failed", "skipped": true}
ok: [frontend001] => (item=rpcbind) => {"changed": false, "enabled": true, "item": "rpcbind", "name": "rpcbind", "state": "started"}
ok: [frontend001] => (item=yppasswdd) => {"changed": false, "enabled": true, "item": "yppasswdd", "name": "yppasswdd", "state": "started"}
ok: [frontend001] => (item=ypserv) => {"changed": false, "enabled": true, "item": "ypserv", "name": "ypserv", "state": "started"}

TASK [nis : Update NIS/YP databases (NIS master server)] ***********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml:41
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
changed: [frontend001] => {"changed": true, "cmd": ["make"], "delta": "0:00:00.277209", "end": "2018-04-26 09:43:40.670071", "rc": 0, "start": "2018-04-26 09:43:40.392862", "stderr": "", "stdout": "gmake[1]: Entering directory `/var/yp/elasticluster'\nUpdating passwd.byname...\nUpdating passwd.byuid...\nUpdating group.byname...\nUpdating group.bygid...\nUpdating hosts.byname...\nUpdating hosts.byaddr...\nUpdating netid.byname...\ngmake[1]: Leaving directory `/var/yp/elasticluster'", "stdout_lines": ["gmake[1]: Entering directory `/var/yp/elasticluster'", "Updating passwd.byname...", "Updating passwd.byuid...", "Updating group.byname...", "Updating group.bygid...", "Updating hosts.byname...", "Updating hosts.byaddr...", "Updating netid.byname...", "gmake[1]: Leaving directory `/var/yp/elasticluster'"], "warnings": []}

TASK [nis : Deploy `ypbind` configuration files] *******************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypbind.yml:3
skipping: [frontend001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
NOTIFIED HANDLER restart ypbind
changed: [compute002] => {"changed": true, "checksum": "ccafbafdf6830b9f4c1fc505dda7c510da9d48c6", "dest": "/etc/yp.conf", "gid": 0, "group": "root", "md5sum": "174daa74804c3fb22ad4cdf399256011", "mode": "0400", "owner": "root", "size": 725, "src": "/root/.ansible/tmp/ansible-tmp-1524707015.64-174821267426494/source", "state": "file", "uid": 0}
NOTIFIED HANDLER restart ypbind
changed: [compute001] => {"changed": true, "checksum": "ccafbafdf6830b9f4c1fc505dda7c510da9d48c6", "dest": "/etc/yp.conf", "gid": 0, "group": "root", "md5sum": "174daa74804c3fb22ad4cdf399256011", "mode": "0400", "owner": "root", "size": 725, "src": "/root/.ansible/tmp/ansible-tmp-1524707015.63-66576577373952/source", "state": "file", "uid": 0}

TASK [nis : Ensure NIS/YP is used as a name service] ***************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypbind.yml:12
skipping: [frontend001] => (item=passwd)  => {"changed": false, "item": "passwd", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [frontend001] => (item=group)  => {"changed": false, "item": "group", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [frontend001] => (item=shadow)  => {"changed": false, "item": "shadow", "skip_reason": "Conditional check failed", "skipped": true}
changed: [compute001] => (item=passwd) => {"backup": "", "changed": true, "item": "passwd", "msg": "line replaced"}
changed: [compute002] => (item=passwd) => {"backup": "", "changed": true, "item": "passwd", "msg": "line replaced"}
changed: [compute001] => (item=group) => {"backup": "", "changed": true, "item": "group", "msg": "line replaced"}
changed: [compute002] => (item=group) => {"backup": "", "changed": true, "item": "group", "msg": "line replaced"}
changed: [compute001] => (item=shadow) => {"backup": "", "changed": true, "item": "shadow", "msg": "line replaced"}
changed: [compute002] => (item=shadow) => {"backup": "", "changed": true, "item": "shadow", "msg": "line replaced"}

TASK [nis : Replace `compat` in `/etc/nsswitch.conf` with `files nis`] *********
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypbind.yml:26
skipping: [frontend001] => (item=passwd)  => {"changed": false, "item": "passwd", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [frontend001] => (item=group)  => {"changed": false, "item": "group", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [frontend001] => (item=shadow)  => {"changed": false, "item": "shadow", "skip_reason": "Conditional check failed", "skipped": true}
ok: [compute001] => (item=passwd) => {"changed": false, "item": "passwd", "msg": ""}
ok: [compute002] => (item=passwd) => {"changed": false, "item": "passwd", "msg": ""}
ok: [compute002] => (item=group) => {"changed": false, "item": "group", "msg": ""}
ok: [compute001] => (item=group) => {"changed": false, "item": "group", "msg": ""}
ok: [compute001] => (item=shadow) => {"changed": false, "item": "shadow", "msg": ""}
ok: [compute002] => (item=shadow) => {"changed": false, "item": "shadow", "msg": ""}

TASK [nis : Ensure `ypbind` starts at boot] ************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypbind.yml:37
skipping: [frontend001] => (item=rpcbind)  => {"changed": false, "item": "rpcbind", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [frontend001] => (item=ypbind)  => {"changed": false, "item": "ypbind", "skip_reason": "Conditional check failed", "skipped": true}
ok: [compute001] => (item=rpcbind) => {"changed": false, "enabled": true, "item": "rpcbind", "name": "rpcbind", "state": "started"}
ok: [compute002] => (item=rpcbind) => {"changed": false, "enabled": true, "item": "rpcbind", "name": "rpcbind", "state": "started"}
changed: [compute001] => (item=ypbind) => {"changed": true, "enabled": true, "item": "ypbind", "name": "ypbind", "state": "started"}
changed: [compute002] => (item=ypbind) => {"changed": true, "enabled": true, "item": "ypbind", "name": "ypbind", "state": "started"}

RUNNING HANDLER [nis : restart ypbind] *****************************************
changed: [compute002] => (item=rpcbind) => {"changed": true, "item": "rpcbind", "name": "rpcbind", "state": "started"}
changed: [compute001] => (item=rpcbind) => {"changed": true, "item": "rpcbind", "name": "rpcbind", "state": "started"}
changed: [compute002] => (item=ypbind) => {"changed": true, "item": "ypbind", "name": "ypbind", "state": "started"}
changed: [compute001] => (item=ypbind) => {"changed": true, "item": "ypbind", "name": "ypbind", "state": "started"}

TASK [nfs-client : install NFS client software (Debian/Ubuntu)] ****************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-client/tasks/main.yml:3
skipping: [frontend001] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}

TASK [nfs-client : install NFS client software (RHEL-compatible)] **************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-client/tasks/main.yml:15
ok: [compute001] => (item=[u'nfs-utils', u'nfs4-acl-tools']) => {"changed": false, "item": ["nfs-utils", "nfs4-acl-tools"], "msg": "", "rc": 0, "results": ["nfs-utils-1:1.2.3-75.el6_9.x86_64 providing nfs-utils is already installed", "nfs4-acl-tools-0.3.3-8.el6.x86_64 providing nfs4-acl-tools is already installed"]}
ok: [frontend001] => (item=[u'nfs-utils', u'nfs4-acl-tools']) => {"changed": false, "item": ["nfs-utils", "nfs4-acl-tools"], "msg": "", "rc": 0, "results": ["nfs-utils-1:1.2.3-75.el6_9.x86_64 providing nfs-utils is already installed", "nfs4-acl-tools-0.3.3-8.el6.x86_64 providing nfs4-acl-tools is already installed"]}
ok: [compute002] => (item=[u'nfs-utils', u'nfs4-acl-tools']) => {"changed": false, "item": ["nfs-utils", "nfs4-acl-tools"], "msg": "", "rc": 0, "results": ["nfs-utils-1:1.2.3-75.el6_9.x86_64 providing nfs-utils is already installed", "nfs4-acl-tools-0.3.3-8.el6.x86_64 providing nfs4-acl-tools is already installed"]}

TASK [nfs-client : Ensure `rpcbind` is running (Debian)] ***********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-client/tasks/main.yml:28
skipping: [frontend001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [nfs-client : Ensure `rpcbind` is running (RHEL-compatible)] **************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-client/tasks/main.yml:35
ok: [compute001] => {"changed": false, "enabled": true, "name": "rpcbind", "state": "started"}
ok: [frontend001] => {"changed": false, "enabled": true, "name": "rpcbind", "state": "started"}
ok: [compute002] => {"changed": false, "enabled": true, "name": "rpcbind", "state": "started"}

TASK [nfs-client : Ensure `portmap` is running (Ubuntu prior to 14.04)] ********
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-client/tasks/main.yml:42
skipping: [frontend001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [nfs-client : Ensure `rpcbind` is running (Ubuntu 14.04 or newer)] ********
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-client/tasks/main.yml:49
skipping: [frontend001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [nfs-client : Mount NFS filesystems] **************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-client/tasks/main.yml:57
included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-client/tasks/nfsmount.yml for frontend001, compute001, compute002

TASK [nfs-client : ensure /sfs directory exists] *******************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-client/tasks/nfsmount.yml:3
changed: [frontend001] => {"changed": true, "gid": 0, "group": "root", "mode": "0755", "owner": "root", "path": "/sfs", "size": 4096, "state": "directory", "uid": 0}
changed: [compute001] => {"changed": true, "gid": 0, "group": "root", "mode": "0755", "owner": "root", "path": "/sfs", "size": 4096, "state": "directory", "uid": 0}
changed: [compute002] => {"changed": true, "gid": 0, "group": "root", "mode": "0755", "owner": "root", "path": "/sfs", "size": 4096, "state": "directory", "uid": 0}

TASK [nfs-client : add to /etc/fstab] ******************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-client/tasks/nfsmount.yml:8
changed: [compute001] => {"changed": true, "dump": "0", "fstab": "/etc/fstab", "fstype": "nfs", "name": "/sfs", "opts": "rw,async", "passno": "0", "src": "sfs-nas1.cn-north-1.myhuaweicloud.com:/share-480fca1b"}
changed: [compute002] => {"changed": true, "dump": "0", "fstab": "/etc/fstab", "fstype": "nfs", "name": "/sfs", "opts": "rw,async", "passno": "0", "src": "sfs-nas1.cn-north-1.myhuaweicloud.com:/share-480fca1b"}
changed: [frontend001] => {"changed": true, "dump": "0", "fstab": "/etc/fstab", "fstype": "nfs", "name": "/sfs", "opts": "rw,async", "passno": "0", "src": "sfs-nas1.cn-north-1.myhuaweicloud.com:/share-480fca1b"}

TASK [autofs : Load distribution-specific parameters] **************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/autofs/tasks/main.yml:8
included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/autofs/tasks/init-RedHat.yml for frontend001, compute001, compute002

TASK [autofs : Provide RHEL-specific values] ***********************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/autofs/tasks/init-RedHat.yml:6
ok: [frontend001] => {"ansible_facts": {"autofs_packages": ["autofs"], "autofs_service": "autofs", "autofs_username": "root"}, "changed": false}
ok: [compute001] => {"ansible_facts": {"autofs_packages": ["autofs"], "autofs_service": "autofs", "autofs_username": "root"}, "changed": false}
ok: [compute002] => {"ansible_facts": {"autofs_packages": ["autofs"], "autofs_service": "autofs", "autofs_username": "root"}, "changed": false}

TASK [autofs : Deploy autofs configuration] ************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/autofs/tasks/main.yml:12
changed: [frontend001] => (item=etc/auto.master) => {"changed": true, "checksum": "758d13b8a76f743e400c0badc6157e8327f1a8f1", "dest": "/etc/auto.master", "gid": 0, "group": "root", "item": "etc/auto.master", "md5sum": "e3d05b4f4a066db7039fed20a9574a94", "mode": "0444", "owner": "root", "size": 540, "src": "/root/.ansible/tmp/ansible-tmp-1524707028.99-207164313971903/source", "state": "file", "uid": 0}
changed: [compute001] => (item=etc/auto.master) => {"changed": true, "checksum": "758d13b8a76f743e400c0badc6157e8327f1a8f1", "dest": "/etc/auto.master", "gid": 0, "group": "root", "item": "etc/auto.master", "md5sum": "e3d05b4f4a066db7039fed20a9574a94", "mode": "0444", "owner": "root", "size": 540, "src": "/root/.ansible/tmp/ansible-tmp-1524707029.0-62580071599372/source", "state": "file", "uid": 0}
changed: [compute002] => (item=etc/auto.master) => {"changed": true, "checksum": "758d13b8a76f743e400c0badc6157e8327f1a8f1", "dest": "/etc/auto.master", "gid": 0, "group": "root", "item": "etc/auto.master", "md5sum": "e3d05b4f4a066db7039fed20a9574a94", "mode": "0444", "owner": "root", "size": 540, "src": "/root/.ansible/tmp/ansible-tmp-1524707029.02-250051950768156/source", "state": "file", "uid": 0}
changed: [frontend001] => (item=etc/auto.home) => {"changed": true, "checksum": "df9645aa3e6a41b1aece0b5867b79d0c9c16f430", "dest": "/etc/auto.home", "gid": 0, "group": "root", "item": "etc/auto.home", "md5sum": "271d38d13da93386ff2c00c836c5608b", "mode": "0444", "owner": "root", "size": 852, "src": "/root/.ansible/tmp/ansible-tmp-1524707029.61-142773235696822/source", "state": "file", "uid": 0}
changed: [compute002] => (item=etc/auto.home) => {"changed": true, "checksum": "df9645aa3e6a41b1aece0b5867b79d0c9c16f430", "dest": "/etc/auto.home", "gid": 0, "group": "root", "item": "etc/auto.home", "md5sum": "271d38d13da93386ff2c00c836c5608b", "mode": "0444", "owner": "root", "size": 852, "src": "/root/.ansible/tmp/ansible-tmp-1524707029.65-28700255478302/source", "state": "file", "uid": 0}
changed: [compute001] => (item=etc/auto.home) => {"changed": true, "checksum": "df9645aa3e6a41b1aece0b5867b79d0c9c16f430", "dest": "/etc/auto.home", "gid": 0, "group": "root", "item": "etc/auto.home", "md5sum": "271d38d13da93386ff2c00c836c5608b", "mode": "0444", "owner": "root", "size": 852, "src": "/root/.ansible/tmp/ansible-tmp-1524707029.64-19012215139455/source", "state": "file", "uid": 0}

TASK [autofs : Deploy autofs mount script for NFSv4 (Debian/Ubuntu)] ***********
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/autofs/tasks/main.yml:24
skipping: [frontend001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [autofs : Install Autofs] *************************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/autofs/tasks/main.yml:34
ok: [frontend001] => (item=[u'autofs']) => {"changed": false, "item": ["autofs"], "msg": "", "rc": 0, "results": ["autofs-1:5.0.5-133.el6_9.x86_64 providing autofs is already installed"]}
ok: [compute001] => (item=[u'autofs']) => {"changed": false, "item": ["autofs"], "msg": "", "rc": 0, "results": ["autofs-1:5.0.5-133.el6_9.x86_64 providing autofs is already installed"]}
ok: [compute002] => (item=[u'autofs']) => {"changed": false, "item": ["autofs"], "msg": "", "rc": 0, "results": ["autofs-1:5.0.5-133.el6_9.x86_64 providing autofs is already installed"]}

TASK [autofs : Ensure autofs is running and starts at boot] ********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/autofs/tasks/main.yml:41
ok: [frontend001] => {"changed": false, "enabled": true, "name": "autofs", "state": "started"}
ok: [compute001] => {"changed": false, "enabled": true, "name": "autofs", "state": "started"}
ok: [compute002] => {"changed": false, "enabled": true, "name": "autofs", "state": "started"}

TASK [slurm-common : Load distribution-specific parameters] ********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:3
included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/init-RedHat.yml for frontend001, compute001, compute002

TASK [slurm-common : Set SLURM common playbook params (RHEL 7.x compatible)] ***
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/init-RedHat.yml:3
ok: [frontend001] => {"ansible_facts": {"slurm_pid_dir": "/var/run"}, "changed": false}
ok: [compute001] => {"ansible_facts": {"slurm_pid_dir": "/var/run"}, "changed": false}
ok: [compute002] => {"ansible_facts": {"slurm_pid_dir": "/var/run"}, "changed": false}

TASK [slurm-common : Create `slurm` system group] ******************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:16
ok: [compute002] => {"changed": false, "gid": 498, "name": "slurm", "state": "present", "system": true}
ok: [compute001] => {"changed": false, "gid": 498, "name": "slurm", "state": "present", "system": true}
ok: [frontend001] => {"changed": false, "gid": 498, "name": "slurm", "state": "present", "system": true}

TASK [slurm-common : Create `slurm` system user] *******************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:24
ok: [compute002] => {"append": false, "changed": false, "comment": "", "group": 498, "home": "/home/slurm", "move_home": false, "name": "slurm", "shell": "/bin/bash", "state": "present", "uid": 498}
ok: [compute001] => {"append": false, "changed": false, "comment": "", "group": 498, "home": "/home/slurm", "move_home": false, "name": "slurm", "shell": "/bin/bash", "state": "present", "uid": 498}
ok: [frontend001] => {"append": false, "changed": false, "comment": "", "group": 498, "home": "/home/slurm", "move_home": false, "name": "slurm", "shell": "/bin/bash", "state": "present", "uid": 498}

TASK [slurm-common : Create work directory {{item}}] ***************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:33
ok: [frontend001] => (item=/etc/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/etc/slurm", "mode": "0755", "owner": "slurm", "path": "/etc/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [compute001] => (item=/etc/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/etc/slurm", "mode": "0755", "owner": "slurm", "path": "/etc/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [compute002] => (item=/etc/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/etc/slurm", "mode": "0755", "owner": "slurm", "path": "/etc/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [frontend001] => (item=/var/lib/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/lib/slurm", "mode": "0755", "owner": "slurm", "path": "/var/lib/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [compute001] => (item=/var/lib/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/lib/slurm", "mode": "0755", "owner": "slurm", "path": "/var/lib/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [compute002] => (item=/var/lib/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/lib/slurm", "mode": "0755", "owner": "slurm", "path": "/var/lib/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [frontend001] => (item=/var/lib/slurm/checkpoint) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/lib/slurm/checkpoint", "mode": "0755", "owner": "slurm", "path": "/var/lib/slurm/checkpoint", "size": 4096, "state": "directory", "uid": 498}
ok: [compute001] => (item=/var/lib/slurm/checkpoint) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/lib/slurm/checkpoint", "mode": "0755", "owner": "slurm", "path": "/var/lib/slurm/checkpoint", "size": 4096, "state": "directory", "uid": 498}
ok: [compute002] => (item=/var/lib/slurm/checkpoint) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/lib/slurm/checkpoint", "mode": "0755", "owner": "slurm", "path": "/var/lib/slurm/checkpoint", "size": 4096, "state": "directory", "uid": 498}
ok: [frontend001] => (item=/var/lib/slurm/slurmctld) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/lib/slurm/slurmctld", "mode": "0755", "owner": "slurm", "path": "/var/lib/slurm/slurmctld", "size": 4096, "state": "directory", "uid": 498}
ok: [compute002] => (item=/var/lib/slurm/slurmctld) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/lib/slurm/slurmctld", "mode": "0755", "owner": "slurm", "path": "/var/lib/slurm/slurmctld", "size": 4096, "state": "directory", "uid": 498}
ok: [compute001] => (item=/var/lib/slurm/slurmctld) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/lib/slurm/slurmctld", "mode": "0755", "owner": "slurm", "path": "/var/lib/slurm/slurmctld", "size": 4096, "state": "directory", "uid": 498}
ok: [frontend001] => (item=/var/lib/slurm/slurmd) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/lib/slurm/slurmd", "mode": "0755", "owner": "slurm", "path": "/var/lib/slurm/slurmd", "size": 4096, "state": "directory", "uid": 498}
ok: [compute002] => (item=/var/lib/slurm/slurmd) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/lib/slurm/slurmd", "mode": "0755", "owner": "slurm", "path": "/var/lib/slurm/slurmd", "size": 4096, "state": "directory", "uid": 498}
ok: [compute001] => (item=/var/lib/slurm/slurmd) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/lib/slurm/slurmd", "mode": "0755", "owner": "slurm", "path": "/var/lib/slurm/slurmd", "size": 4096, "state": "directory", "uid": 498}
ok: [frontend001] => (item=/var/log/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/log/slurm", "mode": "0755", "owner": "slurm", "path": "/var/log/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [compute001] => (item=/var/log/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/log/slurm", "mode": "0755", "owner": "slurm", "path": "/var/log/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [compute002] => (item=/var/log/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/log/slurm", "mode": "0755", "owner": "slurm", "path": "/var/log/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [frontend001] => (item=/var/run/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/run/slurm", "mode": "0755", "owner": "slurm", "path": "/var/run/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [compute002] => (item=/var/run/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/run/slurm", "mode": "0755", "owner": "slurm", "path": "/var/run/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [compute001] => (item=/var/run/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/run/slurm", "mode": "0755", "owner": "slurm", "path": "/var/run/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [frontend001] => (item=/var/spool/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/spool/slurm", "mode": "0755", "owner": "slurm", "path": "/var/spool/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [compute002] => (item=/var/spool/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/spool/slurm", "mode": "0755", "owner": "slurm", "path": "/var/spool/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [compute001] => (item=/var/spool/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/spool/slurm", "mode": "0755", "owner": "slurm", "path": "/var/spool/slurm", "size": 4096, "state": "directory", "uid": 498}

TASK [slurm-common : Make compatibility symlinks] ******************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:56
skipping: [frontend001] => (item={u'to': u'/etc/slurm-llnl', u'from': u'/etc/slurm'})  => {"changed": false, "item": {"from": "/etc/slurm", "to": "/etc/slurm-llnl"}, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [frontend001] => (item={u'to': u'/var/lib/slurm-llnl', u'from': u'/var/lib/slurm'})  => {"changed": false, "item": {"from": "/var/lib/slurm", "to": "/var/lib/slurm-llnl"}, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => (item={u'to': u'/etc/slurm-llnl', u'from': u'/etc/slurm'})  => {"changed": false, "item": {"from": "/etc/slurm", "to": "/etc/slurm-llnl"}, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [frontend001] => (item={u'to': u'/var/log/slurm-llnl', u'from': u'/var/log/slurm'})  => {"changed": false, "item": {"from": "/var/log/slurm", "to": "/var/log/slurm-llnl"}, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => (item={u'to': u'/var/lib/slurm-llnl', u'from': u'/var/lib/slurm'})  => {"changed": false, "item": {"from": "/var/lib/slurm", "to": "/var/lib/slurm-llnl"}, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item={u'to': u'/etc/slurm-llnl', u'from': u'/etc/slurm'})  => {"changed": false, "item": {"from": "/etc/slurm", "to": "/etc/slurm-llnl"}, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => (item={u'to': u'/var/log/slurm-llnl', u'from': u'/var/log/slurm'})  => {"changed": false, "item": {"from": "/var/log/slurm", "to": "/var/log/slurm-llnl"}, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [frontend001] => (item={u'to': u'/var/run/slurm-llnl', u'from': u'/var/run/slurm'})  => {"changed": false, "item": {"from": "/var/run/slurm", "to": "/var/run/slurm-llnl"}, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item={u'to': u'/var/lib/slurm-llnl', u'from': u'/var/lib/slurm'})  => {"changed": false, "item": {"from": "/var/lib/slurm", "to": "/var/lib/slurm-llnl"}, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => (item={u'to': u'/var/run/slurm-llnl', u'from': u'/var/run/slurm'})  => {"changed": false, "item": {"from": "/var/run/slurm", "to": "/var/run/slurm-llnl"}, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item={u'to': u'/var/log/slurm-llnl', u'from': u'/var/log/slurm'})  => {"changed": false, "item": {"from": "/var/log/slurm", "to": "/var/log/slurm-llnl"}, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item={u'to': u'/var/run/slurm-llnl', u'from': u'/var/run/slurm'})  => {"changed": false, "item": {"from": "/var/run/slurm", "to": "/var/run/slurm-llnl"}, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-common : Enable Copr SLURM repo by verdurin] ***********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:73
changed: [compute001] => {"changed": true, "checksum": "18960fef3cdfefe9d932e02cb1791c9c43780896", "dest": "/etc/yum.repos.d/copr-slurm.repo", "gid": 0, "group": "root", "md5sum": "9d472c764f0985eec79d4192598f2d06", "mode": "0444", "owner": "root", "size": 615, "src": "/root/.ansible/tmp/ansible-tmp-1524707033.87-72348655605500/source", "state": "file", "uid": 0}
changed: [compute002] => {"changed": true, "checksum": "18960fef3cdfefe9d932e02cb1791c9c43780896", "dest": "/etc/yum.repos.d/copr-slurm.repo", "gid": 0, "group": "root", "md5sum": "9d472c764f0985eec79d4192598f2d06", "mode": "0444", "owner": "root", "size": 615, "src": "/root/.ansible/tmp/ansible-tmp-1524707033.89-251837915863540/source", "state": "file", "uid": 0}
changed: [frontend001] => {"changed": true, "checksum": "18960fef3cdfefe9d932e02cb1791c9c43780896", "dest": "/etc/yum.repos.d/copr-slurm.repo", "gid": 0, "group": "root", "md5sum": "9d472c764f0985eec79d4192598f2d06", "mode": "0444", "owner": "root", "size": 615, "src": "/root/.ansible/tmp/ansible-tmp-1524707033.86-138853762384738/source", "state": "file", "uid": 0}

TASK [slurm-common : Prevent MUNGE from starting] ******************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:20
skipping: [frontend001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-common : Install MUNGE (Debian/Ubuntu)] ****************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:31
skipping: [frontend001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-common : Install MUNGE (RHEL-compatible)] **************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:43
changed: [compute002] => {"changed": true, "msg": "Warning: RPMDB altered outside of yum.\n", "rc": 0, "results": ["Loaded plugins: fastestmirror, security\nSetting up Install Process\nLoading mirror speeds from cached hostfile\nResolving Dependencies\n--> Running transaction check\n---> Package slurm-munge.x86_64 0:17.02.7-1.el6 will be installed\n--> Processing Dependency: slurm for package: slurm-munge-17.02.7-1.el6.x86_64\n--> Running transaction check\n---> Package slurm.x86_64 0:17.02.7-1.el6 will be installed\n--> Processing Dependency: slurm-plugins for package: slurm-17.02.7-1.el6.x86_64\n--> Running transaction check\n---> Package slurm-plugins.x86_64 0:17.02.7-1.el6 will be installed\n--> Finished Dependency Resolution\n\nDependencies Resolved\n\n================================================================================\n Package             Arch         Version               Repository         Size\n================================================================================\nInstalling:\n slurm-munge         x86_64       17.02.7-1.el6         local-slurm        16 k\nInstalling for dependencies:\n slurm               x86_64       17.02.7-1.el6         local-slurm        27 M\n slurm-plugins       x86_64       17.02.7-1.el6         local-slurm       1.1 M\n\nTransaction Summary\n================================================================================\nInstall       3 Package(s)\n\nTotal download size: 28 M\nInstalled size: 98 M\nDownloading Packages:\n--------------------------------------------------------------------------------\nTotal                                           106 MB/s |  28 MB     00:00     \nRunning rpm_check_debug\nRunning Transaction Test\nTransaction Test Succeeded\nRunning Transaction\n\r  Installing : slurm-plugins-17.02.7-1.el6.x86_64                           1/3 \n\r  Installing : slurm-17.02.7-1.el6.x86_64                                   2/3 \n\r  Installing : slurm-munge-17.02.7-1.el6.x86_64                             3/3 \n\r  Verifying  : slurm-plugins-17.02.7-1.el6.x86_64                           1/3 \n\r  Verifying  : slurm-17.02.7-1.el6.x86_64                                   2/3 \n\r  Verifying  : slurm-munge-17.02.7-1.el6.x86_64                             3/3 \n\nInstalled:\n  slurm-munge.x86_64 0:17.02.7-1.el6                                            \n\nDependency Installed:\n  slurm.x86_64 0:17.02.7-1.el6       slurm-plugins.x86_64 0:17.02.7-1.el6      \n\nComplete!\n"]}
changed: [frontend001] => {"changed": true, "msg": "Warning: RPMDB altered outside of yum.\n", "rc": 0, "results": ["Loaded plugins: fastestmirror, security\nSetting up Install Process\nLoading mirror speeds from cached hostfile\nResolving Dependencies\n--> Running transaction check\n---> Package slurm-munge.x86_64 0:17.02.7-1.el6 will be installed\n--> Processing Dependency: slurm for package: slurm-munge-17.02.7-1.el6.x86_64\n--> Running transaction check\n---> Package slurm.x86_64 0:17.02.7-1.el6 will be installed\n--> Processing Dependency: slurm-plugins for package: slurm-17.02.7-1.el6.x86_64\n--> Running transaction check\n---> Package slurm-plugins.x86_64 0:17.02.7-1.el6 will be installed\n--> Finished Dependency Resolution\n\nDependencies Resolved\n\n================================================================================\n Package             Arch         Version               Repository         Size\n================================================================================\nInstalling:\n slurm-munge         x86_64       17.02.7-1.el6         local-slurm        16 k\nInstalling for dependencies:\n slurm               x86_64       17.02.7-1.el6         local-slurm        27 M\n slurm-plugins       x86_64       17.02.7-1.el6         local-slurm       1.1 M\n\nTransaction Summary\n================================================================================\nInstall       3 Package(s)\n\nTotal download size: 28 M\nInstalled size: 98 M\nDownloading Packages:\n--------------------------------------------------------------------------------\nTotal                                            97 MB/s |  28 MB     00:00     \nRunning rpm_check_debug\nRunning Transaction Test\nTransaction Test Succeeded\nRunning Transaction\n\r  Installing : slurm-plugins-17.02.7-1.el6.x86_64                           1/3 \n\r  Installing : slurm-17.02.7-1.el6.x86_64                                   2/3 \n\r  Installing : slurm-munge-17.02.7-1.el6.x86_64                             3/3 \n\r  Verifying  : slurm-plugins-17.02.7-1.el6.x86_64                           1/3 \n\r  Verifying  : slurm-17.02.7-1.el6.x86_64                                   2/3 \n\r  Verifying  : slurm-munge-17.02.7-1.el6.x86_64                             3/3 \n\nInstalled:\n  slurm-munge.x86_64 0:17.02.7-1.el6                                            \n\nDependency Installed:\n  slurm.x86_64 0:17.02.7-1.el6       slurm-plugins.x86_64 0:17.02.7-1.el6      \n\nComplete!\n"]}
changed: [compute001] => {"changed": true, "msg": "Warning: RPMDB altered outside of yum.\n", "rc": 0, "results": ["Loaded plugins: fastestmirror, security\nSetting up Install Process\nLoading mirror speeds from cached hostfile\nResolving Dependencies\n--> Running transaction check\n---> Package slurm-munge.x86_64 0:17.02.7-1.el6 will be installed\n--> Processing Dependency: slurm for package: slurm-munge-17.02.7-1.el6.x86_64\n--> Running transaction check\n---> Package slurm.x86_64 0:17.02.7-1.el6 will be installed\n--> Processing Dependency: slurm-plugins for package: slurm-17.02.7-1.el6.x86_64\n--> Running transaction check\n---> Package slurm-plugins.x86_64 0:17.02.7-1.el6 will be installed\n--> Finished Dependency Resolution\n\nDependencies Resolved\n\n================================================================================\n Package             Arch         Version               Repository         Size\n================================================================================\nInstalling:\n slurm-munge         x86_64       17.02.7-1.el6         local-slurm        16 k\nInstalling for dependencies:\n slurm               x86_64       17.02.7-1.el6         local-slurm        27 M\n slurm-plugins       x86_64       17.02.7-1.el6         local-slurm       1.1 M\n\nTransaction Summary\n================================================================================\nInstall       3 Package(s)\n\nTotal download size: 28 M\nInstalled size: 98 M\nDownloading Packages:\n--------------------------------------------------------------------------------\nTotal                                            85 MB/s |  28 MB     00:00     \nRunning rpm_check_debug\nRunning Transaction Test\nTransaction Test Succeeded\nRunning Transaction\n\r  Installing : slurm-plugins-17.02.7-1.el6.x86_64                           1/3 \n\r  Installing : slurm-17.02.7-1.el6.x86_64                                   2/3 \n\r  Installing : slurm-munge-17.02.7-1.el6.x86_64                             3/3 \n\r  Verifying  : slurm-plugins-17.02.7-1.el6.x86_64                           1/3 \n\r  Verifying  : slurm-17.02.7-1.el6.x86_64                                   2/3 \n\r  Verifying  : slurm-munge-17.02.7-1.el6.x86_64                             3/3 \n\nInstalled:\n  slurm-munge.x86_64 0:17.02.7-1.el6                                            \n\nDependency Installed:\n  slurm.x86_64 0:17.02.7-1.el6       slurm-plugins.x86_64 0:17.02.7-1.el6      \n\nComplete!\n"]}

TASK [slurm-common : Allow MUNGE to start] *************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:57
skipping: [frontend001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-common : Add `--syslog` option to `munged` startup] ****************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:66
skipping: [frontend001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-common : Reload systemd configuration] *****************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:77
skipping: [frontend001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-common : Add `--syslog` startup options for `munged` (Ubuntu 14.04)] ***
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:86
skipping: [frontend001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-common : Configure MUNGE] ******************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:101
ok: [compute001] => {"changed": false, "checksum": "222afe887f907039b799b394715664ffa584fec5", "dest": "/etc/munge/munge.key", "gid": 497, "group": "munge", "mode": "0400", "owner": "munge", "path": "/etc/munge/munge.key", "size": 1024, "state": "file", "uid": 497}
ok: [compute002] => {"changed": false, "checksum": "222afe887f907039b799b394715664ffa584fec5", "dest": "/etc/munge/munge.key", "gid": 497, "group": "munge", "mode": "0400", "owner": "munge", "path": "/etc/munge/munge.key", "size": 1024, "state": "file", "uid": 497}
ok: [frontend001] => {"changed": false, "checksum": "222afe887f907039b799b394715664ffa584fec5", "dest": "/etc/munge/munge.key", "gid": 497, "group": "munge", "mode": "0400", "owner": "munge", "path": "/etc/munge/munge.key", "size": 1024, "state": "file", "uid": 497}

TASK [slurm-common : Ensure the MUNGE service is running] **********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:114
ok: [frontend001] => {"changed": false, "enabled": true, "name": "munge", "state": "started"}
ok: [compute002] => {"changed": false, "enabled": true, "name": "munge", "state": "started"}
ok: [compute001] => {"changed": false, "enabled": true, "name": "munge", "state": "started"}

TASK [slurm-common : Deploy SLURM configuration file] **************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:86
changed: [frontend001] => {"changed": true, "checksum": "f1a62dfbfeed8cfb3bb74ea1e7fc03448bf1cc2c", "dest": "/etc/slurm/slurm.conf", "gid": 0, "group": "root", "md5sum": "e0604eb1c720625a6ea9f1c83d2c1e70", "mode": "0444", "owner": "root", "size": 4156, "src": "/root/.ansible/tmp/ansible-tmp-1524707051.19-31945884291369/source", "state": "file", "uid": 0}
changed: [compute002] => {"changed": true, "checksum": "f1a62dfbfeed8cfb3bb74ea1e7fc03448bf1cc2c", "dest": "/etc/slurm/slurm.conf", "gid": 0, "group": "root", "md5sum": "e0604eb1c720625a6ea9f1c83d2c1e70", "mode": "0444", "owner": "root", "size": 4156, "src": "/root/.ansible/tmp/ansible-tmp-1524707051.22-212447737583107/source", "state": "file", "uid": 0}
changed: [compute001] => {"changed": true, "checksum": "f1a62dfbfeed8cfb3bb74ea1e7fc03448bf1cc2c", "dest": "/etc/slurm/slurm.conf", "gid": 0, "group": "root", "md5sum": "e0604eb1c720625a6ea9f1c83d2c1e70", "mode": "0444", "owner": "root", "size": 4156, "src": "/root/.ansible/tmp/ansible-tmp-1524707051.21-196019650672805/source", "state": "file", "uid": 0}

TASK [slurm-common : Deploy script to suspend cloud instance] ******************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:96
changed: [compute001] => {"changed": true, "checksum": "495068227fbc18ec696164fac24000f9d68d21a9", "dest": "/etc/slurm/slurm.suspend.sh", "gid": 0, "group": "root", "md5sum": "be36c50de6d76984cb4c6450da33e6c4", "mode": "0555", "owner": "root", "size": 604, "src": "/root/.ansible/tmp/ansible-tmp-1524707051.93-145197534223407/source", "state": "file", "uid": 0}
changed: [compute002] => {"changed": true, "checksum": "495068227fbc18ec696164fac24000f9d68d21a9", "dest": "/etc/slurm/slurm.suspend.sh", "gid": 0, "group": "root", "md5sum": "be36c50de6d76984cb4c6450da33e6c4", "mode": "0555", "owner": "root", "size": 604, "src": "/root/.ansible/tmp/ansible-tmp-1524707051.97-37487982077628/source", "state": "file", "uid": 0}
changed: [frontend001] => {"changed": true, "checksum": "495068227fbc18ec696164fac24000f9d68d21a9", "dest": "/etc/slurm/slurm.suspend.sh", "gid": 0, "group": "root", "md5sum": "be36c50de6d76984cb4c6450da33e6c4", "mode": "0555", "owner": "root", "size": 604, "src": "/root/.ansible/tmp/ansible-tmp-1524707051.91-14689513992663/source", "state": "file", "uid": 0}

TASK [slurm-common : Deploy script to resume cloud instance] *******************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:106
changed: [compute002] => {"changed": true, "checksum": "3d83789e9d9065abeef8a6a493a8e7a165f3af37", "dest": "/etc/slurm/slurm.resume.sh", "gid": 0, "group": "root", "md5sum": "475dc8a8fa2b3322148e8200b4425a84", "mode": "0555", "owner": "root", "size": 631, "src": "/root/.ansible/tmp/ansible-tmp-1524707052.71-62819111444704/source", "state": "file", "uid": 0}
changed: [frontend001] => {"changed": true, "checksum": "3d83789e9d9065abeef8a6a493a8e7a165f3af37", "dest": "/etc/slurm/slurm.resume.sh", "gid": 0, "group": "root", "md5sum": "475dc8a8fa2b3322148e8200b4425a84", "mode": "0555", "owner": "root", "size": 631, "src": "/root/.ansible/tmp/ansible-tmp-1524707052.68-272858903611323/source", "state": "file", "uid": 0}
changed: [compute001] => {"changed": true, "checksum": "3d83789e9d9065abeef8a6a493a8e7a165f3af37", "dest": "/etc/slurm/slurm.resume.sh", "gid": 0, "group": "root", "md5sum": "475dc8a8fa2b3322148e8200b4425a84", "mode": "0555", "owner": "root", "size": 631, "src": "/root/.ansible/tmp/ansible-tmp-1524707052.69-84732559931790/source", "state": "file", "uid": 0}

TASK [slurm-common : Install support packages (Debian/Ubuntu)] *****************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:116
skipping: [frontend001] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-common : Install SLURM packages (RHEL-compatible)] *****************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:127
skipping: [frontend001] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-client : Install required SLURM packages (Debian/Ubuntu)] **********
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-client/tasks/main.yml:4
skipping: [frontend001] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-client : Install required SLURM packages (older Debian/Ubuntu)] ****
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-client/tasks/main.yml:23
skipping: [frontend001] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-client : service] **************************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-client/tasks/main.yml:35
skipping: [frontend001] => (item=slurm-llnl)  => {"changed": false, "item": "slurm-llnl", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => (item=slurm-llnl)  => {"changed": false, "item": "slurm-llnl", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item=slurm-llnl)  => {"changed": false, "item": "slurm-llnl", "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-client : Install SLURM packages (RHEL-compatible)] *****************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-client/tasks/main.yml:47
changed: [compute002] => (item=[u'slurm', u'slurm-devel', u'slurm-perlapi', u'slurm-contribs', u'slurm-torque']) => {"changed": true, "item": ["slurm", "slurm-devel", "slurm-perlapi", "slurm-contribs", "slurm-torque"], "msg": "", "rc": 0, "results": ["slurm-17.02.7-1.el6.x86_64 providing slurm is already installed", "Loaded plugins: fastestmirror, security\nSetting up Install Process\nLoading mirror speeds from cached hostfile\nResolving Dependencies\n--> Running transaction check\n---> Package slurm-contribs.x86_64 0:17.02.7-1.el6 will be installed\n---> Package slurm-devel.x86_64 0:17.02.7-1.el6 will be installed\n---> Package slurm-perlapi.x86_64 0:17.02.7-1.el6 will be installed\n---> Package slurm-torque.x86_64 0:17.02.7-1.el6 will be installed\n--> Finished Dependency Resolution\n\nDependencies Resolved\n\n================================================================================\n Package              Arch         Version              Repository         Size\n================================================================================\nInstalling:\n slurm-contribs       x86_64       17.02.7-1.el6        local-slurm        17 k\n slurm-devel          x86_64       17.02.7-1.el6        local-slurm       143 k\n slurm-perlapi        x86_64       17.02.7-1.el6        local-slurm       456 k\n slurm-torque         x86_64       17.02.7-1.el6        local-slurm        37 k\n\nTransaction Summary\n================================================================================\nInstall       4 Package(s)\n\nTotal download size: 653 k\nInstalled size: 2.5 M\nDownloading Packages:\n--------------------------------------------------------------------------------\nTotal                                            52 MB/s | 653 kB     00:00     \nRunning rpm_check_debug\nRunning Transaction Test\nTransaction Test Succeeded\nRunning Transaction\n\r  Installing : slurm-perlapi-17.02.7-1.el6.x86_64                           1/4 \n\r  Installing : slurm-torque-17.02.7-1.el6.x86_64                            2/4 \n\r  Installing : slurm-contribs-17.02.7-1.el6.x86_64                          3/4 \n\r  Installing : slurm-devel-17.02.7-1.el6.x86_64                             4/4 \n\r  Verifying  : slurm-perlapi-17.02.7-1.el6.x86_64                           1/4 \n\r  Verifying  : slurm-devel-17.02.7-1.el6.x86_64                             2/4 \n\r  Verifying  : slurm-torque-17.02.7-1.el6.x86_64                            3/4 \n\r  Verifying  : slurm-contribs-17.02.7-1.el6.x86_64                          4/4 \n\nInstalled:\n  slurm-contribs.x86_64 0:17.02.7-1.el6   slurm-devel.x86_64 0:17.02.7-1.el6   \n  slurm-perlapi.x86_64 0:17.02.7-1.el6    slurm-torque.x86_64 0:17.02.7-1.el6  \n\nComplete!\n"]}
changed: [frontend001] => (item=[u'slurm', u'slurm-devel', u'slurm-perlapi', u'slurm-contribs', u'slurm-torque']) => {"changed": true, "item": ["slurm", "slurm-devel", "slurm-perlapi", "slurm-contribs", "slurm-torque"], "msg": "", "rc": 0, "results": ["slurm-17.02.7-1.el6.x86_64 providing slurm is already installed", "Loaded plugins: fastestmirror, security\nSetting up Install Process\nLoading mirror speeds from cached hostfile\nResolving Dependencies\n--> Running transaction check\n---> Package slurm-contribs.x86_64 0:17.02.7-1.el6 will be installed\n---> Package slurm-devel.x86_64 0:17.02.7-1.el6 will be installed\n---> Package slurm-perlapi.x86_64 0:17.02.7-1.el6 will be installed\n---> Package slurm-torque.x86_64 0:17.02.7-1.el6 will be installed\n--> Finished Dependency Resolution\n\nDependencies Resolved\n\n================================================================================\n Package              Arch         Version              Repository         Size\n================================================================================\nInstalling:\n slurm-contribs       x86_64       17.02.7-1.el6        local-slurm        17 k\n slurm-devel          x86_64       17.02.7-1.el6        local-slurm       143 k\n slurm-perlapi        x86_64       17.02.7-1.el6        local-slurm       456 k\n slurm-torque         x86_64       17.02.7-1.el6        local-slurm        37 k\n\nTransaction Summary\n================================================================================\nInstall       4 Package(s)\n\nTotal download size: 653 k\nInstalled size: 2.5 M\nDownloading Packages:\n--------------------------------------------------------------------------------\nTotal                                            49 MB/s | 653 kB     00:00     \nRunning rpm_check_debug\nRunning Transaction Test\nTransaction Test Succeeded\nRunning Transaction\n\r  Installing : slurm-perlapi-17.02.7-1.el6.x86_64                           1/4 \n\r  Installing : slurm-torque-17.02.7-1.el6.x86_64                            2/4 \n\r  Installing : slurm-contribs-17.02.7-1.el6.x86_64                          3/4 \n\r  Installing : slurm-devel-17.02.7-1.el6.x86_64                             4/4 \n\r  Verifying  : slurm-perlapi-17.02.7-1.el6.x86_64                           1/4 \n\r  Verifying  : slurm-devel-17.02.7-1.el6.x86_64                             2/4 \n\r  Verifying  : slurm-torque-17.02.7-1.el6.x86_64                            3/4 \n\r  Verifying  : slurm-contribs-17.02.7-1.el6.x86_64                          4/4 \n\nInstalled:\n  slurm-contribs.x86_64 0:17.02.7-1.el6   slurm-devel.x86_64 0:17.02.7-1.el6   \n  slurm-perlapi.x86_64 0:17.02.7-1.el6    slurm-torque.x86_64 0:17.02.7-1.el6  \n\nComplete!\n"]}
changed: [compute001] => (item=[u'slurm', u'slurm-devel', u'slurm-perlapi', u'slurm-contribs', u'slurm-torque']) => {"changed": true, "item": ["slurm", "slurm-devel", "slurm-perlapi", "slurm-contribs", "slurm-torque"], "msg": "", "rc": 0, "results": ["slurm-17.02.7-1.el6.x86_64 providing slurm is already installed", "Loaded plugins: fastestmirror, security\nSetting up Install Process\nLoading mirror speeds from cached hostfile\nResolving Dependencies\n--> Running transaction check\n---> Package slurm-contribs.x86_64 0:17.02.7-1.el6 will be installed\n---> Package slurm-devel.x86_64 0:17.02.7-1.el6 will be installed\n---> Package slurm-perlapi.x86_64 0:17.02.7-1.el6 will be installed\n---> Package slurm-torque.x86_64 0:17.02.7-1.el6 will be installed\n--> Finished Dependency Resolution\n\nDependencies Resolved\n\n================================================================================\n Package              Arch         Version              Repository         Size\n================================================================================\nInstalling:\n slurm-contribs       x86_64       17.02.7-1.el6        local-slurm        17 k\n slurm-devel          x86_64       17.02.7-1.el6        local-slurm       143 k\n slurm-perlapi        x86_64       17.02.7-1.el6        local-slurm       456 k\n slurm-torque         x86_64       17.02.7-1.el6        local-slurm        37 k\n\nTransaction Summary\n================================================================================\nInstall       4 Package(s)\n\nTotal download size: 653 k\nInstalled size: 2.5 M\nDownloading Packages:\n--------------------------------------------------------------------------------\nTotal                                            39 MB/s | 653 kB     00:00     \nRunning rpm_check_debug\nRunning Transaction Test\nTransaction Test Succeeded\nRunning Transaction\n\r  Installing : slurm-perlapi-17.02.7-1.el6.x86_64                           1/4 \n\r  Installing : slurm-torque-17.02.7-1.el6.x86_64                            2/4 \n\r  Installing : slurm-contribs-17.02.7-1.el6.x86_64                          3/4 \n\r  Installing : slurm-devel-17.02.7-1.el6.x86_64                             4/4 \n\r  Verifying  : slurm-perlapi-17.02.7-1.el6.x86_64                           1/4 \n\r  Verifying  : slurm-devel-17.02.7-1.el6.x86_64                             2/4 \n\r  Verifying  : slurm-torque-17.02.7-1.el6.x86_64                            3/4 \n\r  Verifying  : slurm-contribs-17.02.7-1.el6.x86_64                          4/4 \n\nInstalled:\n  slurm-contribs.x86_64 0:17.02.7-1.el6   slurm-devel.x86_64 0:17.02.7-1.el6   \n  slurm-perlapi.x86_64 0:17.02.7-1.el6    slurm-torque.x86_64 0:17.02.7-1.el6  \n\nComplete!\n"]}

TASK [slurm-client : Stop SLURM services (RHEL/CentOS 6.x)] ********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-client/tasks/main.yml:59
changed: [frontend001] => {"changed": true, "enabled": false, "name": "slurm", "state": "stopped"}
changed: [compute002] => {"changed": true, "enabled": false, "name": "slurm", "state": "stopped"}
changed: [compute001] => {"changed": true, "enabled": false, "name": "slurm", "state": "stopped"}

TASK [slurm-client : Stop SLURM services (RHEL/CentOS 7.x)] ********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-client/tasks/main.yml:66
skipping: [frontend001] => (item=slurmd)  => {"changed": false, "item": "slurmd", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [frontend001] => (item=slurmctld)  => {"changed": false, "item": "slurmctld", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => (item=slurmd)  => {"changed": false, "item": "slurmd", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => (item=slurmctld)  => {"changed": false, "item": "slurmctld", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item=slurmd)  => {"changed": false, "item": "slurmd", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item=slurmctld)  => {"changed": false, "item": "slurmctld", "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-worker : Load distribution-specific parameters] ********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/main.yml:4
included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/init-RedHat.yml for frontend001, compute001, compute002

TASK [slurm-worker : Set SLURM worker playbook params (RHEL compatible)] *******
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/init-RedHat.yml:3
ok: [frontend001] => {"ansible_facts": {"slurmd_packages": ["slurm", "slurm-plugins", "slurm-sql"]}, "changed": false}
ok: [compute001] => {"ansible_facts": {"slurmd_packages": ["slurm", "slurm-plugins", "slurm-sql"]}, "changed": false}
ok: [compute002] => {"ansible_facts": {"slurmd_packages": ["slurm", "slurm-plugins", "slurm-sql"]}, "changed": false}

TASK [slurm-worker : Set SLURM worker service name (RHEL 7.x compatible)] ******
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/init-RedHat.yml:11
skipping: [frontend001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-worker : Set SLURM worker service name (RHEL 6.x compatible)] ******
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/init-RedHat.yml:16
ok: [frontend001] => {"ansible_facts": {"slurmd_service": "slurm"}, "changed": false}
ok: [compute001] => {"ansible_facts": {"slurmd_service": "slurm"}, "changed": false}
ok: [compute002] => {"ansible_facts": {"slurmd_service": "slurm"}, "changed": false}

TASK [slurm-worker : Deploy SLURM configuration files] *************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/main.yml:7
changed: [compute001] => (item=gres.conf) => {"changed": true, "checksum": "afc9ac4c3247ee557700305700c9743a79f0dfd0", "dest": "/etc/slurm/gres.conf", "gid": 0, "group": "root", "item": "gres.conf", "md5sum": "a178d15a40bf70261e38f1cd729a8d56", "mode": "0444", "owner": "root", "size": 297, "src": "/root/.ansible/tmp/ansible-tmp-1524707058.86-38970739321481/source", "state": "file", "uid": 0}
changed: [frontend001] => (item=gres.conf) => {"changed": true, "checksum": "afc9ac4c3247ee557700305700c9743a79f0dfd0", "dest": "/etc/slurm/gres.conf", "gid": 0, "group": "root", "item": "gres.conf", "md5sum": "a178d15a40bf70261e38f1cd729a8d56", "mode": "0444", "owner": "root", "size": 297, "src": "/root/.ansible/tmp/ansible-tmp-1524707058.85-174083393437841/source", "state": "file", "uid": 0}
changed: [compute002] => (item=gres.conf) => {"changed": true, "checksum": "afc9ac4c3247ee557700305700c9743a79f0dfd0", "dest": "/etc/slurm/gres.conf", "gid": 0, "group": "root", "item": "gres.conf", "md5sum": "a178d15a40bf70261e38f1cd729a8d56", "mode": "0444", "owner": "root", "size": 297, "src": "/root/.ansible/tmp/ansible-tmp-1524707058.89-150816996775988/source", "state": "file", "uid": 0}

TASK [slurm-worker : Deploy kernel config check script] ************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/cgroup.yml:3
skipping: [frontend001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-worker : Check that kernel is configured for cgroups] **************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/cgroup.yml:12
skipping: [frontend001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-worker : Ensure cgroup filesystems are mounted] ********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/cgroup.yml:18
skipping: [frontend001] => (item=blkio)  => {"changed": false, "item": "blkio", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [frontend001] => (item=cpuacct)  => {"changed": false, "item": "cpuacct", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => (item=blkio)  => {"changed": false, "item": "blkio", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => (item=cpuacct)  => {"changed": false, "item": "cpuacct", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [frontend001] => (item=cpuset)  => {"changed": false, "item": "cpuset", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item=blkio)  => {"changed": false, "item": "blkio", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => (item=cpuset)  => {"changed": false, "item": "cpuset", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [frontend001] => (item=devices)  => {"changed": false, "item": "devices", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item=cpuacct)  => {"changed": false, "item": "cpuacct", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => (item=devices)  => {"changed": false, "item": "devices", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item=cpuset)  => {"changed": false, "item": "cpuset", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [frontend001] => (item=freezer)  => {"changed": false, "item": "freezer", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => (item=freezer)  => {"changed": false, "item": "freezer", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [frontend001] => (item=memory)  => {"changed": false, "item": "memory", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item=devices)  => {"changed": false, "item": "devices", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => (item=memory)  => {"changed": false, "item": "memory", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item=freezer)  => {"changed": false, "item": "freezer", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item=memory)  => {"changed": false, "item": "memory", "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-worker : Ensure kernel is booted with swap accounting enabled] *****
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/cgroup.yml:44
skipping: [frontend001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-worker : Check if swap accouting is enabled (may fail!)] ***********
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/cgroup.yml:53
skipping: [frontend001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-worker : Reboot to enable swap accounting] *************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/cgroup.yml:59
skipping: [frontend001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-worker : Wait for server to come up again] *************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/cgroup.yml:72
skipping: [frontend001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-worker : Ensure release agent directory exists] ********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/cgroup.yml:84
skipping: [frontend001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-worker : Deploy cgroup-specific release agent script] **************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/cgroup.yml:93
skipping: [frontend001] => (item=blkio)  => {"changed": false, "item": "blkio", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [frontend001] => (item=cpuacct)  => {"changed": false, "item": "cpuacct", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => (item=blkio)  => {"changed": false, "item": "blkio", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => (item=cpuacct)  => {"changed": false, "item": "cpuacct", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [frontend001] => (item=cpuset)  => {"changed": false, "item": "cpuset", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item=blkio)  => {"changed": false, "item": "blkio", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => (item=cpuset)  => {"changed": false, "item": "cpuset", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [frontend001] => (item=devices)  => {"changed": false, "item": "devices", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item=cpuacct)  => {"changed": false, "item": "cpuacct", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => (item=devices)  => {"changed": false, "item": "devices", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [frontend001] => (item=freezer)  => {"changed": false, "item": "freezer", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item=cpuset)  => {"changed": false, "item": "cpuset", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => (item=freezer)  => {"changed": false, "item": "freezer", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [frontend001] => (item=memory)  => {"changed": false, "item": "memory", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item=devices)  => {"changed": false, "item": "devices", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => (item=memory)  => {"changed": false, "item": "memory", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item=freezer)  => {"changed": false, "item": "freezer", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item=memory)  => {"changed": false, "item": "memory", "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-worker : Deploy SLURM cgroup configuration file (main)] ************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/cgroup.yml:103
skipping: [frontend001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-worker : Deploy SLURM cgroup configuration file (devices)] *********
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/cgroup.yml:112
skipping: [frontend001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-worker : Install SLURM worker packages] ****************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/main.yml:27
changed: [compute002] => (item=[u'slurm', u'slurm-plugins', u'slurm-sql']) => {"changed": true, "item": ["slurm", "slurm-plugins", "slurm-sql"], "msg": "", "rc": 0, "results": ["slurm-17.02.7-1.el6.x86_64 providing slurm is already installed", "slurm-plugins-17.02.7-1.el6.x86_64 providing slurm-plugins is already installed", "Loaded plugins: fastestmirror, security\nSetting up Install Process\nLoading mirror speeds from cached hostfile\nResolving Dependencies\n--> Running transaction check\n---> Package slurm-sql.x86_64 0:17.02.7-1.el6 will be installed\n--> Finished Dependency Resolution\n\nDependencies Resolved\n\n================================================================================\n Package          Arch          Version                Repository          Size\n================================================================================\nInstalling:\n slurm-sql        x86_64        17.02.7-1.el6          local-slurm        306 k\n\nTransaction Summary\n================================================================================\nInstall       1 Package(s)\n\nTotal download size: 306 k\nInstalled size: 1.1 M\nDownloading Packages:\nRunning rpm_check_debug\nRunning Transaction Test\nTransaction Test Succeeded\nRunning Transaction\n\r  Installing : slurm-sql-17.02.7-1.el6.x86_64                               1/1 \n\r  Verifying  : slurm-sql-17.02.7-1.el6.x86_64                               1/1 \n\nInstalled:\n  slurm-sql.x86_64 0:17.02.7-1.el6                                              \n\nComplete!\n"]}
changed: [compute001] => (item=[u'slurm', u'slurm-plugins', u'slurm-sql']) => {"changed": true, "item": ["slurm", "slurm-plugins", "slurm-sql"], "msg": "", "rc": 0, "results": ["slurm-17.02.7-1.el6.x86_64 providing slurm is already installed", "slurm-plugins-17.02.7-1.el6.x86_64 providing slurm-plugins is already installed", "Loaded plugins: fastestmirror, security\nSetting up Install Process\nLoading mirror speeds from cached hostfile\nResolving Dependencies\n--> Running transaction check\n---> Package slurm-sql.x86_64 0:17.02.7-1.el6 will be installed\n--> Finished Dependency Resolution\n\nDependencies Resolved\n\n================================================================================\n Package          Arch          Version                Repository          Size\n================================================================================\nInstalling:\n slurm-sql        x86_64        17.02.7-1.el6          local-slurm        306 k\n\nTransaction Summary\n================================================================================\nInstall       1 Package(s)\n\nTotal download size: 306 k\nInstalled size: 1.1 M\nDownloading Packages:\nRunning rpm_check_debug\nRunning Transaction Test\nTransaction Test Succeeded\nRunning Transaction\n\r  Installing : slurm-sql-17.02.7-1.el6.x86_64                               1/1 \n\r  Verifying  : slurm-sql-17.02.7-1.el6.x86_64                               1/1 \n\nInstalled:\n  slurm-sql.x86_64 0:17.02.7-1.el6                                              \n\nComplete!\n"]}
changed: [frontend001] => (item=[u'slurm', u'slurm-plugins', u'slurm-sql']) => {"changed": true, "item": ["slurm", "slurm-plugins", "slurm-sql"], "msg": "", "rc": 0, "results": ["slurm-17.02.7-1.el6.x86_64 providing slurm is already installed", "slurm-plugins-17.02.7-1.el6.x86_64 providing slurm-plugins is already installed", "Loaded plugins: fastestmirror, security\nSetting up Install Process\nLoading mirror speeds from cached hostfile\nResolving Dependencies\n--> Running transaction check\n---> Package slurm-sql.x86_64 0:17.02.7-1.el6 will be installed\n--> Finished Dependency Resolution\n\nDependencies Resolved\n\n================================================================================\n Package          Arch          Version                Repository          Size\n================================================================================\nInstalling:\n slurm-sql        x86_64        17.02.7-1.el6          local-slurm        306 k\n\nTransaction Summary\n================================================================================\nInstall       1 Package(s)\n\nTotal download size: 306 k\nInstalled size: 1.1 M\nDownloading Packages:\nRunning rpm_check_debug\nRunning Transaction Test\nTransaction Test Succeeded\nRunning Transaction\n\r  Installing : slurm-sql-17.02.7-1.el6.x86_64                               1/1 \n\r  Verifying  : slurm-sql-17.02.7-1.el6.x86_64                               1/1 \n\nInstalled:\n  slurm-sql.x86_64 0:17.02.7-1.el6                                              \n\nComplete!\n"]}

TASK [slurm-worker : Ensure SLURMd starts at boot] *****************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/main.yml:33
changed: [frontend001] => {"changed": true, "enabled": true, "name": "slurm"}
changed: [compute001] => {"changed": true, "enabled": true, "name": "slurm"}
changed: [compute002] => {"changed": true, "enabled": true, "name": "slurm"}

PLAY [Setup OBS-initial mount directory and password file] *********************

TASK [setup] *******************************************************************
ok: [compute001]
ok: [compute002]
ok: [frontend001]

TASK [Create the obs directory] ************************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/s3fs.yml:8
changed: [frontend001] => {"changed": true, "gid": 0, "group": "root", "mode": "0755", "owner": "root", "path": "/obs", "size": 4096, "state": "directory", "uid": 0}
changed: [compute001] => {"changed": true, "gid": 0, "group": "root", "mode": "0755", "owner": "root", "path": "/obs", "size": 4096, "state": "directory", "uid": 0}
changed: [compute002] => {"changed": true, "gid": 0, "group": "root", "mode": "0755", "owner": "root", "path": "/obs", "size": 4096, "state": "directory", "uid": 0}

TASK [Create the obs password file] ********************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/s3fs.yml:12
changed: [compute001] => {"changed": true, "dest": "/etc/passwd-obs", "gid": 0, "group": "root", "mode": "0600", "owner": "root", "size": 0, "state": "file", "uid": 0}
changed: [frontend001] => {"changed": true, "dest": "/etc/passwd-obs", "gid": 0, "group": "root", "mode": "0600", "owner": "root", "size": 0, "state": "file", "uid": 0}
changed: [compute002] => {"changed": true, "dest": "/etc/passwd-obs", "gid": 0, "group": "root", "mode": "0600", "owner": "root", "size": 0, "state": "file", "uid": 0}

TASK [Setup OBS-insert into password file] *************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/s3fs.yml:17
changed: [frontend001] => {"changed": true, "cmd": "echo CNYVGDKK2PPBQEEMUYQG:6imAmmCVVzMqR1dm5x0AKJcL1BILA7TlOsSMhu4G > /etc/passwd-obs", "delta": "0:00:00.003634", "end": "2018-04-26 09:44:29.283417", "rc": 0, "start": "2018-04-26 09:44:29.279783", "stderr": "", "stdout": "", "stdout_lines": [], "warnings": []}
changed: [compute001] => {"changed": true, "cmd": "echo CNYVGDKK2PPBQEEMUYQG:6imAmmCVVzMqR1dm5x0AKJcL1BILA7TlOsSMhu4G > /etc/passwd-obs", "delta": "0:00:00.003670", "end": "2018-04-26 09:44:29.452506", "rc": 0, "start": "2018-04-26 09:44:29.448836", "stderr": "", "stdout": "", "stdout_lines": [], "warnings": []}
changed: [compute002] => {"changed": true, "cmd": "echo CNYVGDKK2PPBQEEMUYQG:6imAmmCVVzMqR1dm5x0AKJcL1BILA7TlOsSMhu4G > /etc/passwd-obs", "delta": "0:00:00.003263", "end": "2018-04-26 09:44:29.415677", "rc": 0, "start": "2018-04-26 09:44:29.412414", "stderr": "", "stdout": "", "stdout_lines": [], "warnings": []}

TASK [Setup OBS-umount obs] ****************************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/s3fs.yml:19
ok: [compute001] => {"changed": false, "dump": "0", "fstab": "/etc/fstab", "fstype": "none", "name": "/obs", "opts": "defaults", "passno": "0", "src": "none"}
ok: [frontend001] => {"changed": false, "dump": "0", "fstab": "/etc/fstab", "fstype": "none", "name": "/obs", "opts": "defaults", "passno": "0", "src": "none"}
ok: [compute002] => {"changed": false, "dump": "0", "fstab": "/etc/fstab", "fstype": "none", "name": "/obs", "opts": "defaults", "passno": "0", "src": "none"}

TASK [Setup OBS-mount obs] *****************************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/s3fs.yml:25
changed: [frontend001] => {"changed": true, "cmd": "s3fs hwc-obs /obs -o url=http://obs.cn-north-1.myhwclouds.com -o endpoint=cn-north-1 -o passwd_file=/etc/passwd-obs", "delta": "0:00:00.032905", "end": "2018-04-26 09:44:29.846066", "rc": 0, "start": "2018-04-26 09:44:29.813161", "stderr": "", "stdout": "", "stdout_lines": [], "warnings": []}
changed: [compute001] => {"changed": true, "cmd": "s3fs hwc-obs /obs -o url=http://obs.cn-north-1.myhwclouds.com -o endpoint=cn-north-1 -o passwd_file=/etc/passwd-obs", "delta": "0:00:00.039466", "end": "2018-04-26 09:44:30.024262", "rc": 0, "start": "2018-04-26 09:44:29.984796", "stderr": "", "stdout": "", "stdout_lines": [], "warnings": []}
changed: [compute002] => {"changed": true, "cmd": "s3fs hwc-obs /obs -o url=http://obs.cn-north-1.myhwclouds.com -o endpoint=cn-north-1 -o passwd_file=/etc/passwd-obs", "delta": "0:00:00.032673", "end": "2018-04-26 09:44:29.986093", "rc": 0, "start": "2018-04-26 09:44:29.953420", "stderr": "", "stdout": "", "stdout_lines": [], "warnings": []}

TASK [Remove item from /etc/fstaba] ********************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/s3fs.yml:27
ok: [frontend001] => {"backup": "", "changed": false, "found": 0, "msg": ""}
ok: [compute001] => {"backup": "", "changed": false, "found": 0, "msg": ""}
ok: [compute002] => {"backup": "", "changed": false, "found": 0, "msg": ""}

TASK [Add to /etc/fstab] *******************************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/s3fs.yml:33
changed: [compute001] => {"changed": true, "cmd": "echo s3fs#hwc-obs /obs fuse _netdev,allow_other,use_path_request_style,url=http://obs.cn-north-1.myhwclouds.com,passwd_file=/etc/passwd-obs 0 0 >> /etc/fstab", "delta": "0:00:00.003428", "end": "2018-04-26 09:44:30.518886", "rc": 0, "start": "2018-04-26 09:44:30.515458", "stderr": "", "stdout": "", "stdout_lines": [], "warnings": []}
changed: [frontend001] => {"changed": true, "cmd": "echo s3fs#hwc-obs /obs fuse _netdev,allow_other,use_path_request_style,url=http://obs.cn-north-1.myhwclouds.com,passwd_file=/etc/passwd-obs 0 0 >> /etc/fstab", "delta": "0:00:00.004001", "end": "2018-04-26 09:44:30.369425", "rc": 0, "start": "2018-04-26 09:44:30.365424", "stderr": "", "stdout": "", "stdout_lines": [], "warnings": []}
changed: [compute002] => {"changed": true, "cmd": "echo s3fs#hwc-obs /obs fuse _netdev,allow_other,use_path_request_style,url=http://obs.cn-north-1.myhwclouds.com,passwd_file=/etc/passwd-obs 0 0 >> /etc/fstab", "delta": "0:00:00.003785", "end": "2018-04-26 09:44:30.497132", "rc": 0, "start": "2018-04-26 09:44:30.493347", "stderr": "", "stdout": "", "stdout_lines": [], "warnings": []}

PLAY [Slurm master Playbook] ***************************************************

TASK [setup] *******************************************************************
ok: [frontend001]

TASK [nis : Load distribution-specific parameters] *****************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/main.yml:3
included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/init-RedHat.yml for frontend001

TASK [nis : Set NIS common playbook params (CentOS/RHEL)] **********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/init-RedHat.yml:3
ok: [frontend001] => {"ansible_facts": {"nis_client_packages": ["ypbind"], "nis_client_services": ["rpcbind", "ypbind"], "nis_common_packages": ["yp-tools"], "nis_master_packages": ["ypserv"], "nis_master_services": ["rpcbind", "yppasswdd", "ypserv"], "nis_securenets_path": "/var/yp/securenets"}, "changed": false}

TASK [nis : Pre-load debconf answer to questions (Debian/Ubuntu)] **************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/main.yml:7
skipping: [frontend001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [nis : Deploy additional NIS configuration (Debian/Ubuntu)] ***************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/main.yml:18
skipping: [frontend001] => (item=etc/default/nis)  => {"changed": false, "item": "etc/default/nis", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [frontend001] => (item=etc/defaultdomain)  => {"changed": false, "item": "etc/defaultdomain", "skip_reason": "Conditional check failed", "skipped": true}

TASK [nis : Set NIS domain (CentOS/RHEL)] **************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/main.yml:29
changed: [frontend001] => {"backup": "", "changed": true, "msg": "line added"}

TASK [nis : Install NIS common packages] ***************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/main.yml:39
ok: [frontend001] => (item=[u'yp-tools']) => {"changed": false, "item": ["yp-tools"], "msg": "", "rc": 0, "results": ["yp-tools-2.9-12.el6.x86_64 providing yp-tools is already installed"]}

TASK [nis : Deploy `ypserv` configuration files] *******************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml:3
ok: [frontend001] => {"changed": false, "gid": 0, "group": "root", "mode": "0400", "owner": "root", "path": "/var/yp/securenets", "size": 707, "state": "file", "uid": 0}

TASK [nis : Deploy `yppasswdd` configuration file (CentOS/RHEL)] ***************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml:12
ok: [frontend001] => {"changed": false, "checksum": "18392b4e3ecc4f9e7832c274663b5f6cee00d3f1", "dest": "/etc/sysconfig/yppasswdd", "gid": 0, "group": "root", "mode": "0444", "owner": "root", "path": "/etc/sysconfig/yppasswdd", "size": 681, "state": "file", "uid": 0}

TASK [nis : Install NIS master server packages] ********************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml:22
ok: [frontend001] => (item=[u'ypserv']) => {"changed": false, "item": ["ypserv"], "msg": "", "rc": 0, "results": ["ypserv-2.19-31.el6.x86_64 providing ypserv is already installed"]}

TASK [nis : Ensure `ypserv` starts at boot] ************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml:33
ok: [frontend001] => (item=rpcbind) => {"changed": false, "enabled": true, "item": "rpcbind", "name": "rpcbind", "state": "started"}
ok: [frontend001] => (item=yppasswdd) => {"changed": false, "enabled": true, "item": "yppasswdd", "name": "yppasswdd", "state": "started"}
ok: [frontend001] => (item=ypserv) => {"changed": false, "enabled": true, "item": "ypserv", "name": "ypserv", "state": "started"}

TASK [nis : Update NIS/YP databases (NIS master server)] ***********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml:41
changed: [frontend001] => {"changed": true, "cmd": ["make"], "delta": "0:00:00.030867", "end": "2018-04-26 09:44:33.714434", "rc": 0, "start": "2018-04-26 09:44:33.683567", "stderr": "", "stdout": "gmake[1]: Entering directory `/var/yp/elasticluster'\nUpdating netid.byname...\ngmake[1]: Leaving directory `/var/yp/elasticluster'", "stdout_lines": ["gmake[1]: Entering directory `/var/yp/elasticluster'", "Updating netid.byname...", "gmake[1]: Leaving directory `/var/yp/elasticluster'"], "warnings": []}

TASK [nis : Deploy `ypbind` configuration files] *******************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypbind.yml:3
skipping: [frontend001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [nis : Ensure NIS/YP is used as a name service] ***************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypbind.yml:12
skipping: [frontend001] => (item=passwd)  => {"changed": false, "item": "passwd", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [frontend001] => (item=group)  => {"changed": false, "item": "group", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [frontend001] => (item=shadow)  => {"changed": false, "item": "shadow", "skip_reason": "Conditional check failed", "skipped": true}

TASK [nis : Replace `compat` in `/etc/nsswitch.conf` with `files nis`] *********
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypbind.yml:26
skipping: [frontend001] => (item=passwd)  => {"changed": false, "item": "passwd", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [frontend001] => (item=group)  => {"changed": false, "item": "group", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [frontend001] => (item=shadow)  => {"changed": false, "item": "shadow", "skip_reason": "Conditional check failed", "skipped": true}

TASK [nis : Ensure `ypbind` starts at boot] ************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypbind.yml:37
skipping: [frontend001] => (item=rpcbind)  => {"changed": false, "item": "rpcbind", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [frontend001] => (item=ypbind)  => {"changed": false, "item": "ypbind", "skip_reason": "Conditional check failed", "skipped": true}

TASK [nfs-server : Load distribution-specific parameters] **********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-server/tasks/main.yml:6
included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-server/tasks/init-RedHat.yml for frontend001

TASK [nfs-server : Set NFS server variables (RHEL/CentOS 7.x)] *****************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-server/tasks/init-RedHat.yml:6
skipping: [frontend001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [nfs-server : Set NFS server variables (RHEL/CentOS 6.x)] *****************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-server/tasks/init-RedHat.yml:20
ok: [frontend001] => {"ansible_facts": {"_nfs_server_started_state": "started", "nfs_server_packages": ["nfs-utils"], "nfs_server_services": ["rpcbind", "nfslock", "nfs"]}, "changed": false}

TASK [nfs-server : install NFS server software] ********************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-server/tasks/main.yml:12
ok: [frontend001] => (item=[u'nfs-utils']) => {"changed": false, "item": ["nfs-utils"], "msg": "", "rc": 0, "results": ["nfs-utils-1:1.2.3-75.el6_9.x86_64 providing nfs-utils is already installed"]}

TASK [nfs-server : Export directories] *****************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-server/tasks/main.yml:25
changed: [frontend001] => (item={u'path': u'/home', u'clients': [u'compute001', u'compute002']}) => {"changed": true, "item": {"clients": ["compute001", "compute002"], "path": "/home"}}

TASK [nfs-server : Ensure portmapper is running] *******************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-server/tasks/main.yml:38
ok: [frontend001] => {"changed": false, "enabled": true, "name": "rpcbind", "state": "started"}

TASK [nfs-server : Ensure NFS server is running (Debian 8 "jessie")] ***********
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-server/tasks/main.yml:48
skipping: [frontend001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [nfs-server : Ensure NFS server is running] *******************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-server/tasks/main.yml:59
ok: [frontend001] => (item=rpcbind) => {"changed": false, "enabled": true, "item": "rpcbind", "name": "rpcbind", "state": "started"}
ok: [frontend001] => (item=nfslock) => {"changed": false, "enabled": true, "item": "nfslock", "name": "nfslock", "state": "started"}
ok: [frontend001] => (item=nfs) => {"changed": false, "enabled": true, "item": "nfs", "name": "nfs", "state": "started"}

TASK [nfs-server : Reload NFS exports file] ************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-server/tasks/main.yml:71
changed: [frontend001] => {"changed": true, "cmd": ["exportfs", "-r"], "delta": "0:00:00.003453", "end": "2018-04-26 09:44:37.013550", "rc": 0, "start": "2018-04-26 09:44:37.010097", "stderr": "", "stdout": "", "stdout_lines": [], "warnings": []}

TASK [nfs-server : Restart NFS server services] ********************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-server/tasks/main.yml:79
changed: [frontend001] => {"changed": true, "name": "nfs", "state": "started"}

TASK [slurm-common : Load distribution-specific parameters] ********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:3
included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/init-RedHat.yml for frontend001

TASK [slurm-common : Set SLURM common playbook params (RHEL 7.x compatible)] ***
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/init-RedHat.yml:3
ok: [frontend001] => {"ansible_facts": {"slurm_pid_dir": "/var/run"}, "changed": false}

TASK [slurm-common : Create `slurm` system group] ******************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:16
ok: [frontend001] => {"changed": false, "gid": 498, "name": "slurm", "state": "present", "system": true}

TASK [slurm-common : Create `slurm` system user] *******************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:24
ok: [frontend001] => {"append": false, "changed": false, "comment": "", "group": 498, "home": "/home/slurm", "move_home": false, "name": "slurm", "shell": "/bin/bash", "state": "present", "uid": 498}

TASK [slurm-common : Create work directory {{item}}] ***************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:33
changed: [frontend001] => (item=/etc/slurm) => {"changed": true, "gid": 498, "group": "slurm", "item": "/etc/slurm", "mode": "0755", "owner": "slurm", "path": "/etc/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [frontend001] => (item=/var/lib/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/lib/slurm", "mode": "0755", "owner": "slurm", "path": "/var/lib/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [frontend001] => (item=/var/lib/slurm/checkpoint) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/lib/slurm/checkpoint", "mode": "0755", "owner": "slurm", "path": "/var/lib/slurm/checkpoint", "size": 4096, "state": "directory", "uid": 498}
ok: [frontend001] => (item=/var/lib/slurm/slurmctld) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/lib/slurm/slurmctld", "mode": "0755", "owner": "slurm", "path": "/var/lib/slurm/slurmctld", "size": 4096, "state": "directory", "uid": 498}
ok: [frontend001] => (item=/var/lib/slurm/slurmd) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/lib/slurm/slurmd", "mode": "0755", "owner": "slurm", "path": "/var/lib/slurm/slurmd", "size": 4096, "state": "directory", "uid": 498}
ok: [frontend001] => (item=/var/log/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/log/slurm", "mode": "0755", "owner": "slurm", "path": "/var/log/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [frontend001] => (item=/var/run/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/run/slurm", "mode": "0755", "owner": "slurm", "path": "/var/run/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [frontend001] => (item=/var/spool/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/spool/slurm", "mode": "0755", "owner": "slurm", "path": "/var/spool/slurm", "size": 4096, "state": "directory", "uid": 498}

TASK [slurm-common : Make compatibility symlinks] ******************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:56
skipping: [frontend001] => (item={u'to': u'/var/lib/slurm-llnl', u'from': u'/var/lib/slurm'})  => {"changed": false, "item": {"from": "/var/lib/slurm", "to": "/var/lib/slurm-llnl"}, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [frontend001] => (item={u'to': u'/etc/slurm-llnl', u'from': u'/etc/slurm'})  => {"changed": false, "item": {"from": "/etc/slurm", "to": "/etc/slurm-llnl"}, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [frontend001] => (item={u'to': u'/var/log/slurm-llnl', u'from': u'/var/log/slurm'})  => {"changed": false, "item": {"from": "/var/log/slurm", "to": "/var/log/slurm-llnl"}, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [frontend001] => (item={u'to': u'/var/run/slurm-llnl', u'from': u'/var/run/slurm'})  => {"changed": false, "item": {"from": "/var/run/slurm", "to": "/var/run/slurm-llnl"}, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-common : Enable Copr SLURM repo by verdurin] ***********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:73
ok: [frontend001] => {"changed": false, "gid": 0, "group": "root", "mode": "0444", "owner": "root", "path": "/etc/yum.repos.d/copr-slurm.repo", "size": 615, "state": "file", "uid": 0}

TASK [slurm-common : Prevent MUNGE from starting] ******************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:20
skipping: [frontend001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-common : Install MUNGE (Debian/Ubuntu)] ****************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:31
skipping: [frontend001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-common : Install MUNGE (RHEL-compatible)] **************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:43
ok: [frontend001] => {"changed": false, "msg": "", "rc": 0, "results": ["slurm-munge-17.02.7-1.el6.x86_64 providing slurm-munge is already installed"]}

TASK [slurm-common : Allow MUNGE to start] *************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:57
skipping: [frontend001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-common : Add `--syslog` option to `munged` startup] ****************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:66
skipping: [frontend001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-common : Reload systemd configuration] *****************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:77
skipping: [frontend001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-common : Add `--syslog` startup options for `munged` (Ubuntu 14.04)] ***
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:86
skipping: [frontend001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-common : Configure MUNGE] ******************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:101
ok: [frontend001] => {"changed": false, "checksum": "222afe887f907039b799b394715664ffa584fec5", "dest": "/etc/munge/munge.key", "gid": 497, "group": "munge", "mode": "0400", "owner": "munge", "path": "/etc/munge/munge.key", "size": 1024, "state": "file", "uid": 497}

TASK [slurm-common : Ensure the MUNGE service is running] **********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:114
ok: [frontend001] => {"changed": false, "enabled": true, "name": "munge", "state": "started"}

TASK [slurm-common : Deploy SLURM configuration file] **************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:86
ok: [frontend001] => {"changed": false, "gid": 0, "group": "root", "mode": "0444", "owner": "root", "path": "/etc/slurm/slurm.conf", "size": 4156, "state": "file", "uid": 0}

TASK [slurm-common : Deploy script to suspend cloud instance] ******************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:96
ok: [frontend001] => {"changed": false, "gid": 0, "group": "root", "mode": "0555", "owner": "root", "path": "/etc/slurm/slurm.suspend.sh", "size": 604, "state": "file", "uid": 0}

TASK [slurm-common : Deploy script to resume cloud instance] *******************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:106
ok: [frontend001] => {"changed": false, "gid": 0, "group": "root", "mode": "0555", "owner": "root", "path": "/etc/slurm/slurm.resume.sh", "size": 631, "state": "file", "uid": 0}

TASK [slurm-common : Install support packages (Debian/Ubuntu)] *****************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:116
skipping: [frontend001] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-common : Install SLURM packages (RHEL-compatible)] *****************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:127
skipping: [frontend001] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-client : Install required SLURM packages (Debian/Ubuntu)] **********
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-client/tasks/main.yml:4
skipping: [frontend001] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-client : Install required SLURM packages (older Debian/Ubuntu)] ****
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-client/tasks/main.yml:23
skipping: [frontend001] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-client : service] **************************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-client/tasks/main.yml:35
skipping: [frontend001] => (item=slurm-llnl)  => {"changed": false, "item": "slurm-llnl", "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-client : Install SLURM packages (RHEL-compatible)] *****************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-client/tasks/main.yml:47
ok: [frontend001] => (item=[u'slurm', u'slurm-devel', u'slurm-perlapi', u'slurm-contribs', u'slurm-torque']) => {"changed": false, "item": ["slurm", "slurm-devel", "slurm-perlapi", "slurm-contribs", "slurm-torque"], "msg": "", "rc": 0, "results": ["slurm-17.02.7-1.el6.x86_64 providing slurm is already installed", "slurm-devel-17.02.7-1.el6.x86_64 providing slurm-devel is already installed", "slurm-perlapi-17.02.7-1.el6.x86_64 providing slurm-perlapi is already installed", "slurm-contribs-17.02.7-1.el6.x86_64 providing slurm-contribs is already installed", "slurm-torque-17.02.7-1.el6.x86_64 providing slurm-torque is already installed"]}

TASK [slurm-client : Stop SLURM services (RHEL/CentOS 6.x)] ********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-client/tasks/main.yml:59
changed: [frontend001] => {"changed": true, "enabled": false, "name": "slurm", "state": "stopped"}

TASK [slurm-client : Stop SLURM services (RHEL/CentOS 7.x)] ********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-client/tasks/main.yml:66
skipping: [frontend001] => (item=slurmd)  => {"changed": false, "item": "slurmd", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [frontend001] => (item=slurmctld)  => {"changed": false, "item": "slurmctld", "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-master : Load distribution-specific parameters] ********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-master/tasks/main.yml:4
included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-master/tasks/init-RedHat.yml for frontend001

TASK [slurm-master : Set SLURM master playbook params (RHEL 7.x compatible)] ***
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-master/tasks/init-RedHat.yml:3
skipping: [frontend001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-master : Set SLURM playbook params (RHEL 6.x compatible)] **********
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-master/tasks/init-RedHat.yml:18
ok: [frontend001] => {"ansible_facts": {"slurmctld_packages": ["mailx", "slurm-plugins", "slurm"], "slurmctld_service_name": "slurm", "slurmdbd_packages": ["slurm-sql", "slurm-slurmdbd"], "slurmdbd_service_name": "slurmdbd"}, "changed": false}

TASK [slurm-master : Set variables (Debian/Ubuntu)] ****************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-master/tasks/db.yml:3
skipping: [frontend001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-master : Set variables (older Debian/Ubuntu)] **********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-master/tasks/db.yml:11
skipping: [frontend001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-master : Set variables (RHEL 7.x)] *********************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-master/tasks/db.yml:19
skipping: [frontend001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-master : Set variables (RHEL 6.x)] *********************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-master/tasks/db.yml:27
ok: [frontend001] => {"ansible_facts": {"slurm_db_python_pkg": "MySQL-python", "slurm_db_server_name": "MySQL", "slurm_db_server_pkg": "mysql-server", "slurm_db_service_name": "mysqld"}, "changed": false}

TASK [slurm-master : Install MySQL, used for SLURM accounting] *****************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-master/tasks/db.yml:36
ok: [frontend001] => (item=[u'mysql-server', u'MySQL-python']) => {"changed": false, "item": ["mysql-server", "MySQL-python"], "msg": "", "rc": 0, "results": ["mysql-server-5.1.73-8.el6_8.x86_64 providing mysql-server is already installed", "MySQL-python-1.2.3-0.3.c1.1.el6.x86_64 providing MySQL-python is already installed"]}

TASK [slurm-master : Ensure MySQL daemon is up] ********************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-master/tasks/db.yml:48
ok: [frontend001] => {"changed": false, "enabled": true, "name": "mysqld", "state": "started"}

TASK [slurm-master : Create DB for SLURMDBD] ***********************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-master/tasks/db.yml:58
ok: [frontend001] => {"changed": false, "db": "slurm"}

TASK [slurm-master : Create DB user for SLURMDBD] ******************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-master/tasks/db.yml:68
ok: [frontend001] => (item=frontend001) => {"changed": false, "item": "frontend001", "user": "slurm"}
ok: [frontend001] => (item=localhost) => {"changed": false, "item": "localhost", "user": "slurm"}

TASK [slurm-master : Deploy SLURMDBD configuration] ****************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-master/tasks/install-slurmdbd.yml:5
ok: [frontend001] => {"changed": false, "gid": 0, "group": "root", "mode": "0444", "owner": "root", "path": "/etc/slurm/slurmdbd.conf", "size": 2864, "state": "file", "uid": 0}

TASK [slurm-master : Install SLURM DBD packages] *******************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-master/tasks/install-slurmdbd.yml:19
changed: [frontend001] => (item=[u'slurm-sql', u'slurm-slurmdbd']) => {"changed": true, "item": ["slurm-sql", "slurm-slurmdbd"], "msg": "", "rc": 0, "results": ["slurm-sql-17.02.7-1.el6.x86_64 providing slurm-sql is already installed", "Loaded plugins: fastestmirror, security\nSetting up Install Process\nLoading mirror speeds from cached hostfile\nResolving Dependencies\n--> Running transaction check\n---> Package slurm-slurmdbd.x86_64 0:17.02.7-1.el6 will be installed\n--> Finished Dependency Resolution\n\nDependencies Resolved\n\n================================================================================\n Package              Arch         Version              Repository         Size\n================================================================================\nInstalling:\n slurm-slurmdbd       x86_64       17.02.7-1.el6        local-slurm       1.1 M\n\nTransaction Summary\n================================================================================\nInstall       1 Package(s)\n\nTotal download size: 1.1 M\nInstalled size: 3.9 M\nDownloading Packages:\nRunning rpm_check_debug\nRunning Transaction Test\nTransaction Test Succeeded\nRunning Transaction\n\r  Installing : slurm-slurmdbd-17.02.7-1.el6.x86_64                          1/1 \n\r  Verifying  : slurm-slurmdbd-17.02.7-1.el6.x86_64                          1/1 \n\nInstalled:\n  slurm-slurmdbd.x86_64 0:17.02.7-1.el6                                         \n\nComplete!\n"]}

TASK [slurm-master : Ensure `slurmdbd` is running] *****************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-master/tasks/install-slurmdbd.yml:26
changed: [frontend001] => (item=slurmdbd) => {"changed": true, "enabled": true, "item": "slurmdbd", "name": "slurmdbd", "state": "started"}

TASK [slurm-master : Install SLURM master packages] ****************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-master/tasks/install-slurmctld.yml:3
ok: [frontend001] => (item=[u'mailx', u'slurm-plugins', u'slurm']) => {"changed": false, "item": ["mailx", "slurm-plugins", "slurm"], "msg": "", "rc": 0, "results": ["mailx-12.4-8.el6_6.x86_64 providing mailx is already installed", "slurm-plugins-17.02.7-1.el6.x86_64 providing slurm-plugins is already installed", "slurm-17.02.7-1.el6.x86_64 providing slurm is already installed"]}

TASK [slurm-master : Create cluster in accounting database] ********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-master/tasks/main.yml:13
changed: [frontend001] => {"changed": true, "cmd": "sacctmgr --parsable --noheader list cluster | grep '^elasticluster|' || sacctmgr -i -Q add cluster elasticluster", "delta": "0:00:00.330410", "end": "2018-04-26 09:44:49.792670", "rc": 0, "start": "2018-04-26 09:44:49.462260", "stderr": "", "stdout": "elasticluster|192.168.0.171|6817|7936|1||||||||normal||", "stdout_lines": ["elasticluster|192.168.0.171|6817|7936|1||||||||normal||"], "warnings": []}

TASK [slurm-master : Create an account for default cluster] ********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-master/tasks/main.yml:20
changed: [frontend001] => {"changed": true, "cmd": "sacctmgr --immediate --parsable --noheader list account Cluster=elasticluster | grep '^root|' || sacctmgr -i --quiet add account root Cluster=elasticluster", "delta": "0:00:00.169220", "end": "2018-04-26 09:44:50.124797", "rc": 0, "start": "2018-04-26 09:44:49.955577", "stderr": "", "stdout": "root|default root account|root|", "stdout_lines": ["root|default root account|root|"], "warnings": []}

TASK [slurm-master : Add default user to cluster] ******************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-master/tasks/main.yml:27
changed: [frontend001] => (item=root) => {"changed": true, "cmd": "sacctmgr --immediate --parsable --noheader list user Account=root | grep '^root|' || sacctmgr --immediate --quiet add user 'root' DefaultAccount=root", "delta": "0:00:00.170051", "end": "2018-04-26 09:44:50.461648", "item": "root", "rc": 0, "start": "2018-04-26 09:44:50.291597", "stderr": "", "stdout": "root|root|Administrator|", "stdout_lines": ["root|root|Administrator|"], "warnings": []}

TASK [slurm-master : Ensure `slurmctld` is running] ****************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-master/tasks/main.yml:35
changed: [frontend001] => (item=slurm) => {"changed": true, "enabled": true, "item": "slurm", "name": "slurm", "state": "started"}

PLAY [Slurm worker nodes Playbook] *********************************************

TASK [setup] *******************************************************************
ok: [compute002]
ok: [compute001]

TASK [nis : Load distribution-specific parameters] *****************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/main.yml:3
included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/init-RedHat.yml for compute001, compute002

TASK [nis : Set NIS common playbook params (CentOS/RHEL)] **********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/init-RedHat.yml:3
ok: [compute001] => {"ansible_facts": {"nis_client_packages": ["ypbind"], "nis_client_services": ["rpcbind", "ypbind"], "nis_common_packages": ["yp-tools"], "nis_master_packages": ["ypserv"], "nis_master_services": ["rpcbind", "yppasswdd", "ypserv"], "nis_securenets_path": "/var/yp/securenets"}, "changed": false}
ok: [compute002] => {"ansible_facts": {"nis_client_packages": ["ypbind"], "nis_client_services": ["rpcbind", "ypbind"], "nis_common_packages": ["yp-tools"], "nis_master_packages": ["ypserv"], "nis_master_services": ["rpcbind", "yppasswdd", "ypserv"], "nis_securenets_path": "/var/yp/securenets"}, "changed": false}

TASK [nis : Pre-load debconf answer to questions (Debian/Ubuntu)] **************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/main.yml:7
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [nis : Deploy additional NIS configuration (Debian/Ubuntu)] ***************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/main.yml:18
skipping: [compute001] => (item=etc/default/nis)  => {"changed": false, "item": "etc/default/nis", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => (item=etc/defaultdomain)  => {"changed": false, "item": "etc/defaultdomain", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item=etc/default/nis)  => {"changed": false, "item": "etc/default/nis", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item=etc/defaultdomain)  => {"changed": false, "item": "etc/defaultdomain", "skip_reason": "Conditional check failed", "skipped": true}

TASK [nis : Set NIS domain (CentOS/RHEL)] **************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/main.yml:29
changed: [compute002] => {"backup": "", "changed": true, "msg": "line added"}
changed: [compute001] => {"backup": "", "changed": true, "msg": "line added"}

TASK [nis : Install NIS common packages] ***************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/main.yml:39
ok: [compute002] => (item=[u'yp-tools']) => {"changed": false, "item": ["yp-tools"], "msg": "", "rc": 0, "results": ["yp-tools-2.9-12.el6.x86_64 providing yp-tools is already installed"]}
ok: [compute001] => (item=[u'yp-tools']) => {"changed": false, "item": ["yp-tools"], "msg": "", "rc": 0, "results": ["yp-tools-2.9-12.el6.x86_64 providing yp-tools is already installed"]}

TASK [nis : Deploy `ypserv` configuration files] *******************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml:3
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [nis : Deploy `yppasswdd` configuration file (CentOS/RHEL)] ***************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml:12
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [nis : Install NIS master server packages] ********************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml:22
skipping: [compute001] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}

TASK [nis : Ensure `ypserv` starts at boot] ************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml:33
skipping: [compute001] => (item=rpcbind)  => {"changed": false, "item": "rpcbind", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item=rpcbind)  => {"changed": false, "item": "rpcbind", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => (item=yppasswdd)  => {"changed": false, "item": "yppasswdd", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item=yppasswdd)  => {"changed": false, "item": "yppasswdd", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => (item=ypserv)  => {"changed": false, "item": "ypserv", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item=ypserv)  => {"changed": false, "item": "ypserv", "skip_reason": "Conditional check failed", "skipped": true}

TASK [nis : Update NIS/YP databases (NIS master server)] ***********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml:41
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [nis : Deploy `ypbind` configuration files] *******************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypbind.yml:3
ok: [compute002] => {"changed": false, "gid": 0, "group": "root", "mode": "0400", "owner": "root", "path": "/etc/yp.conf", "size": 725, "state": "file", "uid": 0}
ok: [compute001] => {"changed": false, "gid": 0, "group": "root", "mode": "0400", "owner": "root", "path": "/etc/yp.conf", "size": 725, "state": "file", "uid": 0}

TASK [nis : Ensure NIS/YP is used as a name service] ***************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypbind.yml:12
ok: [compute001] => (item=passwd) => {"backup": "", "changed": false, "item": "passwd", "msg": ""}
ok: [compute002] => (item=passwd) => {"backup": "", "changed": false, "item": "passwd", "msg": ""}
ok: [compute002] => (item=group) => {"backup": "", "changed": false, "item": "group", "msg": ""}
ok: [compute001] => (item=group) => {"backup": "", "changed": false, "item": "group", "msg": ""}
ok: [compute002] => (item=shadow) => {"backup": "", "changed": false, "item": "shadow", "msg": ""}
ok: [compute001] => (item=shadow) => {"backup": "", "changed": false, "item": "shadow", "msg": ""}

TASK [nis : Replace `compat` in `/etc/nsswitch.conf` with `files nis`] *********
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypbind.yml:26
ok: [compute001] => (item=passwd) => {"changed": false, "item": "passwd", "msg": ""}
ok: [compute002] => (item=passwd) => {"changed": false, "item": "passwd", "msg": ""}
ok: [compute001] => (item=group) => {"changed": false, "item": "group", "msg": ""}
ok: [compute002] => (item=group) => {"changed": false, "item": "group", "msg": ""}
ok: [compute001] => (item=shadow) => {"changed": false, "item": "shadow", "msg": ""}
ok: [compute002] => (item=shadow) => {"changed": false, "item": "shadow", "msg": ""}

TASK [nis : Ensure `ypbind` starts at boot] ************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypbind.yml:37
ok: [compute001] => (item=rpcbind) => {"changed": false, "enabled": true, "item": "rpcbind", "name": "rpcbind", "state": "started"}
ok: [compute002] => (item=rpcbind) => {"changed": false, "enabled": true, "item": "rpcbind", "name": "rpcbind", "state": "started"}
ok: [compute001] => (item=ypbind) => {"changed": false, "enabled": true, "item": "ypbind", "name": "ypbind", "state": "started"}
ok: [compute002] => (item=ypbind) => {"changed": false, "enabled": true, "item": "ypbind", "name": "ypbind", "state": "started"}

TASK [nfs-client : install NFS client software (Debian/Ubuntu)] ****************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-client/tasks/main.yml:3
skipping: [compute001] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}

TASK [nfs-client : install NFS client software (RHEL-compatible)] **************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-client/tasks/main.yml:15
ok: [compute001] => (item=[u'nfs-utils', u'nfs4-acl-tools']) => {"changed": false, "item": ["nfs-utils", "nfs4-acl-tools"], "msg": "", "rc": 0, "results": ["nfs-utils-1:1.2.3-75.el6_9.x86_64 providing nfs-utils is already installed", "nfs4-acl-tools-0.3.3-8.el6.x86_64 providing nfs4-acl-tools is already installed"]}
ok: [compute002] => (item=[u'nfs-utils', u'nfs4-acl-tools']) => {"changed": false, "item": ["nfs-utils", "nfs4-acl-tools"], "msg": "", "rc": 0, "results": ["nfs-utils-1:1.2.3-75.el6_9.x86_64 providing nfs-utils is already installed", "nfs4-acl-tools-0.3.3-8.el6.x86_64 providing nfs4-acl-tools is already installed"]}

TASK [nfs-client : Ensure `rpcbind` is running (Debian)] ***********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-client/tasks/main.yml:28
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [nfs-client : Ensure `rpcbind` is running (RHEL-compatible)] **************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-client/tasks/main.yml:35
ok: [compute001] => {"changed": false, "enabled": true, "name": "rpcbind", "state": "started"}
ok: [compute002] => {"changed": false, "enabled": true, "name": "rpcbind", "state": "started"}

TASK [nfs-client : Ensure `portmap` is running (Ubuntu prior to 14.04)] ********
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-client/tasks/main.yml:42
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [nfs-client : Ensure `rpcbind` is running (Ubuntu 14.04 or newer)] ********
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-client/tasks/main.yml:49
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [nfs-client : Mount NFS filesystems] **************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-client/tasks/main.yml:57
included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-client/tasks/nfsmount.yml for compute001, compute002

TASK [nfs-client : ensure /home directory exists] ******************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-client/tasks/nfsmount.yml:3
ok: [compute001] => {"changed": false, "gid": 0, "group": "root", "mode": "0755", "owner": "root", "path": "/home", "size": 4096, "state": "directory", "uid": 0}
ok: [compute002] => {"changed": false, "gid": 0, "group": "root", "mode": "0755", "owner": "root", "path": "/home", "size": 4096, "state": "directory", "uid": 0}

TASK [nfs-client : add to /etc/fstab] ******************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-client/tasks/nfsmount.yml:8
changed: [compute001] => {"changed": true, "dump": "0", "fstab": "/etc/fstab", "fstype": "nfs", "name": "/home", "opts": "rw,async", "passno": "0", "src": "frontend001:/home"}
changed: [compute002] => {"changed": true, "dump": "0", "fstab": "/etc/fstab", "fstype": "nfs", "name": "/home", "opts": "rw,async", "passno": "0", "src": "frontend001:/home"}

TASK [autofs : Load distribution-specific parameters] **************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/autofs/tasks/main.yml:8
included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/autofs/tasks/init-RedHat.yml for compute001, compute002

TASK [autofs : Provide RHEL-specific values] ***********************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/autofs/tasks/init-RedHat.yml:6
ok: [compute001] => {"ansible_facts": {"autofs_packages": ["autofs"], "autofs_service": "autofs", "autofs_username": "root"}, "changed": false}
ok: [compute002] => {"ansible_facts": {"autofs_packages": ["autofs"], "autofs_service": "autofs", "autofs_username": "root"}, "changed": false}

TASK [autofs : Deploy autofs configuration] ************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/autofs/tasks/main.yml:12
ok: [compute002] => (item=etc/auto.master) => {"changed": false, "gid": 0, "group": "root", "item": "etc/auto.master", "mode": "0444", "owner": "root", "path": "/etc/auto.master", "size": 540, "state": "file", "uid": 0}
ok: [compute001] => (item=etc/auto.master) => {"changed": false, "gid": 0, "group": "root", "item": "etc/auto.master", "mode": "0444", "owner": "root", "path": "/etc/auto.master", "size": 540, "state": "file", "uid": 0}
ok: [compute002] => (item=etc/auto.home) => {"changed": false, "gid": 0, "group": "root", "item": "etc/auto.home", "mode": "0444", "owner": "root", "path": "/etc/auto.home", "size": 852, "state": "file", "uid": 0}
ok: [compute001] => (item=etc/auto.home) => {"changed": false, "gid": 0, "group": "root", "item": "etc/auto.home", "mode": "0444", "owner": "root", "path": "/etc/auto.home", "size": 852, "state": "file", "uid": 0}

TASK [autofs : Deploy autofs mount script for NFSv4 (Debian/Ubuntu)] ***********
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/autofs/tasks/main.yml:24
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [autofs : Install Autofs] *************************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/autofs/tasks/main.yml:34
ok: [compute001] => (item=[u'autofs']) => {"changed": false, "item": ["autofs"], "msg": "", "rc": 0, "results": ["autofs-1:5.0.5-133.el6_9.x86_64 providing autofs is already installed"]}
ok: [compute002] => (item=[u'autofs']) => {"changed": false, "item": ["autofs"], "msg": "", "rc": 0, "results": ["autofs-1:5.0.5-133.el6_9.x86_64 providing autofs is already installed"]}

TASK [autofs : Ensure autofs is running and starts at boot] ********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/autofs/tasks/main.yml:41
ok: [compute001] => {"changed": false, "enabled": true, "name": "autofs", "state": "started"}
ok: [compute002] => {"changed": false, "enabled": true, "name": "autofs", "state": "started"}

TASK [slurm-common : Load distribution-specific parameters] ********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:3
included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/init-RedHat.yml for compute001, compute002

TASK [slurm-common : Set SLURM common playbook params (RHEL 7.x compatible)] ***
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/init-RedHat.yml:3
ok: [compute001] => {"ansible_facts": {"slurm_pid_dir": "/var/run"}, "changed": false}
ok: [compute002] => {"ansible_facts": {"slurm_pid_dir": "/var/run"}, "changed": false}

TASK [slurm-common : Create `slurm` system group] ******************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:16
ok: [compute001] => {"changed": false, "gid": 498, "name": "slurm", "state": "present", "system": true}
ok: [compute002] => {"changed": false, "gid": 498, "name": "slurm", "state": "present", "system": true}

TASK [slurm-common : Create `slurm` system user] *******************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:24
ok: [compute001] => {"append": false, "changed": false, "comment": "", "group": 498, "home": "/home/slurm", "move_home": false, "name": "slurm", "shell": "/bin/bash", "state": "present", "uid": 498}
ok: [compute002] => {"append": false, "changed": false, "comment": "", "group": 498, "home": "/home/slurm", "move_home": false, "name": "slurm", "shell": "/bin/bash", "state": "present", "uid": 498}

TASK [slurm-common : Create work directory {{item}}] ***************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:33
changed: [compute001] => (item=/etc/slurm) => {"changed": true, "gid": 498, "group": "slurm", "item": "/etc/slurm", "mode": "0755", "owner": "slurm", "path": "/etc/slurm", "size": 4096, "state": "directory", "uid": 498}
changed: [compute002] => (item=/etc/slurm) => {"changed": true, "gid": 498, "group": "slurm", "item": "/etc/slurm", "mode": "0755", "owner": "slurm", "path": "/etc/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [compute001] => (item=/var/lib/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/lib/slurm", "mode": "0755", "owner": "slurm", "path": "/var/lib/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [compute002] => (item=/var/lib/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/lib/slurm", "mode": "0755", "owner": "slurm", "path": "/var/lib/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [compute001] => (item=/var/lib/slurm/checkpoint) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/lib/slurm/checkpoint", "mode": "0755", "owner": "slurm", "path": "/var/lib/slurm/checkpoint", "size": 4096, "state": "directory", "uid": 498}
ok: [compute002] => (item=/var/lib/slurm/checkpoint) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/lib/slurm/checkpoint", "mode": "0755", "owner": "slurm", "path": "/var/lib/slurm/checkpoint", "size": 4096, "state": "directory", "uid": 498}
ok: [compute002] => (item=/var/lib/slurm/slurmctld) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/lib/slurm/slurmctld", "mode": "0755", "owner": "slurm", "path": "/var/lib/slurm/slurmctld", "size": 4096, "state": "directory", "uid": 498}
ok: [compute001] => (item=/var/lib/slurm/slurmctld) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/lib/slurm/slurmctld", "mode": "0755", "owner": "slurm", "path": "/var/lib/slurm/slurmctld", "size": 4096, "state": "directory", "uid": 498}
ok: [compute002] => (item=/var/lib/slurm/slurmd) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/lib/slurm/slurmd", "mode": "0755", "owner": "slurm", "path": "/var/lib/slurm/slurmd", "size": 4096, "state": "directory", "uid": 498}
ok: [compute001] => (item=/var/lib/slurm/slurmd) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/lib/slurm/slurmd", "mode": "0755", "owner": "slurm", "path": "/var/lib/slurm/slurmd", "size": 4096, "state": "directory", "uid": 498}
ok: [compute002] => (item=/var/log/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/log/slurm", "mode": "0755", "owner": "slurm", "path": "/var/log/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [compute001] => (item=/var/log/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/log/slurm", "mode": "0755", "owner": "slurm", "path": "/var/log/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [compute002] => (item=/var/run/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/run/slurm", "mode": "0755", "owner": "slurm", "path": "/var/run/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [compute001] => (item=/var/run/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/run/slurm", "mode": "0755", "owner": "slurm", "path": "/var/run/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [compute001] => (item=/var/spool/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/spool/slurm", "mode": "0755", "owner": "slurm", "path": "/var/spool/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [compute002] => (item=/var/spool/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/spool/slurm", "mode": "0755", "owner": "slurm", "path": "/var/spool/slurm", "size": 4096, "state": "directory", "uid": 498}

TASK [slurm-common : Make compatibility symlinks] ******************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:56
skipping: [compute001] => (item={u'to': u'/etc/slurm-llnl', u'from': u'/etc/slurm'})  => {"changed": false, "item": {"from": "/etc/slurm", "to": "/etc/slurm-llnl"}, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => (item={u'to': u'/var/lib/slurm-llnl', u'from': u'/var/lib/slurm'})  => {"changed": false, "item": {"from": "/var/lib/slurm", "to": "/var/lib/slurm-llnl"}, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item={u'to': u'/etc/slurm-llnl', u'from': u'/etc/slurm'})  => {"changed": false, "item": {"from": "/etc/slurm", "to": "/etc/slurm-llnl"}, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => (item={u'to': u'/var/log/slurm-llnl', u'from': u'/var/log/slurm'})  => {"changed": false, "item": {"from": "/var/log/slurm", "to": "/var/log/slurm-llnl"}, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item={u'to': u'/var/lib/slurm-llnl', u'from': u'/var/lib/slurm'})  => {"changed": false, "item": {"from": "/var/lib/slurm", "to": "/var/lib/slurm-llnl"}, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => (item={u'to': u'/var/run/slurm-llnl', u'from': u'/var/run/slurm'})  => {"changed": false, "item": {"from": "/var/run/slurm", "to": "/var/run/slurm-llnl"}, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item={u'to': u'/var/log/slurm-llnl', u'from': u'/var/log/slurm'})  => {"changed": false, "item": {"from": "/var/log/slurm", "to": "/var/log/slurm-llnl"}, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item={u'to': u'/var/run/slurm-llnl', u'from': u'/var/run/slurm'})  => {"changed": false, "item": {"from": "/var/run/slurm", "to": "/var/run/slurm-llnl"}, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-common : Enable Copr SLURM repo by verdurin] ***********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:73
ok: [compute001] => {"changed": false, "gid": 0, "group": "root", "mode": "0444", "owner": "root", "path": "/etc/yum.repos.d/copr-slurm.repo", "size": 615, "state": "file", "uid": 0}
ok: [compute002] => {"changed": false, "gid": 0, "group": "root", "mode": "0444", "owner": "root", "path": "/etc/yum.repos.d/copr-slurm.repo", "size": 615, "state": "file", "uid": 0}

TASK [slurm-common : Prevent MUNGE from starting] ******************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:20
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-common : Install MUNGE (Debian/Ubuntu)] ****************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:31
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-common : Install MUNGE (RHEL-compatible)] **************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:43
ok: [compute001] => {"changed": false, "msg": "", "rc": 0, "results": ["slurm-munge-17.02.7-1.el6.x86_64 providing slurm-munge is already installed"]}
ok: [compute002] => {"changed": false, "msg": "", "rc": 0, "results": ["slurm-munge-17.02.7-1.el6.x86_64 providing slurm-munge is already installed"]}

TASK [slurm-common : Allow MUNGE to start] *************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:57
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-common : Add `--syslog` option to `munged` startup] ****************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:66
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-common : Reload systemd configuration] *****************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:77
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-common : Add `--syslog` startup options for `munged` (Ubuntu 14.04)] ***
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:86
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-common : Configure MUNGE] ******************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:101
ok: [compute002] => {"changed": false, "checksum": "222afe887f907039b799b394715664ffa584fec5", "dest": "/etc/munge/munge.key", "gid": 497, "group": "munge", "mode": "0400", "owner": "munge", "path": "/etc/munge/munge.key", "size": 1024, "state": "file", "uid": 497}
ok: [compute001] => {"changed": false, "checksum": "222afe887f907039b799b394715664ffa584fec5", "dest": "/etc/munge/munge.key", "gid": 497, "group": "munge", "mode": "0400", "owner": "munge", "path": "/etc/munge/munge.key", "size": 1024, "state": "file", "uid": 497}

TASK [slurm-common : Ensure the MUNGE service is running] **********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:114
ok: [compute002] => {"changed": false, "enabled": true, "name": "munge", "state": "started"}
ok: [compute001] => {"changed": false, "enabled": true, "name": "munge", "state": "started"}

TASK [slurm-common : Deploy SLURM configuration file] **************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:86
ok: [compute002] => {"changed": false, "gid": 0, "group": "root", "mode": "0444", "owner": "root", "path": "/etc/slurm/slurm.conf", "size": 4156, "state": "file", "uid": 0}
ok: [compute001] => {"changed": false, "gid": 0, "group": "root", "mode": "0444", "owner": "root", "path": "/etc/slurm/slurm.conf", "size": 4156, "state": "file", "uid": 0}

TASK [slurm-common : Deploy script to suspend cloud instance] ******************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:96
ok: [compute001] => {"changed": false, "gid": 0, "group": "root", "mode": "0555", "owner": "root", "path": "/etc/slurm/slurm.suspend.sh", "size": 604, "state": "file", "uid": 0}
ok: [compute002] => {"changed": false, "gid": 0, "group": "root", "mode": "0555", "owner": "root", "path": "/etc/slurm/slurm.suspend.sh", "size": 604, "state": "file", "uid": 0}

TASK [slurm-common : Deploy script to resume cloud instance] *******************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:106
ok: [compute001] => {"changed": false, "gid": 0, "group": "root", "mode": "0555", "owner": "root", "path": "/etc/slurm/slurm.resume.sh", "size": 631, "state": "file", "uid": 0}
ok: [compute002] => {"changed": false, "gid": 0, "group": "root", "mode": "0555", "owner": "root", "path": "/etc/slurm/slurm.resume.sh", "size": 631, "state": "file", "uid": 0}

TASK [slurm-common : Install support packages (Debian/Ubuntu)] *****************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:116
skipping: [compute001] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-common : Install SLURM packages (RHEL-compatible)] *****************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:127
skipping: [compute001] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-client : Install required SLURM packages (Debian/Ubuntu)] **********
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-client/tasks/main.yml:4
skipping: [compute001] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-client : Install required SLURM packages (older Debian/Ubuntu)] ****
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-client/tasks/main.yml:23
skipping: [compute001] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-client : service] **************************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-client/tasks/main.yml:35
skipping: [compute001] => (item=slurm-llnl)  => {"changed": false, "item": "slurm-llnl", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item=slurm-llnl)  => {"changed": false, "item": "slurm-llnl", "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-client : Install SLURM packages (RHEL-compatible)] *****************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-client/tasks/main.yml:47
ok: [compute002] => (item=[u'slurm', u'slurm-devel', u'slurm-perlapi', u'slurm-contribs', u'slurm-torque']) => {"changed": false, "item": ["slurm", "slurm-devel", "slurm-perlapi", "slurm-contribs", "slurm-torque"], "msg": "", "rc": 0, "results": ["slurm-17.02.7-1.el6.x86_64 providing slurm is already installed", "slurm-devel-17.02.7-1.el6.x86_64 providing slurm-devel is already installed", "slurm-perlapi-17.02.7-1.el6.x86_64 providing slurm-perlapi is already installed", "slurm-contribs-17.02.7-1.el6.x86_64 providing slurm-contribs is already installed", "slurm-torque-17.02.7-1.el6.x86_64 providing slurm-torque is already installed"]}
ok: [compute001] => (item=[u'slurm', u'slurm-devel', u'slurm-perlapi', u'slurm-contribs', u'slurm-torque']) => {"changed": false, "item": ["slurm", "slurm-devel", "slurm-perlapi", "slurm-contribs", "slurm-torque"], "msg": "", "rc": 0, "results": ["slurm-17.02.7-1.el6.x86_64 providing slurm is already installed", "slurm-devel-17.02.7-1.el6.x86_64 providing slurm-devel is already installed", "slurm-perlapi-17.02.7-1.el6.x86_64 providing slurm-perlapi is already installed", "slurm-contribs-17.02.7-1.el6.x86_64 providing slurm-contribs is already installed", "slurm-torque-17.02.7-1.el6.x86_64 providing slurm-torque is already installed"]}

TASK [slurm-client : Stop SLURM services (RHEL/CentOS 6.x)] ********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-client/tasks/main.yml:59
changed: [compute002] => {"changed": true, "enabled": false, "name": "slurm", "state": "stopped"}
changed: [compute001] => {"changed": true, "enabled": false, "name": "slurm", "state": "stopped"}

TASK [slurm-client : Stop SLURM services (RHEL/CentOS 7.x)] ********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-client/tasks/main.yml:66
skipping: [compute001] => (item=slurmd)  => {"changed": false, "item": "slurmd", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => (item=slurmctld)  => {"changed": false, "item": "slurmctld", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item=slurmd)  => {"changed": false, "item": "slurmd", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item=slurmctld)  => {"changed": false, "item": "slurmctld", "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-worker : Load distribution-specific parameters] ********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/main.yml:4
included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/init-RedHat.yml for compute001, compute002

TASK [slurm-worker : Set SLURM worker playbook params (RHEL compatible)] *******
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/init-RedHat.yml:3
ok: [compute001] => {"ansible_facts": {"slurmd_packages": ["slurm", "slurm-plugins", "slurm-sql"]}, "changed": false}
ok: [compute002] => {"ansible_facts": {"slurmd_packages": ["slurm", "slurm-plugins", "slurm-sql"]}, "changed": false}

TASK [slurm-worker : Set SLURM worker service name (RHEL 7.x compatible)] ******
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/init-RedHat.yml:11
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-worker : Set SLURM worker service name (RHEL 6.x compatible)] ******
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/init-RedHat.yml:16
ok: [compute001] => {"ansible_facts": {"slurmd_service": "slurm"}, "changed": false}
ok: [compute002] => {"ansible_facts": {"slurmd_service": "slurm"}, "changed": false}

TASK [slurm-worker : Deploy SLURM configuration files] *************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/main.yml:7
ok: [compute002] => (item=gres.conf) => {"changed": false, "gid": 0, "group": "root", "item": "gres.conf", "mode": "0444", "owner": "root", "path": "/etc/slurm/gres.conf", "size": 297, "state": "file", "uid": 0}
ok: [compute001] => (item=gres.conf) => {"changed": false, "gid": 0, "group": "root", "item": "gres.conf", "mode": "0444", "owner": "root", "path": "/etc/slurm/gres.conf", "size": 297, "state": "file", "uid": 0}

TASK [slurm-worker : Deploy kernel config check script] ************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/cgroup.yml:3
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-worker : Check that kernel is configured for cgroups] **************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/cgroup.yml:12
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-worker : Ensure cgroup filesystems are mounted] ********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/cgroup.yml:18
skipping: [compute001] => (item=blkio)  => {"changed": false, "item": "blkio", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => (item=cpuacct)  => {"changed": false, "item": "cpuacct", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item=blkio)  => {"changed": false, "item": "blkio", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => (item=cpuset)  => {"changed": false, "item": "cpuset", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item=cpuacct)  => {"changed": false, "item": "cpuacct", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => (item=devices)  => {"changed": false, "item": "devices", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item=cpuset)  => {"changed": false, "item": "cpuset", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => (item=freezer)  => {"changed": false, "item": "freezer", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item=devices)  => {"changed": false, "item": "devices", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => (item=memory)  => {"changed": false, "item": "memory", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item=freezer)  => {"changed": false, "item": "freezer", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item=memory)  => {"changed": false, "item": "memory", "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-worker : Ensure kernel is booted with swap accounting enabled] *****
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/cgroup.yml:44
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-worker : Check if swap accouting is enabled (may fail!)] ***********
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/cgroup.yml:53
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-worker : Reboot to enable swap accounting] *************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/cgroup.yml:59
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-worker : Wait for server to come up again] *************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/cgroup.yml:72
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-worker : Ensure release agent directory exists] ********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/cgroup.yml:84
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-worker : Deploy cgroup-specific release agent script] **************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/cgroup.yml:93
skipping: [compute001] => (item=blkio)  => {"changed": false, "item": "blkio", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => (item=cpuacct)  => {"changed": false, "item": "cpuacct", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item=blkio)  => {"changed": false, "item": "blkio", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item=cpuacct)  => {"changed": false, "item": "cpuacct", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => (item=cpuset)  => {"changed": false, "item": "cpuset", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item=cpuset)  => {"changed": false, "item": "cpuset", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => (item=devices)  => {"changed": false, "item": "devices", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item=devices)  => {"changed": false, "item": "devices", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => (item=freezer)  => {"changed": false, "item": "freezer", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item=freezer)  => {"changed": false, "item": "freezer", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => (item=memory)  => {"changed": false, "item": "memory", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item=memory)  => {"changed": false, "item": "memory", "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-worker : Deploy SLURM cgroup configuration file (main)] ************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/cgroup.yml:103
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-worker : Deploy SLURM cgroup configuration file (devices)] *********
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/cgroup.yml:112
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-worker : Install SLURM worker packages] ****************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/main.yml:27
ok: [compute001] => (item=[u'slurm', u'slurm-plugins', u'slurm-sql']) => {"changed": false, "item": ["slurm", "slurm-plugins", "slurm-sql"], "msg": "", "rc": 0, "results": ["slurm-17.02.7-1.el6.x86_64 providing slurm is already installed", "slurm-plugins-17.02.7-1.el6.x86_64 providing slurm-plugins is already installed", "slurm-sql-17.02.7-1.el6.x86_64 providing slurm-sql is already installed"]}
ok: [compute002] => (item=[u'slurm', u'slurm-plugins', u'slurm-sql']) => {"changed": false, "item": ["slurm", "slurm-plugins", "slurm-sql"], "msg": "", "rc": 0, "results": ["slurm-17.02.7-1.el6.x86_64 providing slurm is already installed", "slurm-plugins-17.02.7-1.el6.x86_64 providing slurm-plugins is already installed", "slurm-sql-17.02.7-1.el6.x86_64 providing slurm-sql is already installed"]}

TASK [slurm-worker : Ensure SLURMd starts at boot] *****************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/main.yml:33
changed: [compute001] => {"changed": true, "enabled": true, "name": "slurm"}
changed: [compute002] => {"changed": true, "enabled": true, "name": "slurm"}

PLAY [Restart SLURMd after all config is done] *********************************

TASK [setup] *******************************************************************
ok: [compute001]
ok: [compute002]

TASK [service] *****************************************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm.yml:47
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [service] *****************************************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm.yml:51
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [service] *****************************************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm.yml:55
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [service] *****************************************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm.yml:59
changed: [compute001] => {"changed": true, "name": "slurm", "state": "started"}
changed: [compute002] => {"changed": true, "name": "slurm", "state": "started"}

PLAY [Apply local customizations (after)] **************************************

TASK [setup] *******************************************************************
ok: [frontend001]
ok: [compute001]
ok: [compute002]

PLAY [Report success on cluster creation] **************************************

TASK [Mark host as successfully configured] ************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/site.yml:79
changed: [frontend001 -> localhost] => {"backup": "", "changed": true, "msg": "line added"}
changed: [compute001 -> localhost] => {"backup": "", "changed": true, "msg": "line added"}
changed: [compute002 -> localhost] => {"backup": "", "changed": true, "msg": "line added"}

PLAY RECAP *********************************************************************
compute001                 : ok=123  changed=41   unreachable=0    failed=0   
compute002                 : ok=123  changed=41   unreachable=0    failed=0   
frontend001                : ok=130  changed=46   unreachable=0    failed=0   

2018-04-26 09:45:06 hwc hwc[3075] INFO Cluster correctly configured.
Starting cluster `slurm` with:
* 1 frontend nodes.
* 2 compute nodes.
(This may take a while...)
Configuring the cluster ...
(this too may take a while)

Your cluster `slurm` is ready!

Cluster name:     slurm
Cluster template: slurm
Default ssh to node: frontend001
- frontend nodes: 1
- compute nodes: 2

To login on the frontend node, run the command:

    elasticluster ssh slurm

To upload or download files to the cluster, use the command:

    elasticluster sftp slurm


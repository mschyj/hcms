2018-05-03 20:34:20 hwc hwcc[4151] DEBUG Checking section `cluster/slurm` ...
2018-05-03 20:34:20 hwc hwcc[4151] DEBUG Checking section `login/linux` ...
2018-05-03 20:34:20 hwc hwcc[4151] DEBUG Checking section `setup/ansible-slurm` ...
2018-05-03 20:34:20 hwc hwcc[4151] DEBUG Checking section `cloud/hwc` ...
2018-05-03 20:34:20 hwc hwcc[4151] DEBUG Using class <class 'elasticluster.providers.openstack.OpenStackCloudProvider'> from module <module 'elasticluster.providers.openstack' from '/usr/local/lib/python2.7/site-packages/hwcc-0.1.0-py2.7.egg/elasticluster/providers/openstack.pyc'> to instanciate provider 'openstack'
2018-05-03 20:34:20 hwc hwcc[4151] DEBUG OpenStack auth URL taken from env variable OS_AUTH_URL
2018-05-03 20:34:20 hwc hwcc[4151] DEBUG OpenStack user name taken from env variable OS_USERNAME
2018-05-03 20:34:20 hwc hwcc[4151] DEBUG OpenStack user domain name taken from env variable OS_USER_DOMAIN_NAME
2018-05-03 20:34:20 hwc hwcc[4151] DEBUG OpenStack password taken from env variable OS_PASSWORD
2018-05-03 20:34:20 hwc hwcc[4151] DEBUG OpenStack project name taken from env variable OS_PROJECT_NAME
2018-05-03 20:34:20 hwc hwcc[4151] DEBUG OpenStack project domain name taken from env variable OS_PROJECT_DOMAIN_NAME
2018-05-03 20:34:20 hwc hwcc[4151] DEBUG Using class <class 'elasticluster.providers.ansible_provider.AnsibleSetupProvider'> from module <module 'elasticluster.providers.ansible_provider' from '/usr/local/lib/python2.7/site-packages/hwcc-0.1.0-py2.7.egg/elasticluster/providers/ansible_provider.pyc'> to instanciate provider 'ansible'
2018-05-03 20:34:20 hwc hwcc[4151] DEBUG setting variable user_ak=CNYVGDKK2PPBQEEMUYQG for node kind master
2018-05-03 20:34:20 hwc hwcc[4151] DEBUG setting variable user_obs_url=http://obs.cn-north-1.myhwclouds.com for node kind master
2018-05-03 20:34:20 hwc hwcc[4151] DEBUG setting variable obs_enabled=False for node kind master
2018-05-03 20:34:20 hwc hwcc[4151] DEBUG setting variable upgrade_packages=no for node kind master
2018-05-03 20:34:20 hwc hwcc[4151] DEBUG setting variable user_obs_endpoint=cn-north-1 for node kind master
2018-05-03 20:34:20 hwc hwcc[4151] DEBUG setting variable slurm_suspendtime=60 for node kind master
2018-05-03 20:34:20 hwc hwcc[4151] DEBUG setting variable user_obs_name=hwc-obs for node kind master
2018-05-03 20:34:20 hwc hwcc[4151] DEBUG setting variable user_sfs_url=sfs-nas1.cn-north-1.myhuaweicloud.com:/share-480fca1b for node kind master
2018-05-03 20:34:20 hwc hwcc[4151] DEBUG setting variable sfs_enabled=True for node kind master
2018-05-03 20:34:20 hwc hwcc[4151] DEBUG setting variable user_client_ip=192.168.0.230 for node kind master
2018-05-03 20:34:20 hwc hwcc[4151] DEBUG setting variable user_sk=6imAmmCVVzMqR1dm5x0AKJcL1BILA7TlOsSMhu4G for node kind master
2018-05-03 20:34:20 hwc hwcc[4151] DEBUG setting variable user_ak=CNYVGDKK2PPBQEEMUYQG for node kind worker
2018-05-03 20:34:20 hwc hwcc[4151] DEBUG setting variable user_obs_url=http://obs.cn-north-1.myhwclouds.com for node kind worker
2018-05-03 20:34:20 hwc hwcc[4151] DEBUG setting variable obs_enabled=False for node kind worker
2018-05-03 20:34:20 hwc hwcc[4151] DEBUG setting variable upgrade_packages=no for node kind worker
2018-05-03 20:34:20 hwc hwcc[4151] DEBUG setting variable user_obs_endpoint=cn-north-1 for node kind worker
2018-05-03 20:34:20 hwc hwcc[4151] DEBUG setting variable slurm_suspendtime=60 for node kind worker
2018-05-03 20:34:20 hwc hwcc[4151] DEBUG setting variable user_obs_name=hwc-obs for node kind worker
2018-05-03 20:34:20 hwc hwcc[4151] DEBUG setting variable user_sfs_url=sfs-nas1.cn-north-1.myhuaweicloud.com:/share-480fca1b for node kind worker
2018-05-03 20:34:20 hwc hwcc[4151] DEBUG setting variable sfs_enabled=True for node kind worker
2018-05-03 20:34:20 hwc hwcc[4151] DEBUG setting variable user_client_ip=192.168.0.230 for node kind worker
2018-05-03 20:34:20 hwc hwcc[4151] DEBUG setting variable user_sk=6imAmmCVVzMqR1dm5x0AKJcL1BILA7TlOsSMhu4G for node kind worker
2018-05-03 20:34:20 hwc hwcc[4151] INFO Starting cluster nodes (timeout: 600 seconds) ...
2018-05-03 20:34:20 hwc hwcc[4151] DEBUG Note: starting 3 nodes concurrently.
2018-05-03 20:34:20 hwc hwcc[4151] DEBUG _start_node: working on node `worker001`
2018-05-03 20:34:20 hwc hwcc[4151] INFO Starting node `worker001` from image `befbd7ca-dd7a-4737-bc60-33a1cdf6ea8b` with flavor c1.medium ...
2018-05-03 20:34:20 hwc hwcc[4151] DEBUG Initializing OpenStack API clients: OS_AUTH_URL='https://iam.cn-north-1.myhwclouds.com/v3' OS_USERNAME='mschyj' OS_USER_DOMAIN_NAME='mschyj' OS_PROJECT_NAME='cn-north-1' OS_PROJECT_DOMAIN_NAME='mschyj' OS_REGION_NAME='cn-north-1' OS_CACERT='None'
2018-05-03 20:34:20 hwc hwcc[4151] INFO Using Keystone API v3 session to authenticate to OpenStack
2018-05-03 20:34:20 hwc hwcc[4151] DEBUG Creating OpenStack Compute API (Nova) v2 client ...
2018-05-03 20:34:20 hwc hwcc[4151] DEBUG _start_node: working on node `worker002`
2018-05-03 20:34:20 hwc hwcc[4151] INFO Starting node `worker002` from image `befbd7ca-dd7a-4737-bc60-33a1cdf6ea8b` with flavor c1.medium ...
2018-05-03 20:34:20 hwc hwcc[4151] DEBUG Initializing OpenStack API clients: OS_AUTH_URL='https://iam.cn-north-1.myhwclouds.com/v3' OS_USERNAME='mschyj' OS_USER_DOMAIN_NAME='mschyj' OS_PROJECT_NAME='cn-north-1' OS_PROJECT_DOMAIN_NAME='mschyj' OS_REGION_NAME='cn-north-1' OS_CACERT='None'
2018-05-03 20:34:20 hwc hwcc[4151] DEBUG _start_node: working on node `master001`
2018-05-03 20:34:20 hwc hwcc[4151] INFO Starting node `master001` from image `befbd7ca-dd7a-4737-bc60-33a1cdf6ea8b` with flavor s2.small.1 ...
2018-05-03 20:34:20 hwc hwcc[4151] DEBUG Initializing OpenStack API clients: OS_AUTH_URL='https://iam.cn-north-1.myhwclouds.com/v3' OS_USERNAME='mschyj' OS_USER_DOMAIN_NAME='mschyj' OS_PROJECT_NAME='cn-north-1' OS_PROJECT_DOMAIN_NAME='mschyj' OS_REGION_NAME='cn-north-1' OS_CACERT='None'
2018-05-03 20:34:20 hwc hwcc[4151] INFO Using Keystone API v3 session to authenticate to OpenStack
2018-05-03 20:34:20 hwc hwcc[4151] DEBUG Creating OpenStack Compute API (Nova) v2 client ...
2018-05-03 20:34:20 hwc hwcc[4151] DEBUG Creating OpenStack Network API (Neutron) client ...
2018-05-03 20:34:20 hwc hwcc[4151] INFO Using Keystone API v3 session to authenticate to OpenStack
2018-05-03 20:34:20 hwc hwcc[4151] DEBUG Creating OpenStack Compute API (Nova) v2 client ...
2018-05-03 20:34:20 hwc hwcc[4151] DEBUG Creating OpenStack Network API (Neutron) client ...
2018-05-03 20:34:20 hwc hwcc[4151] DEBUG Creating OpenStack Network API (Neutron) client ...
2018-05-03 20:34:20 hwc hwcc[4151] DEBUG Creating OpenStack Image API (Glance) v2 client ...
2018-05-03 20:34:20 hwc hwcc[4151] DEBUG Creating OpenStack Image API (Glance) v2 client ...
2018-05-03 20:34:20 hwc hwcc[4151] DEBUG Creating OpenStack Image API (Glance) v2 client ...
2018-05-03 20:34:20 hwc hwcc[4151] DEBUG Creating OpenStack Volume API (Cinder) v2 client ...
2018-05-03 20:34:20 hwc hwcc[4151] DEBUG Creating OpenStack Volume API (Cinder) v2 client ...
2018-05-03 20:34:20 hwc hwcc[4151] DEBUG Creating OpenStack Volume API (Cinder) v2 client ...
2018-05-03 20:34:20 hwc hwcc[4151] DEBUG Checking keypair `hwc-key` ...
2018-05-03 20:34:20 hwc hwcc[4151] DEBUG Checking keypair `hwc-key` ...
2018-05-03 20:34:20 hwc hwcc[4151] DEBUG Checking keypair `hwc-key` ...
2018-05-03 20:34:21 hwc hwcc[4151] DEBUG Checking existence of security group(s) ['security-mschyj'] ...
2018-05-03 20:34:21 hwc hwcc[4151] DEBUG Checking existence of security group(s) ['security-mschyj'] ...
2018-05-03 20:34:21 hwc hwcc[4151] DEBUG Checking existence of security group(s) ['security-mschyj'] ...
2018-05-03 20:34:32 hwc hwcc[4151] DEBUG Specifying networks for node slurm-worker001: 6c7d662f-eb15-4a44-b2ef-2e71ff7fce15
2018-05-03 20:34:32 hwc hwcc[4151] DEBUG Specifying networks for node slurm-master001: 6c7d662f-eb15-4a44-b2ef-2e71ff7fce15
2018-05-03 20:34:33 hwc hwcc[4151] DEBUG Specifying networks for node slurm-worker002: 6c7d662f-eb15-4a44-b2ef-2e71ff7fce15
2018-05-03 20:34:37 hwc hwcc[4151] DEBUG Node `worker001` has instance ID `05ba31b4-74c1-4c2d-af94-a347ec9d7566`
2018-05-03 20:34:37 hwc hwcc[4151] INFO Node `worker001` has been started.
2018-05-03 20:34:37 hwc hwcc[4151] DEBUG Node `master001` has instance ID `abe6c209-94d4-4c74-9f16-395c454deef6`
2018-05-03 20:34:37 hwc hwcc[4151] INFO Node `master001` has been started.
2018-05-03 20:34:38 hwc hwcc[4151] DEBUG Node `worker002` has instance ID `a2314a31-6f30-461f-8667-3411b82b2f18`
2018-05-03 20:34:38 hwc hwcc[4151] INFO Node `worker002` has been started.
2018-05-03 20:34:38 hwc hwcc[4151] DEBUG Getting information for instance a2314a31-6f30-461f-8667-3411b82b2f18
2018-05-03 20:34:39 hwc hwcc[4151] DEBUG node `worker002` (instance id `a2314a31-6f30-461f-8667-3411b82b2f18`) still building...
2018-05-03 20:34:39 hwc hwcc[4151] DEBUG Getting information for instance 05ba31b4-74c1-4c2d-af94-a347ec9d7566
2018-05-03 20:34:39 hwc hwcc[4151] DEBUG node `worker001` (instance id `05ba31b4-74c1-4c2d-af94-a347ec9d7566`) still building...
2018-05-03 20:34:39 hwc hwcc[4151] DEBUG Getting information for instance abe6c209-94d4-4c74-9f16-395c454deef6
2018-05-03 20:34:40 hwc hwcc[4151] DEBUG node `master001` (instance id `abe6c209-94d4-4c74-9f16-395c454deef6`) still building...
2018-05-03 20:34:40 hwc hwcc[4151] DEBUG Waiting for 3 more nodes to come up ...
2018-05-03 20:34:50 hwc hwcc[4151] DEBUG Getting information for instance a2314a31-6f30-461f-8667-3411b82b2f18
2018-05-03 20:34:50 hwc hwcc[4151] DEBUG node `worker002` (instance id `a2314a31-6f30-461f-8667-3411b82b2f18`) still building...
2018-05-03 20:34:50 hwc hwcc[4151] DEBUG Getting information for instance 05ba31b4-74c1-4c2d-af94-a347ec9d7566
2018-05-03 20:34:51 hwc hwcc[4151] DEBUG node `worker001` (instance id `05ba31b4-74c1-4c2d-af94-a347ec9d7566`) still building...
2018-05-03 20:34:51 hwc hwcc[4151] DEBUG Getting information for instance abe6c209-94d4-4c74-9f16-395c454deef6
2018-05-03 20:34:51 hwc hwcc[4151] DEBUG node `master001` (instance id `abe6c209-94d4-4c74-9f16-395c454deef6`) still building...
2018-05-03 20:34:51 hwc hwcc[4151] DEBUG Waiting for 3 more nodes to come up ...
2018-05-03 20:35:01 hwc hwcc[4151] DEBUG Getting information for instance a2314a31-6f30-461f-8667-3411b82b2f18
2018-05-03 20:35:01 hwc hwcc[4151] DEBUG node `worker002` (instance id `a2314a31-6f30-461f-8667-3411b82b2f18`) still building...
2018-05-03 20:35:01 hwc hwcc[4151] DEBUG Getting information for instance 05ba31b4-74c1-4c2d-af94-a347ec9d7566
2018-05-03 20:35:02 hwc hwcc[4151] DEBUG node `worker001` (instance id `05ba31b4-74c1-4c2d-af94-a347ec9d7566`) still building...
2018-05-03 20:35:02 hwc hwcc[4151] DEBUG Getting information for instance abe6c209-94d4-4c74-9f16-395c454deef6
2018-05-03 20:35:03 hwc hwcc[4151] DEBUG node `master001` (instance id `abe6c209-94d4-4c74-9f16-395c454deef6`) still building...
2018-05-03 20:35:03 hwc hwcc[4151] DEBUG Waiting for 3 more nodes to come up ...
2018-05-03 20:35:13 hwc hwcc[4151] DEBUG Getting information for instance a2314a31-6f30-461f-8667-3411b82b2f18
2018-05-03 20:35:13 hwc hwcc[4151] DEBUG node `worker002` (instance id `a2314a31-6f30-461f-8667-3411b82b2f18`) still building...
2018-05-03 20:35:13 hwc hwcc[4151] DEBUG Getting information for instance 05ba31b4-74c1-4c2d-af94-a347ec9d7566
2018-05-03 20:35:13 hwc hwcc[4151] DEBUG node `worker001` (instance id `05ba31b4-74c1-4c2d-af94-a347ec9d7566`) still building...
2018-05-03 20:35:13 hwc hwcc[4151] DEBUG Getting information for instance abe6c209-94d4-4c74-9f16-395c454deef6
2018-05-03 20:35:14 hwc hwcc[4151] DEBUG node `master001` (instance id `abe6c209-94d4-4c74-9f16-395c454deef6`) still building...
2018-05-03 20:35:14 hwc hwcc[4151] DEBUG Waiting for 3 more nodes to come up ...
2018-05-03 20:35:24 hwc hwcc[4151] DEBUG Getting information for instance a2314a31-6f30-461f-8667-3411b82b2f18
2018-05-03 20:35:24 hwc hwcc[4151] DEBUG node `worker002` (instance id `a2314a31-6f30-461f-8667-3411b82b2f18`) still building...
2018-05-03 20:35:24 hwc hwcc[4151] DEBUG Getting information for instance 05ba31b4-74c1-4c2d-af94-a347ec9d7566
2018-05-03 20:35:24 hwc hwcc[4151] DEBUG node `worker001` (instance id `05ba31b4-74c1-4c2d-af94-a347ec9d7566`) still building...
2018-05-03 20:35:24 hwc hwcc[4151] DEBUG Getting information for instance abe6c209-94d4-4c74-9f16-395c454deef6
2018-05-03 20:35:25 hwc hwcc[4151] DEBUG node `master001` (instance id `abe6c209-94d4-4c74-9f16-395c454deef6`) still building...
2018-05-03 20:35:25 hwc hwcc[4151] DEBUG Waiting for 3 more nodes to come up ...
2018-05-03 20:35:35 hwc hwcc[4151] DEBUG Getting information for instance a2314a31-6f30-461f-8667-3411b82b2f18
2018-05-03 20:35:35 hwc hwcc[4151] DEBUG node `worker002` (instance id `a2314a31-6f30-461f-8667-3411b82b2f18`) still building...
2018-05-03 20:35:35 hwc hwcc[4151] DEBUG Getting information for instance 05ba31b4-74c1-4c2d-af94-a347ec9d7566
2018-05-03 20:35:35 hwc hwcc[4151] DEBUG node `worker001` (instance id `05ba31b4-74c1-4c2d-af94-a347ec9d7566`) still building...
2018-05-03 20:35:35 hwc hwcc[4151] DEBUG Getting information for instance abe6c209-94d4-4c74-9f16-395c454deef6
2018-05-03 20:35:36 hwc hwcc[4151] DEBUG node `master001` (instance id `abe6c209-94d4-4c74-9f16-395c454deef6`) still building...
2018-05-03 20:35:36 hwc hwcc[4151] DEBUG Waiting for 3 more nodes to come up ...
2018-05-03 20:35:46 hwc hwcc[4151] DEBUG Getting information for instance a2314a31-6f30-461f-8667-3411b82b2f18
2018-05-03 20:35:46 hwc hwcc[4151] DEBUG node `worker002` (instance id `a2314a31-6f30-461f-8667-3411b82b2f18`) still building...
2018-05-03 20:35:46 hwc hwcc[4151] DEBUG Getting information for instance 05ba31b4-74c1-4c2d-af94-a347ec9d7566
2018-05-03 20:35:47 hwc hwcc[4151] DEBUG node `worker001` (instance id `05ba31b4-74c1-4c2d-af94-a347ec9d7566`) still building...
2018-05-03 20:35:47 hwc hwcc[4151] DEBUG Getting information for instance abe6c209-94d4-4c74-9f16-395c454deef6
2018-05-03 20:35:47 hwc hwcc[4151] DEBUG node `master001` (instance id `abe6c209-94d4-4c74-9f16-395c454deef6`) still building...
2018-05-03 20:35:47 hwc hwcc[4151] DEBUG Waiting for 3 more nodes to come up ...
2018-05-03 20:35:57 hwc hwcc[4151] DEBUG Getting information for instance a2314a31-6f30-461f-8667-3411b82b2f18
2018-05-03 20:35:58 hwc hwcc[4151] DEBUG node `worker002` (instance id `a2314a31-6f30-461f-8667-3411b82b2f18`) still building...
2018-05-03 20:35:58 hwc hwcc[4151] DEBUG Getting information for instance 05ba31b4-74c1-4c2d-af94-a347ec9d7566
2018-05-03 20:35:59 hwc hwcc[4151] DEBUG node `worker001` (instance id `05ba31b4-74c1-4c2d-af94-a347ec9d7566`) still building...
2018-05-03 20:35:59 hwc hwcc[4151] DEBUG Getting information for instance abe6c209-94d4-4c74-9f16-395c454deef6
2018-05-03 20:35:59 hwc hwcc[4151] DEBUG node `master001` (instance id `abe6c209-94d4-4c74-9f16-395c454deef6`) still building...
2018-05-03 20:35:59 hwc hwcc[4151] DEBUG Waiting for 3 more nodes to come up ...
2018-05-03 20:36:09 hwc hwcc[4151] DEBUG Getting information for instance a2314a31-6f30-461f-8667-3411b82b2f18
2018-05-03 20:36:10 hwc hwcc[4151] DEBUG node `worker002` (instance id `a2314a31-6f30-461f-8667-3411b82b2f18`) still building...
2018-05-03 20:36:10 hwc hwcc[4151] DEBUG Getting information for instance 05ba31b4-74c1-4c2d-af94-a347ec9d7566
2018-05-03 20:36:10 hwc hwcc[4151] DEBUG node `worker001` (instance id `05ba31b4-74c1-4c2d-af94-a347ec9d7566`) still building...
2018-05-03 20:36:10 hwc hwcc[4151] DEBUG Getting information for instance abe6c209-94d4-4c74-9f16-395c454deef6
2018-05-03 20:36:10 hwc hwcc[4151] DEBUG node `master001` (instance id `abe6c209-94d4-4c74-9f16-395c454deef6`) still building...
2018-05-03 20:36:10 hwc hwcc[4151] DEBUG Waiting for 3 more nodes to come up ...
2018-05-03 20:36:20 hwc hwcc[4151] DEBUG Getting information for instance a2314a31-6f30-461f-8667-3411b82b2f18
2018-05-03 20:36:21 hwc hwcc[4151] DEBUG node `worker002` (instance id `a2314a31-6f30-461f-8667-3411b82b2f18`) still building...
2018-05-03 20:36:21 hwc hwcc[4151] DEBUG Getting information for instance 05ba31b4-74c1-4c2d-af94-a347ec9d7566
2018-05-03 20:36:22 hwc hwcc[4151] DEBUG node `worker001` (instance id `05ba31b4-74c1-4c2d-af94-a347ec9d7566`) still building...
2018-05-03 20:36:22 hwc hwcc[4151] DEBUG Getting information for instance abe6c209-94d4-4c74-9f16-395c454deef6
2018-05-03 20:36:23 hwc hwcc[4151] DEBUG node `master001` (instance id `abe6c209-94d4-4c74-9f16-395c454deef6`) still building...
2018-05-03 20:36:23 hwc hwcc[4151] DEBUG Waiting for 3 more nodes to come up ...
2018-05-03 20:36:33 hwc hwcc[4151] DEBUG Getting information for instance a2314a31-6f30-461f-8667-3411b82b2f18
2018-05-03 20:36:33 hwc hwcc[4151] DEBUG node `worker002` (instance id `a2314a31-6f30-461f-8667-3411b82b2f18`) still building...
2018-05-03 20:36:33 hwc hwcc[4151] DEBUG Getting information for instance 05ba31b4-74c1-4c2d-af94-a347ec9d7566
2018-05-03 20:36:33 hwc hwcc[4151] DEBUG node `worker001` (instance id `05ba31b4-74c1-4c2d-af94-a347ec9d7566`) still building...
2018-05-03 20:36:33 hwc hwcc[4151] DEBUG Getting information for instance abe6c209-94d4-4c74-9f16-395c454deef6
2018-05-03 20:36:34 hwc hwcc[4151] DEBUG node `master001` (instance id `abe6c209-94d4-4c74-9f16-395c454deef6`) still building...
2018-05-03 20:36:34 hwc hwcc[4151] DEBUG Waiting for 3 more nodes to come up ...
2018-05-03 20:36:44 hwc hwcc[4151] DEBUG Getting information for instance a2314a31-6f30-461f-8667-3411b82b2f18
2018-05-03 20:36:44 hwc hwcc[4151] DEBUG node `worker002` (instance id `a2314a31-6f30-461f-8667-3411b82b2f18`) still building...
2018-05-03 20:36:44 hwc hwcc[4151] DEBUG Getting information for instance 05ba31b4-74c1-4c2d-af94-a347ec9d7566
2018-05-03 20:36:44 hwc hwcc[4151] DEBUG node `worker001` (instance id `05ba31b4-74c1-4c2d-af94-a347ec9d7566`) still building...
2018-05-03 20:36:44 hwc hwcc[4151] DEBUG Getting information for instance abe6c209-94d4-4c74-9f16-395c454deef6
2018-05-03 20:36:45 hwc hwcc[4151] DEBUG node `master001` (instance id `abe6c209-94d4-4c74-9f16-395c454deef6`) still building...
2018-05-03 20:36:45 hwc hwcc[4151] DEBUG Waiting for 3 more nodes to come up ...
2018-05-03 20:36:55 hwc hwcc[4151] DEBUG Getting information for instance a2314a31-6f30-461f-8667-3411b82b2f18
2018-05-03 20:36:55 hwc hwcc[4151] DEBUG node `worker002` (instance id `a2314a31-6f30-461f-8667-3411b82b2f18`) still building...
2018-05-03 20:36:55 hwc hwcc[4151] DEBUG Getting information for instance 05ba31b4-74c1-4c2d-af94-a347ec9d7566
2018-05-03 20:36:55 hwc hwcc[4151] DEBUG node `worker001` (instance id `05ba31b4-74c1-4c2d-af94-a347ec9d7566`) still building...
2018-05-03 20:36:55 hwc hwcc[4151] DEBUG Getting information for instance abe6c209-94d4-4c74-9f16-395c454deef6
2018-05-03 20:36:55 hwc hwcc[4151] DEBUG node `master001` (instance id `abe6c209-94d4-4c74-9f16-395c454deef6`) still building...
2018-05-03 20:36:55 hwc hwcc[4151] DEBUG Waiting for 3 more nodes to come up ...
2018-05-03 20:37:05 hwc hwcc[4151] DEBUG Getting information for instance a2314a31-6f30-461f-8667-3411b82b2f18
2018-05-03 20:37:06 hwc hwcc[4151] DEBUG node `worker002` (instance id `a2314a31-6f30-461f-8667-3411b82b2f18`) still building...
2018-05-03 20:37:06 hwc hwcc[4151] DEBUG Getting information for instance 05ba31b4-74c1-4c2d-af94-a347ec9d7566
2018-05-03 20:37:07 hwc hwcc[4151] DEBUG node `worker001` (instance id `05ba31b4-74c1-4c2d-af94-a347ec9d7566`) still building...
2018-05-03 20:37:07 hwc hwcc[4151] DEBUG Getting information for instance abe6c209-94d4-4c74-9f16-395c454deef6
2018-05-03 20:37:07 hwc hwcc[4151] DEBUG node `master001` (instance id `abe6c209-94d4-4c74-9f16-395c454deef6`) still building...
2018-05-03 20:37:07 hwc hwcc[4151] DEBUG Waiting for 3 more nodes to come up ...
2018-05-03 20:37:17 hwc hwcc[4151] DEBUG Getting information for instance a2314a31-6f30-461f-8667-3411b82b2f18
2018-05-03 20:37:17 hwc hwcc[4151] DEBUG node `worker002` (instance id `a2314a31-6f30-461f-8667-3411b82b2f18`) still building...
2018-05-03 20:37:17 hwc hwcc[4151] DEBUG Getting information for instance 05ba31b4-74c1-4c2d-af94-a347ec9d7566
2018-05-03 20:37:18 hwc hwcc[4151] DEBUG node `worker001` (instance id `05ba31b4-74c1-4c2d-af94-a347ec9d7566`) still building...
2018-05-03 20:37:18 hwc hwcc[4151] DEBUG Getting information for instance abe6c209-94d4-4c74-9f16-395c454deef6
2018-05-03 20:37:18 hwc hwcc[4151] DEBUG node `master001` (instance id `abe6c209-94d4-4c74-9f16-395c454deef6`) still building...
2018-05-03 20:37:18 hwc hwcc[4151] DEBUG Waiting for 3 more nodes to come up ...
2018-05-03 20:37:28 hwc hwcc[4151] DEBUG Getting information for instance a2314a31-6f30-461f-8667-3411b82b2f18
2018-05-03 20:37:28 hwc hwcc[4151] DEBUG node `worker002` (instance id `a2314a31-6f30-461f-8667-3411b82b2f18`) still building...
2018-05-03 20:37:28 hwc hwcc[4151] DEBUG Getting information for instance 05ba31b4-74c1-4c2d-af94-a347ec9d7566
2018-05-03 20:37:29 hwc hwcc[4151] DEBUG node `worker001` (instance id `05ba31b4-74c1-4c2d-af94-a347ec9d7566`) still building...
2018-05-03 20:37:29 hwc hwcc[4151] DEBUG Getting information for instance abe6c209-94d4-4c74-9f16-395c454deef6
2018-05-03 20:37:30 hwc hwcc[4151] DEBUG node `master001` (instance id abe6c209-94d4-4c74-9f16-395c454deef6) is up.
2018-05-03 20:37:30 hwc hwcc[4151] DEBUG Waiting for 2 more nodes to come up ...
2018-05-03 20:37:40 hwc hwcc[4151] DEBUG Getting information for instance a2314a31-6f30-461f-8667-3411b82b2f18
2018-05-03 20:37:40 hwc hwcc[4151] DEBUG node `worker002` (instance id `a2314a31-6f30-461f-8667-3411b82b2f18`) still building...
2018-05-03 20:37:40 hwc hwcc[4151] DEBUG Getting information for instance 05ba31b4-74c1-4c2d-af94-a347ec9d7566
2018-05-03 20:37:41 hwc hwcc[4151] DEBUG node `worker001` (instance id `05ba31b4-74c1-4c2d-af94-a347ec9d7566`) still building...
2018-05-03 20:37:41 hwc hwcc[4151] DEBUG Waiting for 2 more nodes to come up ...
2018-05-03 20:37:51 hwc hwcc[4151] DEBUG Getting information for instance a2314a31-6f30-461f-8667-3411b82b2f18
2018-05-03 20:37:51 hwc hwcc[4151] DEBUG node `worker002` (instance id a2314a31-6f30-461f-8667-3411b82b2f18) is up.
2018-05-03 20:37:51 hwc hwcc[4151] DEBUG Getting information for instance 05ba31b4-74c1-4c2d-af94-a347ec9d7566
2018-05-03 20:37:52 hwc hwcc[4151] DEBUG node `worker001` (instance id 05ba31b4-74c1-4c2d-af94-a347ec9d7566) is up.
2018-05-03 20:37:52 hwc hwcc[4151] INFO Checking SSH connection to nodes (timeout: 600 seconds) ...
2018-05-03 20:37:52 hwc hwcc[4151] DEBUG Trying to connect to host worker002 (192.168.0.5) ...
2018-05-03 20:37:57 hwc hwcc[4151] DEBUG Host worker002 (192.168.0.5) not reachable within 5 seconds: timed out -- <class 'socket.timeout'>
2018-05-03 20:37:57 hwc hwcc[4151] DEBUG Trying to connect to host worker001 (192.168.0.89) ...
2018-05-03 20:38:02 hwc hwcc[4151] DEBUG Host worker001 (192.168.0.89) not reachable within 5 seconds: timed out -- <class 'socket.timeout'>
2018-05-03 20:38:02 hwc hwcc[4151] DEBUG Trying to connect to host master001 (192.168.0.209) ...
2018-05-03 20:38:02 hwc hwcc[4151] DEBUG Host master001 (192.168.0.209) not reachable within 5 seconds: [Errno None] Unable to connect to port 22 on 192.168.0.209 -- <class 'paramiko.ssh_exception.NoValidConnectionsError'>
2018-05-03 20:38:12 hwc hwcc[4151] DEBUG Trying to connect to host worker002 (192.168.0.5) ...
2018-05-03 20:38:12 hwc hwcc[4151] DEBUG Host worker002 (192.168.0.5) not reachable within 5 seconds: [Errno None] Unable to connect to port 22 on 192.168.0.5 -- <class 'paramiko.ssh_exception.NoValidConnectionsError'>
2018-05-03 20:38:12 hwc hwcc[4151] DEBUG Trying to connect to host worker001 (192.168.0.89) ...
2018-05-03 20:38:12 hwc hwcc[4151] DEBUG Host worker001 (192.168.0.89) not reachable within 5 seconds: [Errno None] Unable to connect to port 22 on 192.168.0.89 -- <class 'paramiko.ssh_exception.NoValidConnectionsError'>
2018-05-03 20:38:12 hwc hwcc[4151] DEBUG Trying to connect to host master001 (192.168.0.209) ...
2018-05-03 20:38:12 hwc hwcc[4151] INFO Cannot use squid,because squid section is not in config file
2018-05-03 20:38:12 hwc hwcc[4151] DEBUG Connection to 192.168.0.209 succeeded on port 22, will use this IP address for future connections.
2018-05-03 20:38:12 hwc hwcc[4151] INFO Connection to node `master001` successful, using IP address 192.168.0.209 to connect.
2018-05-03 20:38:22 hwc hwcc[4151] DEBUG Trying to connect to host worker002 (192.168.0.5) ...
2018-05-03 20:38:22 hwc hwcc[4151] DEBUG Host worker002 (192.168.0.5) not reachable within 5 seconds: [Errno None] Unable to connect to port 22 on 192.168.0.5 -- <class 'paramiko.ssh_exception.NoValidConnectionsError'>
2018-05-03 20:38:22 hwc hwcc[4151] DEBUG Trying to connect to host worker001 (192.168.0.89) ...
2018-05-03 20:38:22 hwc hwcc[4151] DEBUG Host worker001 (192.168.0.89) not reachable within 5 seconds: [Errno None] Unable to connect to port 22 on 192.168.0.89 -- <class 'paramiko.ssh_exception.NoValidConnectionsError'>
2018-05-03 20:38:32 hwc hwcc[4151] DEBUG Trying to connect to host worker002 (192.168.0.5) ...
2018-05-03 20:38:32 hwc hwcc[4151] DEBUG Host worker002 (192.168.0.5) not reachable within 5 seconds: [Errno None] Unable to connect to port 22 on 192.168.0.5 -- <class 'paramiko.ssh_exception.NoValidConnectionsError'>
2018-05-03 20:38:32 hwc hwcc[4151] DEBUG Trying to connect to host worker001 (192.168.0.89) ...
2018-05-03 20:38:32 hwc hwcc[4151] DEBUG Host worker001 (192.168.0.89) not reachable within 5 seconds: [Errno None] Unable to connect to port 22 on 192.168.0.89 -- <class 'paramiko.ssh_exception.NoValidConnectionsError'>
2018-05-03 20:38:42 hwc hwcc[4151] DEBUG Trying to connect to host worker002 (192.168.0.5) ...
2018-05-03 20:38:43 hwc hwcc[4151] INFO Cannot use squid,because squid section is not in config file
2018-05-03 20:38:43 hwc hwcc[4151] DEBUG Connection to 192.168.0.5 succeeded on port 22, will use this IP address for future connections.
2018-05-03 20:38:43 hwc hwcc[4151] INFO Connection to node `worker002` successful, using IP address 192.168.0.5 to connect.
2018-05-03 20:38:43 hwc hwcc[4151] DEBUG Trying to connect to host worker001 (192.168.0.89) ...
2018-05-03 20:38:43 hwc hwcc[4151] INFO Cannot use squid,because squid section is not in config file
2018-05-03 20:38:43 hwc hwcc[4151] DEBUG Connection to 192.168.0.89 succeeded on port 22, will use this IP address for future connections.
2018-05-03 20:38:43 hwc hwcc[4151] INFO Connection to node `worker001` successful, using IP address 192.168.0.89 to connect.
2018-05-03 20:38:43 hwc hwcc[4151] DEBUG Writing Ansible inventory to file `/root/.hwcc/storage/slurm.inventory` ...
2018-05-03 20:38:43 hwc hwcc[4151] DEBUG Calling `ansible-playbook` with the following environment:
2018-05-03 20:38:43 hwc hwcc[4151] DEBUG - ANSIBLE_ANY_ERRORS_FATAL='yes'
2018-05-03 20:38:43 hwc hwcc[4151] DEBUG - ANSIBLE_FORKS='10'
2018-05-03 20:38:43 hwc hwcc[4151] DEBUG - ANSIBLE_HOST_KEY_CHECKING='no'
2018-05-03 20:38:43 hwc hwcc[4151] DEBUG - ANSIBLE_RETRY_FILES_ENABLED='no'
2018-05-03 20:38:43 hwc hwcc[4151] DEBUG - ANSIBLE_ROLES_PATH='/root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles:/root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks:/etc/ansible/roles'
2018-05-03 20:38:43 hwc hwcc[4151] DEBUG - ANSIBLE_SSH_PIPELINING='yes'
2018-05-03 20:38:43 hwc hwcc[4151] DEBUG - ANSIBLE_TIMEOUT='120'
2018-05-03 20:38:43 hwc hwcc[4151] DEBUG - CINDER_ENDPOINT_TYPE='publicURL'
2018-05-03 20:38:43 hwc hwcc[4151] DEBUG - CLUSTER_HOME='/root/elasticluster'
2018-05-03 20:38:43 hwc hwcc[4151] DEBUG - CVS_RSH='ssh'
2018-05-03 20:38:43 hwc hwcc[4151] DEBUG - G_BROKEN_FILENAMES='1'
2018-05-03 20:38:43 hwc hwcc[4151] DEBUG - HISTCONTROL='ignoredups'
2018-05-03 20:38:43 hwc hwcc[4151] DEBUG - HISTSIZE='1000'
2018-05-03 20:38:43 hwc hwcc[4151] DEBUG - HISTTIMEFORMAT='%F %T root '
2018-05-03 20:38:43 hwc hwcc[4151] DEBUG - HOME='/root'
2018-05-03 20:38:43 hwc hwcc[4151] DEBUG - HOSTNAME='hwc'
2018-05-03 20:38:43 hwc hwcc[4151] DEBUG - LANG='en_US.UTF-8'
2018-05-03 20:38:43 hwc hwcc[4151] DEBUG - LESSOPEN='||/usr/bin/lesspipe.sh %s'
2018-05-03 20:38:43 hwc hwcc[4151] DEBUG - LOGNAME='root'
2018-05-03 20:38:43 hwc hwcc[4151] DEBUG - LS_COLORS='rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=01;05;37;41:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arj=01;31:*.taz=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.dz=01;31:*.gz=01;31:*.lz=01;31:*.xz=01;31:*.bz2=01;31:*.tbz=01;31:*.tbz2=01;31:*.bz=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.rar=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.jpg=01;35:*.jpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.axv=01;35:*.anx=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=01;36:*.au=01;36:*.flac=01;36:*.mid=01;36:*.midi=01;36:*.mka=01;36:*.mp3=01;36:*.mpc=01;36:*.ogg=01;36:*.ra=01;36:*.wav=01;36:*.axa=01;36:*.oga=01;36:*.spx=01;36:*.xspf=01;36:'
2018-05-03 20:38:43 hwc hwcc[4151] DEBUG - MAIL='/var/spool/mail/root'
2018-05-03 20:38:43 hwc hwcc[4151] DEBUG - NOVA_ENDPOINT_TYPE='publicURL'
2018-05-03 20:38:43 hwc hwcc[4151] DEBUG - OS_AUTH_URL='https://iam.cn-north-1.myhwclouds.com/v3'
2018-05-03 20:38:43 hwc hwcc[4151] DEBUG - OS_AVAILABILITY='cn-north-1a'
2018-05-03 20:38:43 hwc hwcc[4151] DEBUG - OS_ENDPOINT_TYPE='publicURL'
2018-05-03 20:38:43 hwc hwcc[4151] DEBUG - OS_IDENTITY_API_VERSION='3'
2018-05-03 20:38:43 hwc hwcc[4151] DEBUG - OS_IMAGE_API_VERSION='2'
2018-05-03 20:38:43 hwc hwcc[4151] DEBUG - OS_PASSWORD='1980813c'
2018-05-03 20:38:43 hwc hwcc[4151] DEBUG - OS_PROJECT_DOMAIN_NAME='mschyj'
2018-05-03 20:38:43 hwc hwcc[4151] DEBUG - OS_PROJECT_NAME='cn-north-1'
2018-05-03 20:38:43 hwc hwcc[4151] DEBUG - OS_TENANT_NAME='cn-north-1'
2018-05-03 20:38:43 hwc hwcc[4151] DEBUG - OS_USERNAME='mschyj'
2018-05-03 20:38:43 hwc hwcc[4151] DEBUG - OS_USER_DOMAIN_NAME='mschyj'
2018-05-03 20:38:43 hwc hwcc[4151] DEBUG - OS_VOLUME_API_VERSION='2'
2018-05-03 20:38:43 hwc hwcc[4151] DEBUG - PATH='/usr/lib64/qt-3.3/bin:/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin:/root/bin:/root/bin'
2018-05-03 20:38:43 hwc hwcc[4151] DEBUG - PWD='/root/hwcc-chenyjie'
2018-05-03 20:38:43 hwc hwcc[4151] DEBUG - QTDIR='/usr/lib64/qt-3.3'
2018-05-03 20:38:43 hwc hwcc[4151] DEBUG - QTINC='/usr/lib64/qt-3.3/include'
2018-05-03 20:38:43 hwc hwcc[4151] DEBUG - QTLIB='/usr/lib64/qt-3.3/lib'
2018-05-03 20:38:43 hwc hwcc[4151] DEBUG - SHELL='/bin/bash'
2018-05-03 20:38:43 hwc hwcc[4151] DEBUG - SHLVL='3'
2018-05-03 20:38:43 hwc hwcc[4151] DEBUG - SSH_CLIENT='58.213.108.56 47353 22'
2018-05-03 20:38:43 hwc hwcc[4151] DEBUG - SSH_CONNECTION='58.213.108.56 12914 192.168.0.230 22'
2018-05-03 20:38:43 hwc hwcc[4151] DEBUG - SSH_TTY='/dev/pts/1'
2018-05-03 20:38:43 hwc hwcc[4151] DEBUG - TERM='screen'
2018-05-03 20:38:43 hwc hwcc[4151] DEBUG - TMUX='/tmp/tmux-0/default,4363,5'
2018-05-03 20:38:43 hwc hwcc[4151] DEBUG - TMUX_PANE='%30'
2018-05-03 20:38:43 hwc hwcc[4151] DEBUG - USER='root'
2018-05-03 20:38:43 hwc hwcc[4151] DEBUG - _='/usr/local/bin/hwcc'
2018-05-03 20:38:43 hwc hwcc[4151] DEBUG Using playbook file /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/site.yml.
2018-05-03 20:38:43 hwc hwcc[4151] DEBUG Running Ansible command `ansible-playbook --private-key=/root/.ssh/id_rsa /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/site.yml --inventory=/root/.hwcc/storage/slurm.inventory --become --become-user=root -vv -e elasticluster_output_dir=/tmp/elasticluster.4QQI2P.d` ...
No config file found; using defaults
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/hosts.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/hostname.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/netgroup.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/ssh_auth.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/ssh_keysign_rhel7.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/cuda/tasks/_check_nvidia_dev.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/cuda/tasks/_reboot_and_wait.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/cuda/tasks/_check_nvidia_dev.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/ceph/tasks/mon.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/ceph/tasks/mgr.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/ceph/tasks/_create_key.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/ceph/tasks/osd.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/ceph/tasks/mds.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/ceph/tasks/fs.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/ceph/tasks/client.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/glusterfs-common/tasks/debian.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/glusterfs-common/tasks/rhel.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/glusterfs-common/tasks/debian.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/glusterfs-common/tasks/rhel.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/ssh_auth.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/ssh_keysign_rhel7.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/handlers/main.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/handlers/main.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypbind.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/cgroup.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypbind.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypbind.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/lmod/tasks/build.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/lmod/tasks/post-install.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypbind.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/lmod/tasks/build.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/lmod/tasks/post-install.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypbind.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/postgresql/tasks/install.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/postgresql/tasks/install_yum.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/postgresql/tasks/extensions.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/postgresql/tasks/extensions/contrib.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/postgresql/tasks/extensions/dev_headers.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/postgresql/tasks/extensions/postgis.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/postgresql/tasks/configure.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/postgresql/tasks/users.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/postgresql/tasks/databases.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/postgresql/tasks/users_privileges.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/postgresql/tasks/monit.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/lmod/tasks/build.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/lmod/tasks/post-install.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/r/tasks/rmpi.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypbind.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/lmod/tasks/build.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/lmod/tasks/post-install.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/r/tasks/rmpi.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/lmod/tasks/build.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/lmod/tasks/post-install.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypbind.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/handlers/main.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/handlers/main.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/ssh_auth.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/ssh_keysign_rhel7.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/jenkins/tasks/main.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/jenkins/handlers/main.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/kubernetes-common/tasks/debian.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/kubernetes-common/tasks/redhat.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/kubernetes-common/tasks/debian.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/kubernetes-common/tasks/redhat.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypbind.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/lmod/tasks/build.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/lmod/tasks/post-install.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/pbs+maui/tasks/master.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/handlers/main.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/pbs+maui/handlers/main.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/pbs+maui/tasks/maui.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/pbs+maui/handlers/main.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypbind.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/lmod/tasks/build.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/lmod/tasks/post-install.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/pbs+maui/tasks/clients.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/pbs+maui/handlers/main.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypbind.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-master/tasks/db.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-master/tasks/install-slurmdbd.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-master/tasks/install-slurmctld.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypbind.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/cgroup.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypbind.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/lmod/tasks/build.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/lmod/tasks/post-install.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/lmod/tasks/build.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/lmod/tasks/post-install.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/r/tasks/rmpi.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/smb-server/tasks/ctdb.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/lmod/tasks/build.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/lmod/tasks/post-install.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/r/tasks/rmpi.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/jupyter/tasks/bash.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/jupyter/tasks/python.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/jupyter/tasks/python.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/jupyter/tasks/pyspark.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/jupyter/tasks/pyspark.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/jupyter/tasks/irkernel.yml
statically included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/jupyter/tasks/matlab.yml

PLAYBOOK: site.yml *************************************************************
44 plays in /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/site.yml

PLAY [Prepare VM for running Ansible] ******************************************

TASK [Ensure Python is installed] **********************************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/site.yml:6
skipping: [worker001] => {"changed": false, "msg": "skipped, since /usr/bin/python exists", "skipped": true}
skipping: [worker002] => {"changed": false, "msg": "skipped, since /usr/bin/python exists", "skipped": true}
skipping: [master001] => {"changed": false, "msg": "skipped, since /usr/bin/python exists", "skipped": true}

PLAY [Apply local customizations (before)] *************************************

TASK [setup] *******************************************************************
ok: [worker002]
ok: [worker001]
ok: [master001]

PLAY [Common setup for all hosts] **********************************************

TASK [setup] *******************************************************************
ok: [master001]
ok: [worker001]
ok: [worker002]

TASK [Ensure apt-daily is *not* running] ***************************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/site.yml:20
skipping: [master001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [common : Provide workaround for YAML syntax error in lines containing colon+space] ***
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/main.yml:3
ok: [master001] => {"ansible_facts": {"__colon__": ":"}, "changed": false}
ok: [worker001] => {"ansible_facts": {"__colon__": ":"}, "changed": false}
ok: [worker002] => {"ansible_facts": {"__colon__": ":"}, "changed": false}

TASK [common : Allow package updates] ******************************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/main.yml:9
skipping: [master001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [common : Disallow package updates] ***************************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/main.yml:14
ok: [master001] => {"ansible_facts": {"pkg_install_state": "present"}, "changed": false}
ok: [worker001] => {"ansible_facts": {"pkg_install_state": "present"}, "changed": false}
ok: [worker002] => {"ansible_facts": {"pkg_install_state": "present"}, "changed": false}

TASK [common : Set /etc/hosts from Ansible hostgroups] *************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/hosts.yml:2
changed: [worker002] => {"changed": true, "checksum": "93e1360c8aca3da9f282ec4ca3ce4a603773fa73", "dest": "/etc/hosts", "gid": 0, "group": "root", "md5sum": "e65c01b3c841a8081843c1869658f334", "mode": "0644", "owner": "root", "size": 417, "src": "/root/.ansible/tmp/ansible-tmp-1525351128.89-117587268964147/source", "state": "file", "uid": 0}
changed: [master001] => {"changed": true, "checksum": "93e1360c8aca3da9f282ec4ca3ce4a603773fa73", "dest": "/etc/hosts", "gid": 0, "group": "root", "md5sum": "e65c01b3c841a8081843c1869658f334", "mode": "0644", "owner": "root", "size": 417, "src": "/root/.ansible/tmp/ansible-tmp-1525351128.86-113377188034361/source", "state": "file", "uid": 0}
changed: [worker001] => {"changed": true, "checksum": "93e1360c8aca3da9f282ec4ca3ce4a603773fa73", "dest": "/etc/hosts", "gid": 0, "group": "root", "md5sum": "e65c01b3c841a8081843c1869658f334", "mode": "0644", "owner": "root", "size": 417, "src": "/root/.ansible/tmp/ansible-tmp-1525351128.87-127901408114711/source", "state": "file", "uid": 0}

TASK [common : Patch `/etc/redhat-release`] ************************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/hostname.yml:8
skipping: [master001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [common : Set host name to Ansible "inventory name"] **********************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/hostname.yml:16
changed: [worker002] => {"ansible_facts": {"ansible_domain": "", "ansible_fqdn": "worker002", "ansible_hostname": "worker002", "ansible_nodename": "worker002"}, "changed": true, "name": "worker002"}
changed: [worker001] => {"ansible_facts": {"ansible_domain": "", "ansible_fqdn": "worker001", "ansible_hostname": "worker001", "ansible_nodename": "worker001"}, "changed": true, "name": "worker001"}
changed: [master001] => {"ansible_facts": {"ansible_domain": "", "ansible_fqdn": "master001", "ansible_hostname": "master001", "ansible_nodename": "master001"}, "changed": true, "name": "master001"}

TASK [common : Undo patch to `/etc/redhat-release`] ****************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/hostname.yml:22
skipping: [master001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [common : Check for cloud-init conf file] *********************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/hostname.yml:30
ok: [master001] => {"changed": false, "stat": {"atime": 1525351086.678, "checksum": "ce75755e6aabf7995730f54951ecb5da27974667", "ctime": 1523322248.64, "dev": 64514, "executable": false, "exists": true, "gid": 0, "gr_name": "root", "inode": 1572909, "isblk": false, "ischr": false, "isdir": false, "isfifo": false, "isgid": false, "islnk": false, "isreg": true, "issock": false, "isuid": false, "md5": "bba22929886688d6488b6b8b85e33d74", "mode": "0664", "mtime": 1523322248.64, "nlink": 1, "path": "/etc/cloud/cloud.cfg", "pw_name": "root", "readable": true, "rgrp": true, "roth": true, "rusr": true, "size": 1627, "uid": 0, "wgrp": true, "woth": false, "writeable": true, "wusr": true, "xgrp": false, "xoth": false, "xusr": false}}
ok: [worker001] => {"changed": false, "stat": {"atime": 1525351115.149, "checksum": "ce75755e6aabf7995730f54951ecb5da27974667", "ctime": 1523322248.64, "dev": 51714, "executable": false, "exists": true, "gid": 0, "gr_name": "root", "inode": 1572909, "isblk": false, "ischr": false, "isdir": false, "isfifo": false, "isgid": false, "islnk": false, "isreg": true, "issock": false, "isuid": false, "md5": "bba22929886688d6488b6b8b85e33d74", "mode": "0664", "mtime": 1523322248.64, "nlink": 1, "path": "/etc/cloud/cloud.cfg", "pw_name": "root", "readable": true, "rgrp": true, "roth": true, "rusr": true, "size": 1627, "uid": 0, "wgrp": true, "woth": false, "writeable": true, "wusr": true, "xgrp": false, "xoth": false, "xusr": false}}
ok: [worker002] => {"changed": false, "stat": {"atime": 1525351113.165, "checksum": "ce75755e6aabf7995730f54951ecb5da27974667", "ctime": 1523322248.64, "dev": 51714, "executable": false, "exists": true, "gid": 0, "gr_name": "root", "inode": 1572909, "isblk": false, "ischr": false, "isdir": false, "isfifo": false, "isgid": false, "islnk": false, "isreg": true, "issock": false, "isuid": false, "md5": "bba22929886688d6488b6b8b85e33d74", "mode": "0664", "mtime": 1523322248.64, "nlink": 1, "path": "/etc/cloud/cloud.cfg", "pw_name": "root", "readable": true, "rgrp": true, "roth": true, "rusr": true, "size": 1627, "uid": 0, "wgrp": true, "woth": false, "writeable": true, "wusr": true, "xgrp": false, "xoth": false, "xusr": false}}

TASK [common : Ensure changes to hostname are not overwritten by cloud-init] ***
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/hostname.yml:41
ok: [worker002] => {"backup": "", "changed": false, "msg": ""}
ok: [worker001] => {"backup": "", "changed": false, "msg": ""}
ok: [master001] => {"backup": "", "changed": false, "msg": ""}

TASK [common : Deploy `/etc/netgroup` file.] ***********************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/netgroup.yml:3
changed: [master001] => {"changed": true, "checksum": "29cc927d1cdac7c015815b4e1b6c53e838c31d16", "dest": "/etc/netgroup", "gid": 0, "group": "root", "md5sum": "efdfdf3b8ffc58f27bdaa773f242c388", "mode": "0444", "owner": "root", "size": 266, "src": "/root/.ansible/tmp/ansible-tmp-1525351131.38-270450266020285/source", "state": "file", "uid": 0}
changed: [worker001] => {"changed": true, "checksum": "29cc927d1cdac7c015815b4e1b6c53e838c31d16", "dest": "/etc/netgroup", "gid": 0, "group": "root", "md5sum": "efdfdf3b8ffc58f27bdaa773f242c388", "mode": "0444", "owner": "root", "size": 266, "src": "/root/.ansible/tmp/ansible-tmp-1525351131.39-216851678336316/source", "state": "file", "uid": 0}
changed: [worker002] => {"changed": true, "checksum": "29cc927d1cdac7c015815b4e1b6c53e838c31d16", "dest": "/etc/netgroup", "gid": 0, "group": "root", "md5sum": "efdfdf3b8ffc58f27bdaa773f242c388", "mode": "0444", "owner": "root", "size": 266, "src": "/root/.ansible/tmp/ansible-tmp-1525351131.4-27861163659822/source", "state": "file", "uid": 0}

TASK [common : Add `files` databases to `netgroup` service (I)] ****************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/netgroup.yml:21
ok: [worker001] => {"changed": false, "msg": ""}
ok: [master001] => {"changed": false, "msg": ""}
ok: [worker002] => {"changed": false, "msg": ""}

TASK [common : Add `files` databases to `netgroup` service (II)] ***************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/netgroup.yml:32
skipping: [master001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [common : Load distribution-dependent values (Debian/Ubuntu)] *************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/ssh_auth.yml:6
skipping: [master001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [common : Load distribution-dependent values (CentOS/RHEL)] ***************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/ssh_auth.yml:12
ok: [master001] => {"ansible_facts": {"ssh_keysign_path": "/usr/libexec/openssh/ssh-keysign"}, "changed": false}
ok: [worker001] => {"ansible_facts": {"ssh_keysign_path": "/usr/libexec/openssh/ssh-keysign"}, "changed": false}
ok: [worker002] => {"ansible_facts": {"ssh_keysign_path": "/usr/libexec/openssh/ssh-keysign"}, "changed": false}

TASK [common : Setup SSH known hosts file] *************************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/ssh_auth.yml:18
changed: [worker001] => {"changed": true, "checksum": "c07d71ede119f0035935b19f874b4846c4f06290", "dest": "/etc/ssh/ssh_known_hosts", "gid": 0, "group": "root", "md5sum": "361a004974e3054682c2c0b6fd8b5e5d", "mode": "0644", "owner": "root", "size": 3048, "src": "/root/.ansible/tmp/ansible-tmp-1525351133.35-185562358171250/source", "state": "file", "uid": 0}
changed: [worker002] => {"changed": true, "checksum": "c07d71ede119f0035935b19f874b4846c4f06290", "dest": "/etc/ssh/ssh_known_hosts", "gid": 0, "group": "root", "md5sum": "361a004974e3054682c2c0b6fd8b5e5d", "mode": "0644", "owner": "root", "size": 3048, "src": "/root/.ansible/tmp/ansible-tmp-1525351133.37-201962340242267/source", "state": "file", "uid": 0}
changed: [master001] => {"changed": true, "checksum": "c07d71ede119f0035935b19f874b4846c4f06290", "dest": "/etc/ssh/ssh_known_hosts", "gid": 0, "group": "root", "md5sum": "361a004974e3054682c2c0b6fd8b5e5d", "mode": "0644", "owner": "root", "size": 3048, "src": "/root/.ansible/tmp/ansible-tmp-1525351133.34-204899871221982/source", "state": "file", "uid": 0}

TASK [common : Setup /etc/ssh/shosts.equiv file] *******************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/ssh_auth.yml:27
changed: [worker001] => {"changed": true, "checksum": "3fcab4260414f40070d262b15a0bf5c618ca3aac", "dest": "/etc/ssh/shosts.equiv", "gid": 0, "group": "root", "md5sum": "b6ac379c6b565277451e04363334f2ae", "mode": "0644", "owner": "root", "size": 69, "src": "/root/.ansible/tmp/ansible-tmp-1525351134.52-258944242827670/source", "state": "file", "uid": 0}
changed: [worker002] => {"changed": true, "checksum": "3fcab4260414f40070d262b15a0bf5c618ca3aac", "dest": "/etc/ssh/shosts.equiv", "gid": 0, "group": "root", "md5sum": "b6ac379c6b565277451e04363334f2ae", "mode": "0644", "owner": "root", "size": 69, "src": "/root/.ansible/tmp/ansible-tmp-1525351134.54-102002025028302/source", "state": "file", "uid": 0}
changed: [master001] => {"changed": true, "checksum": "3fcab4260414f40070d262b15a0bf5c618ca3aac", "dest": "/etc/ssh/shosts.equiv", "gid": 0, "group": "root", "md5sum": "b6ac379c6b565277451e04363334f2ae", "mode": "0644", "owner": "root", "size": 69, "src": "/root/.ansible/tmp/ansible-tmp-1525351134.52-124855904048066/source", "state": "file", "uid": 0}

TASK [common : Setup /root/.shosts file] ***************************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/ssh_auth.yml:35
changed: [worker001] => {"changed": true, "checksum": "3fcab4260414f40070d262b15a0bf5c618ca3aac", "dest": "/root/.shosts", "gid": 0, "group": "root", "md5sum": "b6ac379c6b565277451e04363334f2ae", "mode": "0644", "owner": "root", "size": 69, "src": "/root/.ansible/tmp/ansible-tmp-1525351135.71-182511061098654/source", "state": "file", "uid": 0}
changed: [master001] => {"changed": true, "checksum": "3fcab4260414f40070d262b15a0bf5c618ca3aac", "dest": "/root/.shosts", "gid": 0, "group": "root", "md5sum": "b6ac379c6b565277451e04363334f2ae", "mode": "0644", "owner": "root", "size": 69, "src": "/root/.ansible/tmp/ansible-tmp-1525351135.7-90613208244537/source", "state": "file", "uid": 0}
changed: [worker002] => {"changed": true, "checksum": "3fcab4260414f40070d262b15a0bf5c618ca3aac", "dest": "/root/.shosts", "gid": 0, "group": "root", "md5sum": "b6ac379c6b565277451e04363334f2ae", "mode": "0644", "owner": "root", "size": 69, "src": "/root/.ansible/tmp/ansible-tmp-1525351135.73-20449520829025/source", "state": "file", "uid": 0}

TASK [common : Setup SSH authentication (server configuration file)] ***********
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/ssh_auth.yml:43
ok: [master001] => (item={u'value': u'yes', u'key': u'HostbasedAuthentication'}) => {"backup": "", "changed": false, "item": {"key": "HostbasedAuthentication", "value": "yes"}, "msg": ""}
ok: [worker001] => (item={u'value': u'yes', u'key': u'HostbasedAuthentication'}) => {"backup": "", "changed": false, "item": {"key": "HostbasedAuthentication", "value": "yes"}, "msg": ""}
ok: [worker002] => (item={u'value': u'yes', u'key': u'HostbasedAuthentication'}) => {"backup": "", "changed": false, "item": {"key": "HostbasedAuthentication", "value": "yes"}, "msg": ""}
ok: [master001] => (item={u'value': u'no', u'key': u'IgnoreRhosts'}) => {"backup": "", "changed": false, "item": {"key": "IgnoreRhosts", "value": "no"}, "msg": ""}
ok: [worker001] => (item={u'value': u'no', u'key': u'IgnoreRhosts'}) => {"backup": "", "changed": false, "item": {"key": "IgnoreRhosts", "value": "no"}, "msg": ""}
ok: [worker002] => (item={u'value': u'no', u'key': u'IgnoreRhosts'}) => {"backup": "", "changed": false, "item": {"key": "IgnoreRhosts", "value": "no"}, "msg": ""}
ok: [master001] => (item={u'value': u'yes', u'key': u'PasswordAuthentication'}) => {"backup": "", "changed": false, "item": {"key": "PasswordAuthentication", "value": "yes"}, "msg": ""}
ok: [worker001] => (item={u'value': u'yes', u'key': u'PasswordAuthentication'}) => {"backup": "", "changed": false, "item": {"key": "PasswordAuthentication", "value": "yes"}, "msg": ""}
ok: [worker002] => (item={u'value': u'yes', u'key': u'PasswordAuthentication'}) => {"backup": "", "changed": false, "item": {"key": "PasswordAuthentication", "value": "yes"}, "msg": ""}

TASK [common : Setup SSH authentication (client configuration file)] ***********
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/ssh_auth.yml:55
ok: [master001] => (item={u'value': u'yes', u'key': u'HostbasedAuthentication'}) => {"backup": "", "changed": false, "item": {"key": "HostbasedAuthentication", "value": "yes"}, "msg": ""}
ok: [worker001] => (item={u'value': u'yes', u'key': u'HostbasedAuthentication'}) => {"backup": "", "changed": false, "item": {"key": "HostbasedAuthentication", "value": "yes"}, "msg": ""}
ok: [worker002] => (item={u'value': u'yes', u'key': u'HostbasedAuthentication'}) => {"backup": "", "changed": false, "item": {"key": "HostbasedAuthentication", "value": "yes"}, "msg": ""}
ok: [worker001] => (item={u'value': u'yes', u'key': u'EnableSSHKeysign'}) => {"backup": "", "changed": false, "item": {"key": "EnableSSHKeysign", "value": "yes"}, "msg": ""}
ok: [worker002] => (item={u'value': u'yes', u'key': u'EnableSSHKeysign'}) => {"backup": "", "changed": false, "item": {"key": "EnableSSHKeysign", "value": "yes"}, "msg": ""}
ok: [master001] => (item={u'value': u'yes', u'key': u'EnableSSHKeysign'}) => {"backup": "", "changed": false, "item": {"key": "EnableSSHKeysign", "value": "yes"}, "msg": ""}

TASK [common : Load info about the `ssh-keysign` executable] *******************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/ssh_keysign_rhel7.yml:9
skipping: [master001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [common : List all SSH host keys] *****************************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/ssh_keysign_rhel7.yml:15
skipping: [master001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [common : Ensure SSH host keys can be read by `ssh-keysign`] **************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/ssh_keysign_rhel7.yml:23
skipping: [master001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [iptables : Load distribution-specific data] ******************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/iptables/tasks/main.yml:18
included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/iptables/tasks/init-RedHat.yml for master001, worker001, worker002

TASK [iptables : Load configuration and service names (RHEL-compatible)] *******
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/iptables/tasks/init-RedHat.yml:3
ok: [master001] => {"ansible_facts": {"configfile": {"etc/iptables/rules.v4": "/etc/sysconfig/iptables", "etc/iptables/rules.v6": "/etc/sysconfig/ip6tables"}, "reload": "reload iptables", "service": {"ip6tables": "ip6tables", "iptables": "iptables"}}, "changed": false}
ok: [worker001] => {"ansible_facts": {"configfile": {"etc/iptables/rules.v4": "/etc/sysconfig/iptables", "etc/iptables/rules.v6": "/etc/sysconfig/ip6tables"}, "reload": "reload iptables", "service": {"ip6tables": "ip6tables", "iptables": "iptables"}}, "changed": false}
ok: [worker002] => {"ansible_facts": {"configfile": {"etc/iptables/rules.v4": "/etc/sysconfig/iptables", "etc/iptables/rules.v6": "/etc/sysconfig/ip6tables"}, "reload": "reload iptables", "service": {"ip6tables": "ip6tables", "iptables": "iptables"}}, "changed": false}

TASK [iptables : Load configuration and service names (RHEL6-compatible)] ******
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/iptables/tasks/init-RedHat.yml:17
ok: [master001] => {"ansible_facts": {"packages": ["iptables", "iptables-ipv6"]}, "changed": false}
ok: [worker001] => {"ansible_facts": {"packages": ["iptables", "iptables-ipv6"]}, "changed": false}
ok: [worker002] => {"ansible_facts": {"packages": ["iptables", "iptables-ipv6"]}, "changed": false}

TASK [iptables : Load configuration and service names (RHEL7-compatible)] ******
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/iptables/tasks/init-RedHat.yml:25
skipping: [master001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [iptables : Install iptables packages] ************************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/iptables/tasks/main.yml:25
ok: [worker001] => (item=[u'iptables', u'iptables-ipv6']) => {"changed": false, "item": ["iptables", "iptables-ipv6"], "msg": "", "rc": 0, "results": ["iptables-1.4.7-16.el6.x86_64 providing iptables is already installed", "iptables-ipv6-1.4.7-16.el6.x86_64 providing iptables-ipv6 is already installed"]}
ok: [worker002] => (item=[u'iptables', u'iptables-ipv6']) => {"changed": false, "item": ["iptables", "iptables-ipv6"], "msg": "", "rc": 0, "results": ["iptables-1.4.7-16.el6.x86_64 providing iptables is already installed", "iptables-ipv6-1.4.7-16.el6.x86_64 providing iptables-ipv6 is already installed"]}
ok: [master001] => (item=[u'iptables', u'iptables-ipv6']) => {"changed": false, "item": ["iptables", "iptables-ipv6"], "msg": "", "rc": 0, "results": ["iptables-1.4.7-16.el6.x86_64 providing iptables is already installed", "iptables-ipv6-1.4.7-16.el6.x86_64 providing iptables-ipv6 is already installed"]}

TASK [iptables : Deploy netfilter rules] ***************************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/iptables/tasks/main.yml:33
changed: [worker001] => (item=etc/iptables/rules.v4) => {"changed": true, "checksum": "aac9d522db2ae1070500bedf50417e398cb3df95", "dest": "/etc/sysconfig/iptables", "gid": 0, "group": "root", "item": "etc/iptables/rules.v4", "md5sum": "dac4e796ac950719b14f2c9c97043cb6", "mode": "0444", "owner": "root", "size": 1113, "src": "/root/.ansible/tmp/ansible-tmp-1525351140.04-249666319889921/source", "state": "file", "uid": 0}
changed: [master001] => (item=etc/iptables/rules.v4) => {"changed": true, "checksum": "aac9d522db2ae1070500bedf50417e398cb3df95", "dest": "/etc/sysconfig/iptables", "gid": 0, "group": "root", "item": "etc/iptables/rules.v4", "md5sum": "dac4e796ac950719b14f2c9c97043cb6", "mode": "0444", "owner": "root", "size": 1113, "src": "/root/.ansible/tmp/ansible-tmp-1525351140.04-86439026336582/source", "state": "file", "uid": 0}
changed: [worker002] => (item=etc/iptables/rules.v4) => {"changed": true, "checksum": "aac9d522db2ae1070500bedf50417e398cb3df95", "dest": "/etc/sysconfig/iptables", "gid": 0, "group": "root", "item": "etc/iptables/rules.v4", "md5sum": "dac4e796ac950719b14f2c9c97043cb6", "mode": "0444", "owner": "root", "size": 1113, "src": "/root/.ansible/tmp/ansible-tmp-1525351140.07-198726200010792/source", "state": "file", "uid": 0}
NOTIFIED HANDLER reload iptables
NOTIFIED HANDLER reload iptables
ok: [master001] => (item=etc/iptables/rules.v6) => {"changed": false, "gid": 0, "group": "root", "item": "etc/iptables/rules.v6", "mode": "0444", "owner": "root", "path": "/etc/sysconfig/ip6tables", "size": 1180, "state": "file", "uid": 0}
NOTIFIED HANDLER reload iptables
ok: [worker002] => (item=etc/iptables/rules.v6) => {"changed": false, "gid": 0, "group": "root", "item": "etc/iptables/rules.v6", "mode": "0444", "owner": "root", "path": "/etc/sysconfig/ip6tables", "size": 1180, "state": "file", "uid": 0}
ok: [worker001] => (item=etc/iptables/rules.v6) => {"changed": false, "gid": 0, "group": "root", "item": "etc/iptables/rules.v6", "mode": "0444", "owner": "root", "path": "/etc/sysconfig/ip6tables", "size": 1180, "state": "file", "uid": 0}

TASK [iptables : Ensure netfilter rules are loaded at boot] ********************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/iptables/tasks/main.yml:50
ok: [worker002] => (item=iptables) => {"changed": false, "enabled": true, "item": "iptables", "name": "iptables", "state": "started"}
ok: [worker001] => (item=iptables) => {"changed": false, "enabled": true, "item": "iptables", "name": "iptables", "state": "started"}
ok: [master001] => (item=iptables) => {"changed": false, "enabled": true, "item": "iptables", "name": "iptables", "state": "started"}
ok: [worker002] => (item=ip6tables) => {"changed": false, "enabled": true, "item": "ip6tables", "name": "ip6tables", "state": "started"}
ok: [worker001] => (item=ip6tables) => {"changed": false, "enabled": true, "item": "ip6tables", "name": "ip6tables", "state": "started"}
ok: [master001] => (item=ip6tables) => {"changed": false, "enabled": true, "item": "ip6tables", "name": "ip6tables", "state": "started"}

RUNNING HANDLER [iptables : reload iptables] ***********************************
changed: [master001] => (item=iptables) => {"changed": true, "item": "iptables", "name": "iptables", "state": "started"}
changed: [worker001] => (item=iptables) => {"changed": true, "item": "iptables", "name": "iptables", "state": "started"}
changed: [worker002] => (item=iptables) => {"changed": true, "item": "iptables", "name": "iptables", "state": "started"}
changed: [worker001] => (item=ip6tables) => {"changed": true, "item": "ip6tables", "name": "ip6tables", "state": "started"}
changed: [master001] => (item=ip6tables) => {"changed": true, "item": "ip6tables", "name": "ip6tables", "state": "started"}
changed: [worker002] => (item=ip6tables) => {"changed": true, "item": "ip6tables", "name": "ip6tables", "state": "started"}

TASK [ntpd : Load distribution-specific parameters] ****************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/ntpd/tasks/main.yml:3
included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/ntpd/tasks/init-RedHat.yml for master001, worker001, worker002

TASK [ntpd : Set NTPd common playbook params (RHEL/CentOS)] ********************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/ntpd/tasks/init-RedHat.yml:3
ok: [master001] => {"ansible_facts": {"ntpd_package": "ntp", "ntpd_service": "ntpd"}, "changed": false}
ok: [worker001] => {"ansible_facts": {"ntpd_package": "ntp", "ntpd_service": "ntpd"}, "changed": false}
ok: [worker002] => {"ansible_facts": {"ntpd_package": "ntp", "ntpd_service": "ntpd"}, "changed": false}

TASK [ntpd : Deploy NTP configuration file] ************************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/ntpd/tasks/main.yml:7
NOTIFIED HANDLER restart ntpd
changed: [master001] => {"changed": true, "checksum": "810f4e75590ced84918279c1a6eedc6db730a850", "dest": "/etc/ntp.conf", "gid": 0, "group": "root", "md5sum": "3f0fcad0852e765bd65a7d660371a23e", "mode": "0444", "owner": "root", "size": 1977, "src": "/root/.ansible/tmp/ansible-tmp-1525351143.16-227828563342806/source", "state": "file", "uid": 0}
NOTIFIED HANDLER restart ntpd
changed: [worker002] => {"changed": true, "checksum": "16110bfbe6e55e05c1f1399b23a4f62a5a24ef03", "dest": "/etc/ntp.conf", "gid": 0, "group": "root", "md5sum": "b70516f2d863afce219120c2c75090a9", "mode": "0444", "owner": "root", "size": 1977, "src": "/root/.ansible/tmp/ansible-tmp-1525351143.19-242898602011184/source", "state": "file", "uid": 0}
NOTIFIED HANDLER restart ntpd
changed: [worker001] => {"changed": true, "checksum": "cfc3960489cdd48070a44d108d7ef929acfb9273", "dest": "/etc/ntp.conf", "gid": 0, "group": "root", "md5sum": "39d1bfe95ab653f4ce084794408efb54", "mode": "0444", "owner": "root", "size": 1977, "src": "/root/.ansible/tmp/ansible-tmp-1525351143.17-264840573009879/source", "state": "file", "uid": 0}

TASK [ntpd : Install NTP packages] *********************************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/ntpd/tasks/main.yml:19
ok: [worker001] => {"changed": false, "msg": "", "rc": 0, "results": ["ntp-4.2.6p5-12.el6.centos.2.x86_64 providing ntp is already installed"]}
ok: [master001] => {"changed": false, "msg": "", "rc": 0, "results": ["ntp-4.2.6p5-12.el6.centos.2.x86_64 providing ntp is already installed"]}
ok: [worker002] => {"changed": false, "msg": "", "rc": 0, "results": ["ntp-4.2.6p5-12.el6.centos.2.x86_64 providing ntp is already installed"]}

TASK [ntpd : Enable NTP service at boot] ***************************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/ntpd/tasks/main.yml:42
ok: [worker001] => {"changed": false, "enabled": true, "name": "ntpd", "state": "started"}
ok: [master001] => {"changed": false, "enabled": true, "name": "ntpd", "state": "started"}
ok: [worker002] => {"changed": false, "enabled": true, "name": "ntpd", "state": "started"}

TASK [pdsh : Install pdsh packages (Debian-family)] ****************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/pdsh/tasks/main.yml:3
skipping: [master001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [pdsh : Install pdsh packages (RHEL-family)] ******************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/pdsh/tasks/main.yml:11
ok: [worker001] => (item=[u'pdsh', u'pdsh-rcmd-ssh', u'pdsh-mod-genders']) => {"changed": false, "item": ["pdsh", "pdsh-rcmd-ssh", "pdsh-mod-genders"], "msg": "", "rc": 0, "results": ["pdsh-2.26-4.el6.x86_64 providing pdsh is already installed", "pdsh-rcmd-ssh-2.26-4.el6.x86_64 providing pdsh-rcmd-ssh is already installed", "pdsh-mod-genders-2.26-4.el6.x86_64 providing pdsh-mod-genders is already installed"]}
ok: [master001] => (item=[u'pdsh', u'pdsh-rcmd-ssh', u'pdsh-mod-genders']) => {"changed": false, "item": ["pdsh", "pdsh-rcmd-ssh", "pdsh-mod-genders"], "msg": "", "rc": 0, "results": ["pdsh-2.26-4.el6.x86_64 providing pdsh is already installed", "pdsh-rcmd-ssh-2.26-4.el6.x86_64 providing pdsh-rcmd-ssh is already installed", "pdsh-mod-genders-2.26-4.el6.x86_64 providing pdsh-mod-genders is already installed"]}
ok: [worker002] => (item=[u'pdsh', u'pdsh-rcmd-ssh', u'pdsh-mod-genders']) => {"changed": false, "item": ["pdsh", "pdsh-rcmd-ssh", "pdsh-mod-genders"], "msg": "", "rc": 0, "results": ["pdsh-2.26-4.el6.x86_64 providing pdsh is already installed", "pdsh-rcmd-ssh-2.26-4.el6.x86_64 providing pdsh-rcmd-ssh is already installed", "pdsh-mod-genders-2.26-4.el6.x86_64 providing pdsh-mod-genders is already installed"]}

TASK [pdsh : Create genders file for PDSH] *************************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/pdsh/tasks/main.yml:21
changed: [master001] => {"changed": true, "checksum": "2084e052920c00e25730fff7ce4585deffaa67e0", "dest": "/etc/genders", "gid": 0, "group": "root", "md5sum": "220cafd3055bbfb7c8e3fb0e6e8195b7", "mode": "0444", "owner": "root", "size": 202, "src": "/root/.ansible/tmp/ansible-tmp-1525351146.08-27967551100973/source", "state": "file", "uid": 0}
changed: [worker002] => {"changed": true, "checksum": "2084e052920c00e25730fff7ce4585deffaa67e0", "dest": "/etc/genders", "gid": 0, "group": "root", "md5sum": "220cafd3055bbfb7c8e3fb0e6e8195b7", "mode": "0444", "owner": "root", "size": 202, "src": "/root/.ansible/tmp/ansible-tmp-1525351146.1-245758697815588/source", "state": "file", "uid": 0}
changed: [worker001] => {"changed": true, "checksum": "2084e052920c00e25730fff7ce4585deffaa67e0", "dest": "/etc/genders", "gid": 0, "group": "root", "md5sum": "220cafd3055bbfb7c8e3fb0e6e8195b7", "mode": "0444", "owner": "root", "size": 202, "src": "/root/.ansible/tmp/ansible-tmp-1525351146.09-205104192167480/source", "state": "file", "uid": 0}

TASK [pdsh : Make SSH the default exec method for PDSH] ************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/pdsh/tasks/main.yml:29
ok: [worker002] => {"changed": false, "checksum": "5929e07c9e9a681ab81ae9da0e1a4c17630509ab", "dest": "/etc/profile.d/pdsh.sh", "gid": 0, "group": "root", "mode": "0444", "owner": "root", "path": "/etc/profile.d/pdsh.sh", "size": 278, "state": "file", "uid": 0}
ok: [worker001] => {"changed": false, "checksum": "5929e07c9e9a681ab81ae9da0e1a4c17630509ab", "dest": "/etc/profile.d/pdsh.sh", "gid": 0, "group": "root", "mode": "0444", "owner": "root", "path": "/etc/profile.d/pdsh.sh", "size": 278, "state": "file", "uid": 0}
ok: [master001] => {"changed": false, "checksum": "5929e07c9e9a681ab81ae9da0e1a4c17630509ab", "dest": "/etc/profile.d/pdsh.sh", "gid": 0, "group": "root", "mode": "0444", "owner": "root", "path": "/etc/profile.d/pdsh.sh", "size": 278, "state": "file", "uid": 0}

RUNNING HANDLER [ntpd : restart ntpd] ******************************************
changed: [master001] => {"changed": true, "name": "ntpd", "state": "started"}
changed: [worker002] => {"changed": true, "name": "ntpd", "state": "started"}
changed: [worker001] => {"changed": true, "name": "ntpd", "state": "started"}

PLAY [Slurm worker nodes Playbook] *********************************************

TASK [setup] *******************************************************************
ok: [worker001]
ok: [master001]
ok: [worker002]

TASK [nis : Load distribution-specific parameters] *****************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/main.yml:3
included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/init-RedHat.yml for master001, worker001, worker002

TASK [nis : Set NIS common playbook params (CentOS/RHEL)] **********************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/init-RedHat.yml:3
ok: [worker001] => {"ansible_facts": {"nis_client_packages": ["ypbind"], "nis_client_services": ["rpcbind", "ypbind"], "nis_common_packages": ["yp-tools"], "nis_master_packages": ["ypserv"], "nis_master_services": ["rpcbind", "yppasswdd", "ypserv"], "nis_securenets_path": "/var/yp/securenets"}, "changed": false}
ok: [master001] => {"ansible_facts": {"nis_client_packages": ["ypbind"], "nis_client_services": ["rpcbind", "ypbind"], "nis_common_packages": ["yp-tools"], "nis_master_packages": ["ypserv"], "nis_master_services": ["rpcbind", "yppasswdd", "ypserv"], "nis_securenets_path": "/var/yp/securenets"}, "changed": false}
ok: [worker002] => {"ansible_facts": {"nis_client_packages": ["ypbind"], "nis_client_services": ["rpcbind", "ypbind"], "nis_common_packages": ["yp-tools"], "nis_master_packages": ["ypserv"], "nis_master_services": ["rpcbind", "yppasswdd", "ypserv"], "nis_securenets_path": "/var/yp/securenets"}, "changed": false}

TASK [nis : Pre-load debconf answer to questions (Debian/Ubuntu)] **************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/main.yml:7
skipping: [master001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [nis : Deploy additional NIS configuration (Debian/Ubuntu)] ***************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/main.yml:18
skipping: [master001] => (item=etc/default/nis)  => {"changed": false, "item": "etc/default/nis", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => (item=etc/default/nis)  => {"changed": false, "item": "etc/default/nis", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [master001] => (item=etc/defaultdomain)  => {"changed": false, "item": "etc/defaultdomain", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => (item=etc/defaultdomain)  => {"changed": false, "item": "etc/defaultdomain", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item=etc/default/nis)  => {"changed": false, "item": "etc/default/nis", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item=etc/defaultdomain)  => {"changed": false, "item": "etc/defaultdomain", "skip_reason": "Conditional check failed", "skipped": true}

TASK [nis : Set NIS domain (CentOS/RHEL)] **************************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/main.yml:29
changed: [master001] => {"backup": "", "changed": true, "msg": "line added"}
changed: [worker001] => {"backup": "", "changed": true, "msg": "line added"}
changed: [worker002] => {"backup": "", "changed": true, "msg": "line added"}

TASK [nis : Install NIS common packages] ***************************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/main.yml:39
ok: [master001] => (item=[u'yp-tools']) => {"changed": false, "item": ["yp-tools"], "msg": "", "rc": 0, "results": ["yp-tools-2.9-12.el6.x86_64 providing yp-tools is already installed"]}
ok: [worker001] => (item=[u'yp-tools']) => {"changed": false, "item": ["yp-tools"], "msg": "", "rc": 0, "results": ["yp-tools-2.9-12.el6.x86_64 providing yp-tools is already installed"]}
ok: [worker002] => (item=[u'yp-tools']) => {"changed": false, "item": ["yp-tools"], "msg": "", "rc": 0, "results": ["yp-tools-2.9-12.el6.x86_64 providing yp-tools is already installed"]}

TASK [nis : Deploy `ypserv` configuration files] *******************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml:3
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
NOTIFIED HANDLER restart NIS master services
changed: [master001] => {"changed": true, "checksum": "9a628298c92f060094426fa843df21f7701d808c", "dest": "/var/yp/securenets", "gid": 0, "group": "root", "md5sum": "476cf0c0c48240ad950b416bdcde3e17", "mode": "0400", "owner": "root", "size": 706, "src": "/root/.ansible/tmp/ansible-tmp-1525351150.18-42347120414173/source", "state": "file", "uid": 0}

TASK [nis : Deploy `yppasswdd` configuration file (CentOS/RHEL)] ***************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml:12
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
ok: [master001] => {"changed": false, "checksum": "18392b4e3ecc4f9e7832c274663b5f6cee00d3f1", "dest": "/etc/sysconfig/yppasswdd", "gid": 0, "group": "root", "mode": "0444", "owner": "root", "path": "/etc/sysconfig/yppasswdd", "size": 681, "state": "file", "uid": 0}

TASK [nis : Install NIS master server packages] ********************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml:22
skipping: [worker001] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}
ok: [master001] => (item=[u'ypserv']) => {"changed": false, "item": ["ypserv"], "msg": "", "rc": 0, "results": ["ypserv-2.19-31.el6.x86_64 providing ypserv is already installed"]}

RUNNING HANDLER [nis : restart NIS master services] ****************************
changed: [master001] => (item=rpcbind) => {"changed": true, "item": "rpcbind", "name": "rpcbind", "state": "started"}
changed: [master001] => (item=yppasswdd) => {"changed": true, "item": "yppasswdd", "name": "yppasswdd", "state": "started"}
changed: [master001] => (item=ypserv) => {"changed": true, "item": "ypserv", "name": "ypserv", "state": "started"}

TASK [nis : Ensure `ypserv` starts at boot] ************************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml:33
skipping: [worker001] => (item=rpcbind)  => {"changed": false, "item": "rpcbind", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item=rpcbind)  => {"changed": false, "item": "rpcbind", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => (item=yppasswdd)  => {"changed": false, "item": "yppasswdd", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item=yppasswdd)  => {"changed": false, "item": "yppasswdd", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => (item=ypserv)  => {"changed": false, "item": "ypserv", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item=ypserv)  => {"changed": false, "item": "ypserv", "skip_reason": "Conditional check failed", "skipped": true}
ok: [master001] => (item=rpcbind) => {"changed": false, "enabled": true, "item": "rpcbind", "name": "rpcbind", "state": "started"}
ok: [master001] => (item=yppasswdd) => {"changed": false, "enabled": true, "item": "yppasswdd", "name": "yppasswdd", "state": "started"}
ok: [master001] => (item=ypserv) => {"changed": false, "enabled": true, "item": "ypserv", "name": "ypserv", "state": "started"}

TASK [nis : Update NIS/YP databases (NIS master server)] ***********************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml:41
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
changed: [master001] => {"changed": true, "cmd": ["make"], "delta": "0:00:00.180918", "end": "2018-05-03 20:39:14.822159", "rc": 0, "start": "2018-05-03 20:39:14.641241", "stderr": "", "stdout": "gmake[1]: Entering directory `/var/yp/elasticluster'\nUpdating passwd.byname...\nUpdating passwd.byuid...\nUpdating group.byname...\nUpdating group.bygid...\nUpdating hosts.byname...\nUpdating hosts.byaddr...\nUpdating netid.byname...\ngmake[1]: Leaving directory `/var/yp/elasticluster'", "stdout_lines": ["gmake[1]: Entering directory `/var/yp/elasticluster'", "Updating passwd.byname...", "Updating passwd.byuid...", "Updating group.byname...", "Updating group.bygid...", "Updating hosts.byname...", "Updating hosts.byaddr...", "Updating netid.byname...", "gmake[1]: Leaving directory `/var/yp/elasticluster'"], "warnings": []}

TASK [nis : Deploy `ypbind` configuration files] *******************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypbind.yml:3
skipping: [master001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
NOTIFIED HANDLER restart ypbind
changed: [worker002] => {"changed": true, "checksum": "1d96dd5b0ffb492e179507891ef4701a7afe84b6", "dest": "/etc/yp.conf", "gid": 0, "group": "root", "md5sum": "daa2803ee93589645ef31b0eef25ee39", "mode": "0400", "owner": "root", "size": 723, "src": "/root/.ansible/tmp/ansible-tmp-1525351154.27-95065169769545/source", "state": "file", "uid": 0}
NOTIFIED HANDLER restart ypbind
changed: [worker001] => {"changed": true, "checksum": "1d96dd5b0ffb492e179507891ef4701a7afe84b6", "dest": "/etc/yp.conf", "gid": 0, "group": "root", "md5sum": "daa2803ee93589645ef31b0eef25ee39", "mode": "0400", "owner": "root", "size": 723, "src": "/root/.ansible/tmp/ansible-tmp-1525351154.26-217897967828882/source", "state": "file", "uid": 0}

TASK [nis : Ensure NIS/YP is used as a name service] ***************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypbind.yml:12
skipping: [master001] => (item=passwd)  => {"changed": false, "item": "passwd", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [master001] => (item=group)  => {"changed": false, "item": "group", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [master001] => (item=shadow)  => {"changed": false, "item": "shadow", "skip_reason": "Conditional check failed", "skipped": true}
changed: [worker001] => (item=passwd) => {"backup": "", "changed": true, "item": "passwd", "msg": "line replaced"}
changed: [worker002] => (item=passwd) => {"backup": "", "changed": true, "item": "passwd", "msg": "line replaced"}
changed: [worker001] => (item=group) => {"backup": "", "changed": true, "item": "group", "msg": "line replaced"}
changed: [worker002] => (item=group) => {"backup": "", "changed": true, "item": "group", "msg": "line replaced"}
changed: [worker001] => (item=shadow) => {"backup": "", "changed": true, "item": "shadow", "msg": "line replaced"}
changed: [worker002] => (item=shadow) => {"backup": "", "changed": true, "item": "shadow", "msg": "line replaced"}

TASK [nis : Replace `compat` in `/etc/nsswitch.conf` with `files nis`] *********
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypbind.yml:26
skipping: [master001] => (item=passwd)  => {"changed": false, "item": "passwd", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [master001] => (item=group)  => {"changed": false, "item": "group", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [master001] => (item=shadow)  => {"changed": false, "item": "shadow", "skip_reason": "Conditional check failed", "skipped": true}
ok: [worker001] => (item=passwd) => {"changed": false, "item": "passwd", "msg": ""}
ok: [worker002] => (item=passwd) => {"changed": false, "item": "passwd", "msg": ""}
ok: [worker001] => (item=group) => {"changed": false, "item": "group", "msg": ""}
ok: [worker002] => (item=group) => {"changed": false, "item": "group", "msg": ""}
ok: [worker002] => (item=shadow) => {"changed": false, "item": "shadow", "msg": ""}
ok: [worker001] => (item=shadow) => {"changed": false, "item": "shadow", "msg": ""}

TASK [nis : Ensure `ypbind` starts at boot] ************************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypbind.yml:37
skipping: [master001] => (item=rpcbind)  => {"changed": false, "item": "rpcbind", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [master001] => (item=ypbind)  => {"changed": false, "item": "ypbind", "skip_reason": "Conditional check failed", "skipped": true}
ok: [worker001] => (item=rpcbind) => {"changed": false, "enabled": true, "item": "rpcbind", "name": "rpcbind", "state": "started"}
ok: [worker002] => (item=rpcbind) => {"changed": false, "enabled": true, "item": "rpcbind", "name": "rpcbind", "state": "started"}
changed: [worker001] => (item=ypbind) => {"changed": true, "enabled": true, "item": "ypbind", "name": "ypbind", "state": "started"}
changed: [worker002] => (item=ypbind) => {"changed": true, "enabled": true, "item": "ypbind", "name": "ypbind", "state": "started"}

RUNNING HANDLER [nis : restart ypbind] *****************************************
changed: [worker002] => (item=rpcbind) => {"changed": true, "item": "rpcbind", "name": "rpcbind", "state": "started"}
changed: [worker001] => (item=rpcbind) => {"changed": true, "item": "rpcbind", "name": "rpcbind", "state": "started"}
changed: [worker002] => (item=ypbind) => {"changed": true, "item": "ypbind", "name": "ypbind", "state": "started"}
changed: [worker001] => (item=ypbind) => {"changed": true, "item": "ypbind", "name": "ypbind", "state": "started"}

TASK [nfs-client : install NFS client software (Debian/Ubuntu)] ****************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-client/tasks/main.yml:3
skipping: [master001] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}

TASK [nfs-client : install NFS client software (RHEL-compatible)] **************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-client/tasks/main.yml:15
ok: [master001] => (item=[u'nfs-utils', u'nfs4-acl-tools']) => {"changed": false, "item": ["nfs-utils", "nfs4-acl-tools"], "msg": "", "rc": 0, "results": ["nfs-utils-1:1.2.3-75.el6_9.x86_64 providing nfs-utils is already installed", "nfs4-acl-tools-0.3.3-8.el6.x86_64 providing nfs4-acl-tools is already installed"]}
ok: [worker002] => (item=[u'nfs-utils', u'nfs4-acl-tools']) => {"changed": false, "item": ["nfs-utils", "nfs4-acl-tools"], "msg": "", "rc": 0, "results": ["nfs-utils-1:1.2.3-75.el6_9.x86_64 providing nfs-utils is already installed", "nfs4-acl-tools-0.3.3-8.el6.x86_64 providing nfs4-acl-tools is already installed"]}
ok: [worker001] => (item=[u'nfs-utils', u'nfs4-acl-tools']) => {"changed": false, "item": ["nfs-utils", "nfs4-acl-tools"], "msg": "", "rc": 0, "results": ["nfs-utils-1:1.2.3-75.el6_9.x86_64 providing nfs-utils is already installed", "nfs4-acl-tools-0.3.3-8.el6.x86_64 providing nfs4-acl-tools is already installed"]}

TASK [nfs-client : Ensure `rpcbind` is running (Debian)] ***********************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-client/tasks/main.yml:28
skipping: [master001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [nfs-client : Ensure `rpcbind` is running (RHEL-compatible)] **************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-client/tasks/main.yml:35
ok: [master001] => {"changed": false, "enabled": true, "name": "rpcbind", "state": "started"}
ok: [worker001] => {"changed": false, "enabled": true, "name": "rpcbind", "state": "started"}
ok: [worker002] => {"changed": false, "enabled": true, "name": "rpcbind", "state": "started"}

TASK [nfs-client : Ensure `portmap` is running (Ubuntu prior to 14.04)] ********
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-client/tasks/main.yml:42
skipping: [master001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [nfs-client : Ensure `rpcbind` is running (Ubuntu 14.04 or newer)] ********
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-client/tasks/main.yml:49
skipping: [master001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [nfs-client : Mount NFS filesystems] **************************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-client/tasks/main.yml:57
included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-client/tasks/nfsmount.yml for master001, worker001, worker002

TASK [nfs-client : ensure /sfs directory exists] *******************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-client/tasks/nfsmount.yml:3
changed: [master001] => {"changed": true, "gid": 0, "group": "root", "mode": "0755", "owner": "root", "path": "/sfs", "size": 4096, "state": "directory", "uid": 0}
changed: [worker001] => {"changed": true, "gid": 0, "group": "root", "mode": "0755", "owner": "root", "path": "/sfs", "size": 4096, "state": "directory", "uid": 0}
changed: [worker002] => {"changed": true, "gid": 0, "group": "root", "mode": "0755", "owner": "root", "path": "/sfs", "size": 4096, "state": "directory", "uid": 0}

TASK [nfs-client : add to /etc/fstab] ******************************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-client/tasks/nfsmount.yml:8
changed: [worker002] => {"changed": true, "dump": "0", "fstab": "/etc/fstab", "fstype": "nfs", "name": "/sfs", "opts": "rw,async", "passno": "0", "src": "sfs-nas1.cn-north-1.myhuaweicloud.com:/share-480fca1b"}
changed: [master001] => {"changed": true, "dump": "0", "fstab": "/etc/fstab", "fstype": "nfs", "name": "/sfs", "opts": "rw,async", "passno": "0", "src": "sfs-nas1.cn-north-1.myhuaweicloud.com:/share-480fca1b"}
changed: [worker001] => {"changed": true, "dump": "0", "fstab": "/etc/fstab", "fstype": "nfs", "name": "/sfs", "opts": "rw,async", "passno": "0", "src": "sfs-nas1.cn-north-1.myhuaweicloud.com:/share-480fca1b"}

TASK [autofs : Load distribution-specific parameters] **************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/autofs/tasks/main.yml:8
included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/autofs/tasks/init-RedHat.yml for master001, worker001, worker002

TASK [autofs : Provide RHEL-specific values] ***********************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/autofs/tasks/init-RedHat.yml:6
ok: [master001] => {"ansible_facts": {"autofs_packages": ["autofs"], "autofs_service": "autofs", "autofs_username": "root"}, "changed": false}
ok: [worker001] => {"ansible_facts": {"autofs_packages": ["autofs"], "autofs_service": "autofs", "autofs_username": "root"}, "changed": false}
ok: [worker002] => {"ansible_facts": {"autofs_packages": ["autofs"], "autofs_service": "autofs", "autofs_username": "root"}, "changed": false}

TASK [autofs : Deploy autofs configuration] ************************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/autofs/tasks/main.yml:12
changed: [worker001] => (item=etc/auto.master) => {"changed": true, "checksum": "758d13b8a76f743e400c0badc6157e8327f1a8f1", "dest": "/etc/auto.master", "gid": 0, "group": "root", "item": "etc/auto.master", "md5sum": "e3d05b4f4a066db7039fed20a9574a94", "mode": "0444", "owner": "root", "size": 540, "src": "/root/.ansible/tmp/ansible-tmp-1525351165.69-203072120700977/source", "state": "file", "uid": 0}
changed: [master001] => (item=etc/auto.master) => {"changed": true, "checksum": "758d13b8a76f743e400c0badc6157e8327f1a8f1", "dest": "/etc/auto.master", "gid": 0, "group": "root", "item": "etc/auto.master", "md5sum": "e3d05b4f4a066db7039fed20a9574a94", "mode": "0444", "owner": "root", "size": 540, "src": "/root/.ansible/tmp/ansible-tmp-1525351165.66-151025887898705/source", "state": "file", "uid": 0}
changed: [worker002] => (item=etc/auto.master) => {"changed": true, "checksum": "758d13b8a76f743e400c0badc6157e8327f1a8f1", "dest": "/etc/auto.master", "gid": 0, "group": "root", "item": "etc/auto.master", "md5sum": "e3d05b4f4a066db7039fed20a9574a94", "mode": "0444", "owner": "root", "size": 540, "src": "/root/.ansible/tmp/ansible-tmp-1525351165.71-230211463751380/source", "state": "file", "uid": 0}
changed: [worker001] => (item=etc/auto.home) => {"changed": true, "checksum": "df9645aa3e6a41b1aece0b5867b79d0c9c16f430", "dest": "/etc/auto.home", "gid": 0, "group": "root", "item": "etc/auto.home", "md5sum": "271d38d13da93386ff2c00c836c5608b", "mode": "0444", "owner": "root", "size": 852, "src": "/root/.ansible/tmp/ansible-tmp-1525351166.29-76279537927892/source", "state": "file", "uid": 0}
changed: [master001] => (item=etc/auto.home) => {"changed": true, "checksum": "df9645aa3e6a41b1aece0b5867b79d0c9c16f430", "dest": "/etc/auto.home", "gid": 0, "group": "root", "item": "etc/auto.home", "md5sum": "271d38d13da93386ff2c00c836c5608b", "mode": "0444", "owner": "root", "size": 852, "src": "/root/.ansible/tmp/ansible-tmp-1525351166.29-54796975240528/source", "state": "file", "uid": 0}
changed: [worker002] => (item=etc/auto.home) => {"changed": true, "checksum": "df9645aa3e6a41b1aece0b5867b79d0c9c16f430", "dest": "/etc/auto.home", "gid": 0, "group": "root", "item": "etc/auto.home", "md5sum": "271d38d13da93386ff2c00c836c5608b", "mode": "0444", "owner": "root", "size": 852, "src": "/root/.ansible/tmp/ansible-tmp-1525351166.3-157785669474352/source", "state": "file", "uid": 0}

TASK [autofs : Deploy autofs mount script for NFSv4 (Debian/Ubuntu)] ***********
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/autofs/tasks/main.yml:24
skipping: [master001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [autofs : Install Autofs] *************************************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/autofs/tasks/main.yml:34
ok: [master001] => (item=[u'autofs']) => {"changed": false, "item": ["autofs"], "msg": "", "rc": 0, "results": ["autofs-1:5.0.5-133.el6_9.x86_64 providing autofs is already installed"]}
ok: [worker001] => (item=[u'autofs']) => {"changed": false, "item": ["autofs"], "msg": "", "rc": 0, "results": ["autofs-1:5.0.5-133.el6_9.x86_64 providing autofs is already installed"]}
ok: [worker002] => (item=[u'autofs']) => {"changed": false, "item": ["autofs"], "msg": "", "rc": 0, "results": ["autofs-1:5.0.5-133.el6_9.x86_64 providing autofs is already installed"]}

TASK [autofs : Ensure autofs is running and starts at boot] ********************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/autofs/tasks/main.yml:41
ok: [master001] => {"changed": false, "enabled": true, "name": "autofs", "state": "started"}
ok: [worker001] => {"changed": false, "enabled": true, "name": "autofs", "state": "started"}
ok: [worker002] => {"changed": false, "enabled": true, "name": "autofs", "state": "started"}

TASK [slurm-common : Load distribution-specific parameters] ********************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:3
included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/init-RedHat.yml for master001, worker001, worker002

TASK [slurm-common : Set SLURM common playbook params (RHEL 7.x compatible)] ***
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/init-RedHat.yml:3
ok: [master001] => {"ansible_facts": {"slurm_pid_dir": "/var/run"}, "changed": false}
ok: [worker001] => {"ansible_facts": {"slurm_pid_dir": "/var/run"}, "changed": false}
ok: [worker002] => {"ansible_facts": {"slurm_pid_dir": "/var/run"}, "changed": false}

TASK [slurm-common : Create `slurm` system group] ******************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:16
ok: [worker001] => {"changed": false, "gid": 498, "name": "slurm", "state": "present", "system": true}
ok: [master001] => {"changed": false, "gid": 498, "name": "slurm", "state": "present", "system": true}
ok: [worker002] => {"changed": false, "gid": 498, "name": "slurm", "state": "present", "system": true}

TASK [slurm-common : Create `slurm` system user] *******************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:24
ok: [worker002] => {"append": false, "changed": false, "comment": "", "group": 498, "home": "/home/slurm", "move_home": false, "name": "slurm", "shell": "/bin/bash", "state": "present", "uid": 498}
ok: [master001] => {"append": false, "changed": false, "comment": "", "group": 498, "home": "/home/slurm", "move_home": false, "name": "slurm", "shell": "/bin/bash", "state": "present", "uid": 498}
ok: [worker001] => {"append": false, "changed": false, "comment": "", "group": 498, "home": "/home/slurm", "move_home": false, "name": "slurm", "shell": "/bin/bash", "state": "present", "uid": 498}

TASK [slurm-common : Create work directory {{item}}] ***************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:33
ok: [worker001] => (item=/etc/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/etc/slurm", "mode": "0755", "owner": "slurm", "path": "/etc/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [master001] => (item=/etc/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/etc/slurm", "mode": "0755", "owner": "slurm", "path": "/etc/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [worker002] => (item=/etc/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/etc/slurm", "mode": "0755", "owner": "slurm", "path": "/etc/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [master001] => (item=/var/lib/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/lib/slurm", "mode": "0755", "owner": "slurm", "path": "/var/lib/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [worker001] => (item=/var/lib/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/lib/slurm", "mode": "0755", "owner": "slurm", "path": "/var/lib/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [master001] => (item=/var/lib/slurm/checkpoint) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/lib/slurm/checkpoint", "mode": "0755", "owner": "slurm", "path": "/var/lib/slurm/checkpoint", "size": 4096, "state": "directory", "uid": 498}
ok: [worker002] => (item=/var/lib/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/lib/slurm", "mode": "0755", "owner": "slurm", "path": "/var/lib/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [worker001] => (item=/var/lib/slurm/checkpoint) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/lib/slurm/checkpoint", "mode": "0755", "owner": "slurm", "path": "/var/lib/slurm/checkpoint", "size": 4096, "state": "directory", "uid": 498}
ok: [worker001] => (item=/var/lib/slurm/slurmctld) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/lib/slurm/slurmctld", "mode": "0755", "owner": "slurm", "path": "/var/lib/slurm/slurmctld", "size": 4096, "state": "directory", "uid": 498}
ok: [worker002] => (item=/var/lib/slurm/checkpoint) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/lib/slurm/checkpoint", "mode": "0755", "owner": "slurm", "path": "/var/lib/slurm/checkpoint", "size": 4096, "state": "directory", "uid": 498}
ok: [master001] => (item=/var/lib/slurm/slurmctld) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/lib/slurm/slurmctld", "mode": "0755", "owner": "slurm", "path": "/var/lib/slurm/slurmctld", "size": 4096, "state": "directory", "uid": 498}
ok: [worker001] => (item=/var/lib/slurm/slurmd) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/lib/slurm/slurmd", "mode": "0755", "owner": "slurm", "path": "/var/lib/slurm/slurmd", "size": 4096, "state": "directory", "uid": 498}
ok: [master001] => (item=/var/lib/slurm/slurmd) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/lib/slurm/slurmd", "mode": "0755", "owner": "slurm", "path": "/var/lib/slurm/slurmd", "size": 4096, "state": "directory", "uid": 498}
ok: [worker002] => (item=/var/lib/slurm/slurmctld) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/lib/slurm/slurmctld", "mode": "0755", "owner": "slurm", "path": "/var/lib/slurm/slurmctld", "size": 4096, "state": "directory", "uid": 498}
ok: [master001] => (item=/var/log/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/log/slurm", "mode": "0755", "owner": "slurm", "path": "/var/log/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [worker001] => (item=/var/log/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/log/slurm", "mode": "0755", "owner": "slurm", "path": "/var/log/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [worker002] => (item=/var/lib/slurm/slurmd) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/lib/slurm/slurmd", "mode": "0755", "owner": "slurm", "path": "/var/lib/slurm/slurmd", "size": 4096, "state": "directory", "uid": 498}
ok: [worker001] => (item=/var/run/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/run/slurm", "mode": "0755", "owner": "slurm", "path": "/var/run/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [master001] => (item=/var/run/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/run/slurm", "mode": "0755", "owner": "slurm", "path": "/var/run/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [worker002] => (item=/var/log/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/log/slurm", "mode": "0755", "owner": "slurm", "path": "/var/log/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [master001] => (item=/var/spool/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/spool/slurm", "mode": "0755", "owner": "slurm", "path": "/var/spool/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [worker001] => (item=/var/spool/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/spool/slurm", "mode": "0755", "owner": "slurm", "path": "/var/spool/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [worker002] => (item=/var/run/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/run/slurm", "mode": "0755", "owner": "slurm", "path": "/var/run/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [worker002] => (item=/var/spool/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/spool/slurm", "mode": "0755", "owner": "slurm", "path": "/var/spool/slurm", "size": 4096, "state": "directory", "uid": 498}

TASK [slurm-common : Make compatibility symlinks] ******************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:56
skipping: [master001] => (item={u'to': u'/etc/slurm-llnl', u'from': u'/etc/slurm'})  => {"changed": false, "item": {"from": "/etc/slurm", "to": "/etc/slurm-llnl"}, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [master001] => (item={u'to': u'/var/lib/slurm-llnl', u'from': u'/var/lib/slurm'})  => {"changed": false, "item": {"from": "/var/lib/slurm", "to": "/var/lib/slurm-llnl"}, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => (item={u'to': u'/etc/slurm-llnl', u'from': u'/etc/slurm'})  => {"changed": false, "item": {"from": "/etc/slurm", "to": "/etc/slurm-llnl"}, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [master001] => (item={u'to': u'/var/log/slurm-llnl', u'from': u'/var/log/slurm'})  => {"changed": false, "item": {"from": "/var/log/slurm", "to": "/var/log/slurm-llnl"}, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => (item={u'to': u'/var/lib/slurm-llnl', u'from': u'/var/lib/slurm'})  => {"changed": false, "item": {"from": "/var/lib/slurm", "to": "/var/lib/slurm-llnl"}, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [master001] => (item={u'to': u'/var/run/slurm-llnl', u'from': u'/var/run/slurm'})  => {"changed": false, "item": {"from": "/var/run/slurm", "to": "/var/run/slurm-llnl"}, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item={u'to': u'/etc/slurm-llnl', u'from': u'/etc/slurm'})  => {"changed": false, "item": {"from": "/etc/slurm", "to": "/etc/slurm-llnl"}, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => (item={u'to': u'/var/log/slurm-llnl', u'from': u'/var/log/slurm'})  => {"changed": false, "item": {"from": "/var/log/slurm", "to": "/var/log/slurm-llnl"}, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item={u'to': u'/var/lib/slurm-llnl', u'from': u'/var/lib/slurm'})  => {"changed": false, "item": {"from": "/var/lib/slurm", "to": "/var/lib/slurm-llnl"}, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => (item={u'to': u'/var/run/slurm-llnl', u'from': u'/var/run/slurm'})  => {"changed": false, "item": {"from": "/var/run/slurm", "to": "/var/run/slurm-llnl"}, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item={u'to': u'/var/log/slurm-llnl', u'from': u'/var/log/slurm'})  => {"changed": false, "item": {"from": "/var/log/slurm", "to": "/var/log/slurm-llnl"}, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item={u'to': u'/var/run/slurm-llnl', u'from': u'/var/run/slurm'})  => {"changed": false, "item": {"from": "/var/run/slurm", "to": "/var/run/slurm-llnl"}, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-common : Enable Copr SLURM repo by verdurin] ***********************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:73
changed: [master001] => {"changed": true, "checksum": "96b9b26fadb6d3649c83fb2f2939c13cc736351f", "dest": "/etc/yum.repos.d/copr-slurm.repo", "gid": 0, "group": "root", "md5sum": "d9bbf2880aa10b1c9dbe68c7f42c3394", "mode": "0444", "owner": "root", "size": 615, "src": "/root/.ansible/tmp/ansible-tmp-1525351170.63-36890031241853/source", "state": "file", "uid": 0}
changed: [worker001] => {"changed": true, "checksum": "96b9b26fadb6d3649c83fb2f2939c13cc736351f", "dest": "/etc/yum.repos.d/copr-slurm.repo", "gid": 0, "group": "root", "md5sum": "d9bbf2880aa10b1c9dbe68c7f42c3394", "mode": "0444", "owner": "root", "size": 615, "src": "/root/.ansible/tmp/ansible-tmp-1525351170.64-86367927807029/source", "state": "file", "uid": 0}
changed: [worker002] => {"changed": true, "checksum": "96b9b26fadb6d3649c83fb2f2939c13cc736351f", "dest": "/etc/yum.repos.d/copr-slurm.repo", "gid": 0, "group": "root", "md5sum": "d9bbf2880aa10b1c9dbe68c7f42c3394", "mode": "0444", "owner": "root", "size": 615, "src": "/root/.ansible/tmp/ansible-tmp-1525351170.66-271707689302463/source", "state": "file", "uid": 0}

TASK [slurm-common : Prevent MUNGE from starting] ******************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:20
skipping: [master001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-common : Install MUNGE (Debian/Ubuntu)] ****************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:31
skipping: [master001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-common : Install MUNGE (RHEL-compatible)] **************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:43
changed: [worker001] => {"changed": true, "msg": "https://copr-be.cloud.fedoraproject.org/results/verdurin/slurm-17.02/epel-6-x86_64/repodata/repomd.xml: [Errno 12] Timeout on https://copr-be.cloud.fedoraproject.org/results/verdurin/slurm-17.02/epel-6-x86_64/repodata/repomd.xml: (28, 'connect() timed out!')\nTrying other mirror.\nWarning: RPMDB altered outside of yum.\n", "rc": 0, "results": ["Loaded plugins: fastestmirror, security\nSetting up Install Process\nLoading mirror speeds from cached hostfile\nResolving Dependencies\n--> Running transaction check\n---> Package slurm-munge.x86_64 0:17.02.7-1.el6 will be installed\n--> Processing Dependency: slurm for package: slurm-munge-17.02.7-1.el6.x86_64\n--> Running transaction check\n---> Package slurm.x86_64 0:17.02.7-1.el6 will be installed\n--> Processing Dependency: slurm-plugins for package: slurm-17.02.7-1.el6.x86_64\n--> Running transaction check\n---> Package slurm-plugins.x86_64 0:17.02.7-1.el6 will be installed\n--> Finished Dependency Resolution\n\nDependencies Resolved\n\n================================================================================\n Package             Arch         Version               Repository         Size\n================================================================================\nInstalling:\n slurm-munge         x86_64       17.02.7-1.el6         local-slurm        16 k\nInstalling for dependencies:\n slurm               x86_64       17.02.7-1.el6         local-slurm        27 M\n slurm-plugins       x86_64       17.02.7-1.el6         local-slurm       1.1 M\n\nTransaction Summary\n================================================================================\nInstall       3 Package(s)\n\nTotal download size: 28 M\nInstalled size: 98 M\nDownloading Packages:\n--------------------------------------------------------------------------------\nTotal                                            73 MB/s |  28 MB     00:00     \nRunning rpm_check_debug\nRunning Transaction Test\nTransaction Test Succeeded\nRunning Transaction\n\r  Installing : slurm-plugins-17.02.7-1.el6.x86_64                           1/3 \n\r  Installing : slurm-17.02.7-1.el6.x86_64                                   2/3 \n\r  Installing : slurm-munge-17.02.7-1.el6.x86_64                             3/3 \n\r  Verifying  : slurm-plugins-17.02.7-1.el6.x86_64                           1/3 \n\r  Verifying  : slurm-17.02.7-1.el6.x86_64                                   2/3 \n\r  Verifying  : slurm-munge-17.02.7-1.el6.x86_64                             3/3 \n\nInstalled:\n  slurm-munge.x86_64 0:17.02.7-1.el6                                            \n\nDependency Installed:\n  slurm.x86_64 0:17.02.7-1.el6       slurm-plugins.x86_64 0:17.02.7-1.el6      \n\nComplete!\n"]}
changed: [master001] => {"changed": true, "msg": "https://copr-be.cloud.fedoraproject.org/results/verdurin/slurm-17.02/epel-6-x86_64/repodata/repomd.xml: [Errno 12] Timeout on https://copr-be.cloud.fedoraproject.org/results/verdurin/slurm-17.02/epel-6-x86_64/repodata/repomd.xml: (28, 'connect() timed out!')\nTrying other mirror.\nWarning: RPMDB altered outside of yum.\n", "rc": 0, "results": ["Loaded plugins: fastestmirror, security\nSetting up Install Process\nLoading mirror speeds from cached hostfile\nResolving Dependencies\n--> Running transaction check\n---> Package slurm-munge.x86_64 0:17.02.7-1.el6 will be installed\n--> Processing Dependency: slurm for package: slurm-munge-17.02.7-1.el6.x86_64\n--> Running transaction check\n---> Package slurm.x86_64 0:17.02.7-1.el6 will be installed\n--> Processing Dependency: slurm-plugins for package: slurm-17.02.7-1.el6.x86_64\n--> Running transaction check\n---> Package slurm-plugins.x86_64 0:17.02.7-1.el6 will be installed\n--> Finished Dependency Resolution\n\nDependencies Resolved\n\n================================================================================\n Package             Arch         Version               Repository         Size\n================================================================================\nInstalling:\n slurm-munge         x86_64       17.02.7-1.el6         local-slurm        16 k\nInstalling for dependencies:\n slurm               x86_64       17.02.7-1.el6         local-slurm        27 M\n slurm-plugins       x86_64       17.02.7-1.el6         local-slurm       1.1 M\n\nTransaction Summary\n================================================================================\nInstall       3 Package(s)\n\nTotal download size: 28 M\nInstalled size: 98 M\nDownloading Packages:\n--------------------------------------------------------------------------------\nTotal                                            77 MB/s |  28 MB     00:00     \nRunning rpm_check_debug\nRunning Transaction Test\nTransaction Test Succeeded\nRunning Transaction\n\r  Installing : slurm-plugins-17.02.7-1.el6.x86_64                           1/3 \n\r  Installing : slurm-17.02.7-1.el6.x86_64                                   2/3 \n\r  Installing : slurm-munge-17.02.7-1.el6.x86_64                             3/3 \n\r  Verifying  : slurm-plugins-17.02.7-1.el6.x86_64                           1/3 \n\r  Verifying  : slurm-17.02.7-1.el6.x86_64                                   2/3 \n\r  Verifying  : slurm-munge-17.02.7-1.el6.x86_64                             3/3 \n\nInstalled:\n  slurm-munge.x86_64 0:17.02.7-1.el6                                            \n\nDependency Installed:\n  slurm.x86_64 0:17.02.7-1.el6       slurm-plugins.x86_64 0:17.02.7-1.el6      \n\nComplete!\n"]}
changed: [worker002] => {"changed": true, "msg": "https://copr-be.cloud.fedoraproject.org/results/verdurin/slurm-17.02/epel-6-x86_64/repodata/repomd.xml: [Errno 12] Timeout on https://copr-be.cloud.fedoraproject.org/results/verdurin/slurm-17.02/epel-6-x86_64/repodata/repomd.xml: (28, 'connect() timed out!')\nTrying other mirror.\nWarning: RPMDB altered outside of yum.\n", "rc": 0, "results": ["Loaded plugins: fastestmirror, security\nSetting up Install Process\nLoading mirror speeds from cached hostfile\nResolving Dependencies\n--> Running transaction check\n---> Package slurm-munge.x86_64 0:17.02.7-1.el6 will be installed\n--> Processing Dependency: slurm for package: slurm-munge-17.02.7-1.el6.x86_64\n--> Running transaction check\n---> Package slurm.x86_64 0:17.02.7-1.el6 will be installed\n--> Processing Dependency: slurm-plugins for package: slurm-17.02.7-1.el6.x86_64\n--> Running transaction check\n---> Package slurm-plugins.x86_64 0:17.02.7-1.el6 will be installed\n--> Finished Dependency Resolution\n\nDependencies Resolved\n\n================================================================================\n Package             Arch         Version               Repository         Size\n================================================================================\nInstalling:\n slurm-munge         x86_64       17.02.7-1.el6         local-slurm        16 k\nInstalling for dependencies:\n slurm               x86_64       17.02.7-1.el6         local-slurm        27 M\n slurm-plugins       x86_64       17.02.7-1.el6         local-slurm       1.1 M\n\nTransaction Summary\n================================================================================\nInstall       3 Package(s)\n\nTotal download size: 28 M\nInstalled size: 98 M\nDownloading Packages:\n--------------------------------------------------------------------------------\nTotal                                            83 MB/s |  28 MB     00:00     \nRunning rpm_check_debug\nRunning Transaction Test\nTransaction Test Succeeded\nRunning Transaction\n\r  Installing : slurm-plugins-17.02.7-1.el6.x86_64                           1/3 \n\r  Installing : slurm-17.02.7-1.el6.x86_64                                   2/3 \n\r  Installing : slurm-munge-17.02.7-1.el6.x86_64                             3/3 \n\r  Verifying  : slurm-plugins-17.02.7-1.el6.x86_64                           1/3 \n\r  Verifying  : slurm-17.02.7-1.el6.x86_64                                   2/3 \n\r  Verifying  : slurm-munge-17.02.7-1.el6.x86_64                             3/3 \n\nInstalled:\n  slurm-munge.x86_64 0:17.02.7-1.el6                                            \n\nDependency Installed:\n  slurm.x86_64 0:17.02.7-1.el6       slurm-plugins.x86_64 0:17.02.7-1.el6      \n\nComplete!\n"]}

TASK [slurm-common : Allow MUNGE to start] *************************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:57
skipping: [master001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-common : Add `--syslog` option to `munged` startup] ****************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:66
skipping: [master001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-common : Reload systemd configuration] *****************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:77
skipping: [master001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-common : Add `--syslog` startup options for `munged` (Ubuntu 14.04)] ***
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:86
skipping: [master001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-common : Configure MUNGE] ******************************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:101
ok: [master001] => {"changed": false, "checksum": "222afe887f907039b799b394715664ffa584fec5", "dest": "/etc/munge/munge.key", "gid": 497, "group": "munge", "mode": "0400", "owner": "munge", "path": "/etc/munge/munge.key", "size": 1024, "state": "file", "uid": 497}
ok: [worker001] => {"changed": false, "checksum": "222afe887f907039b799b394715664ffa584fec5", "dest": "/etc/munge/munge.key", "gid": 497, "group": "munge", "mode": "0400", "owner": "munge", "path": "/etc/munge/munge.key", "size": 1024, "state": "file", "uid": 497}
ok: [worker002] => {"changed": false, "checksum": "222afe887f907039b799b394715664ffa584fec5", "dest": "/etc/munge/munge.key", "gid": 497, "group": "munge", "mode": "0400", "owner": "munge", "path": "/etc/munge/munge.key", "size": 1024, "state": "file", "uid": 497}

TASK [slurm-common : Ensure the MUNGE service is running] **********************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:114
ok: [master001] => {"changed": false, "enabled": true, "name": "munge", "state": "started"}
ok: [worker001] => {"changed": false, "enabled": true, "name": "munge", "state": "started"}
ok: [worker002] => {"changed": false, "enabled": true, "name": "munge", "state": "started"}

TASK [slurm-common : Deploy SLURM configuration file] **************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:86
changed: [master001] => {"changed": true, "checksum": "ae66d55ff2e05c03cf1bc134dbcfe01dca2b80da", "dest": "/etc/slurm/slurm.conf", "gid": 0, "group": "root", "md5sum": "6c5ff2ed0a85e8218b7c40c4d474e13a", "mode": "0444", "owner": "root", "size": 4154, "src": "/root/.ansible/tmp/ansible-tmp-1525351247.59-82364151537491/source", "state": "file", "uid": 0}
changed: [worker001] => {"changed": true, "checksum": "ae66d55ff2e05c03cf1bc134dbcfe01dca2b80da", "dest": "/etc/slurm/slurm.conf", "gid": 0, "group": "root", "md5sum": "6c5ff2ed0a85e8218b7c40c4d474e13a", "mode": "0444", "owner": "root", "size": 4154, "src": "/root/.ansible/tmp/ansible-tmp-1525351247.6-103503236999660/source", "state": "file", "uid": 0}
changed: [worker002] => {"changed": true, "checksum": "ae66d55ff2e05c03cf1bc134dbcfe01dca2b80da", "dest": "/etc/slurm/slurm.conf", "gid": 0, "group": "root", "md5sum": "6c5ff2ed0a85e8218b7c40c4d474e13a", "mode": "0444", "owner": "root", "size": 4154, "src": "/root/.ansible/tmp/ansible-tmp-1525351247.62-83470545221320/source", "state": "file", "uid": 0}

TASK [slurm-common : Deploy script to suspend cloud instance] ******************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:96
changed: [master001] => {"changed": true, "checksum": "4a7a8071752a9c1b762b7576296a86f973ebbb4c", "dest": "/etc/slurm/slurm.suspend.sh", "gid": 0, "group": "root", "md5sum": "3b8270f4f460ceed87c2c0c4b8cc9787", "mode": "0555", "owner": "root", "size": 602, "src": "/root/.ansible/tmp/ansible-tmp-1525351248.24-131348950712922/source", "state": "file", "uid": 0}
changed: [worker002] => {"changed": true, "checksum": "4a7a8071752a9c1b762b7576296a86f973ebbb4c", "dest": "/etc/slurm/slurm.suspend.sh", "gid": 0, "group": "root", "md5sum": "3b8270f4f460ceed87c2c0c4b8cc9787", "mode": "0555", "owner": "root", "size": 602, "src": "/root/.ansible/tmp/ansible-tmp-1525351248.28-230111453320761/source", "state": "file", "uid": 0}
changed: [worker001] => {"changed": true, "checksum": "4a7a8071752a9c1b762b7576296a86f973ebbb4c", "dest": "/etc/slurm/slurm.suspend.sh", "gid": 0, "group": "root", "md5sum": "3b8270f4f460ceed87c2c0c4b8cc9787", "mode": "0555", "owner": "root", "size": 602, "src": "/root/.ansible/tmp/ansible-tmp-1525351248.26-133342605682882/source", "state": "file", "uid": 0}

TASK [slurm-common : Deploy script to resume cloud instance] *******************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:106
changed: [worker001] => {"changed": true, "checksum": "0310b5e0e2f918162ad9215a0aa0141529eb354a", "dest": "/etc/slurm/slurm.resume.sh", "gid": 0, "group": "root", "md5sum": "0d6dbb066a019dddfc40a4f4c4627a93", "mode": "0555", "owner": "root", "size": 629, "src": "/root/.ansible/tmp/ansible-tmp-1525351248.92-113688214991766/source", "state": "file", "uid": 0}
changed: [worker002] => {"changed": true, "checksum": "0310b5e0e2f918162ad9215a0aa0141529eb354a", "dest": "/etc/slurm/slurm.resume.sh", "gid": 0, "group": "root", "md5sum": "0d6dbb066a019dddfc40a4f4c4627a93", "mode": "0555", "owner": "root", "size": 629, "src": "/root/.ansible/tmp/ansible-tmp-1525351248.94-169858522269550/source", "state": "file", "uid": 0}
changed: [master001] => {"changed": true, "checksum": "0310b5e0e2f918162ad9215a0aa0141529eb354a", "dest": "/etc/slurm/slurm.resume.sh", "gid": 0, "group": "root", "md5sum": "0d6dbb066a019dddfc40a4f4c4627a93", "mode": "0555", "owner": "root", "size": 629, "src": "/root/.ansible/tmp/ansible-tmp-1525351248.9-280128139526992/source", "state": "file", "uid": 0}

TASK [slurm-common : Install support packages (Debian/Ubuntu)] *****************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:116
skipping: [master001] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-common : Install SLURM packages (RHEL-compatible)] *****************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:127
skipping: [master001] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-client : Install required SLURM packages (Debian/Ubuntu)] **********
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-client/tasks/main.yml:4
skipping: [master001] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-client : Install required SLURM packages (older Debian/Ubuntu)] ****
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-client/tasks/main.yml:23
skipping: [master001] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-client : service] **************************************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-client/tasks/main.yml:35
skipping: [master001] => (item=slurm-llnl)  => {"changed": false, "item": "slurm-llnl", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => (item=slurm-llnl)  => {"changed": false, "item": "slurm-llnl", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item=slurm-llnl)  => {"changed": false, "item": "slurm-llnl", "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-client : Install SLURM packages (RHEL-compatible)] *****************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-client/tasks/main.yml:47
changed: [master001] => (item=[u'slurm', u'slurm-devel', u'slurm-perlapi', u'slurm-contribs', u'slurm-torque']) => {"changed": true, "item": ["slurm", "slurm-devel", "slurm-perlapi", "slurm-contribs", "slurm-torque"], "msg": "https://copr-be.cloud.fedoraproject.org/results/verdurin/slurm-17.02/epel-6-x86_64/repodata/repomd.xml: [Errno 12] Timeout on https://copr-be.cloud.fedoraproject.org/results/verdurin/slurm-17.02/epel-6-x86_64/repodata/repomd.xml: (28, 'connect() timed out!')\nTrying other mirror.\n", "rc": 0, "results": ["slurm-17.02.7-1.el6.x86_64 providing slurm is already installed", "Loaded plugins: fastestmirror, security\nSetting up Install Process\nLoading mirror speeds from cached hostfile\nResolving Dependencies\n--> Running transaction check\n---> Package slurm-contribs.x86_64 0:17.02.7-1.el6 will be installed\n---> Package slurm-devel.x86_64 0:17.02.7-1.el6 will be installed\n---> Package slurm-perlapi.x86_64 0:17.02.7-1.el6 will be installed\n---> Package slurm-torque.x86_64 0:17.02.7-1.el6 will be installed\n--> Finished Dependency Resolution\n\nDependencies Resolved\n\n================================================================================\n Package              Arch         Version              Repository         Size\n================================================================================\nInstalling:\n slurm-contribs       x86_64       17.02.7-1.el6        local-slurm        17 k\n slurm-devel          x86_64       17.02.7-1.el6        local-slurm       143 k\n slurm-perlapi        x86_64       17.02.7-1.el6        local-slurm       456 k\n slurm-torque         x86_64       17.02.7-1.el6        local-slurm        37 k\n\nTransaction Summary\n================================================================================\nInstall       4 Package(s)\n\nTotal download size: 653 k\nInstalled size: 2.5 M\nDownloading Packages:\n--------------------------------------------------------------------------------\nTotal                                            31 MB/s | 653 kB     00:00     \nRunning rpm_check_debug\nRunning Transaction Test\nTransaction Test Succeeded\nRunning Transaction\n\r  Installing : slurm-perlapi-17.02.7-1.el6.x86_64                           1/4 \n\r  Installing : slurm-torque-17.02.7-1.el6.x86_64                            2/4 \n\r  Installing : slurm-contribs-17.02.7-1.el6.x86_64                          3/4 \n\r  Installing : slurm-devel-17.02.7-1.el6.x86_64                             4/4 \n\r  Verifying  : slurm-perlapi-17.02.7-1.el6.x86_64                           1/4 \n\r  Verifying  : slurm-devel-17.02.7-1.el6.x86_64                             2/4 \n\r  Verifying  : slurm-torque-17.02.7-1.el6.x86_64                            3/4 \n\r  Verifying  : slurm-contribs-17.02.7-1.el6.x86_64                          4/4 \n\nInstalled:\n  slurm-contribs.x86_64 0:17.02.7-1.el6   slurm-devel.x86_64 0:17.02.7-1.el6   \n  slurm-perlapi.x86_64 0:17.02.7-1.el6    slurm-torque.x86_64 0:17.02.7-1.el6  \n\nComplete!\n"]}
changed: [worker001] => (item=[u'slurm', u'slurm-devel', u'slurm-perlapi', u'slurm-contribs', u'slurm-torque']) => {"changed": true, "item": ["slurm", "slurm-devel", "slurm-perlapi", "slurm-contribs", "slurm-torque"], "msg": "https://copr-be.cloud.fedoraproject.org/results/verdurin/slurm-17.02/epel-6-x86_64/repodata/repomd.xml: [Errno 12] Timeout on https://copr-be.cloud.fedoraproject.org/results/verdurin/slurm-17.02/epel-6-x86_64/repodata/repomd.xml: (28, 'connect() timed out!')\nTrying other mirror.\n", "rc": 0, "results": ["slurm-17.02.7-1.el6.x86_64 providing slurm is already installed", "Loaded plugins: fastestmirror, security\nSetting up Install Process\nLoading mirror speeds from cached hostfile\nResolving Dependencies\n--> Running transaction check\n---> Package slurm-contribs.x86_64 0:17.02.7-1.el6 will be installed\n---> Package slurm-devel.x86_64 0:17.02.7-1.el6 will be installed\n---> Package slurm-perlapi.x86_64 0:17.02.7-1.el6 will be installed\n---> Package slurm-torque.x86_64 0:17.02.7-1.el6 will be installed\n--> Finished Dependency Resolution\n\nDependencies Resolved\n\n================================================================================\n Package              Arch         Version              Repository         Size\n================================================================================\nInstalling:\n slurm-contribs       x86_64       17.02.7-1.el6        local-slurm        17 k\n slurm-devel          x86_64       17.02.7-1.el6        local-slurm       143 k\n slurm-perlapi        x86_64       17.02.7-1.el6        local-slurm       456 k\n slurm-torque         x86_64       17.02.7-1.el6        local-slurm        37 k\n\nTransaction Summary\n================================================================================\nInstall       4 Package(s)\n\nTotal download size: 653 k\nInstalled size: 2.5 M\nDownloading Packages:\n--------------------------------------------------------------------------------\nTotal                                            54 MB/s | 653 kB     00:00     \nRunning rpm_check_debug\nRunning Transaction Test\nTransaction Test Succeeded\nRunning Transaction\n\r  Installing : slurm-perlapi-17.02.7-1.el6.x86_64                           1/4 \n\r  Installing : slurm-torque-17.02.7-1.el6.x86_64                            2/4 \n\r  Installing : slurm-contribs-17.02.7-1.el6.x86_64                          3/4 \n\r  Installing : slurm-devel-17.02.7-1.el6.x86_64                             4/4 \n\r  Verifying  : slurm-perlapi-17.02.7-1.el6.x86_64                           1/4 \n\r  Verifying  : slurm-devel-17.02.7-1.el6.x86_64                             2/4 \n\r  Verifying  : slurm-torque-17.02.7-1.el6.x86_64                            3/4 \n\r  Verifying  : slurm-contribs-17.02.7-1.el6.x86_64                          4/4 \n\nInstalled:\n  slurm-contribs.x86_64 0:17.02.7-1.el6   slurm-devel.x86_64 0:17.02.7-1.el6   \n  slurm-perlapi.x86_64 0:17.02.7-1.el6    slurm-torque.x86_64 0:17.02.7-1.el6  \n\nComplete!\n"]}
changed: [worker002] => (item=[u'slurm', u'slurm-devel', u'slurm-perlapi', u'slurm-contribs', u'slurm-torque']) => {"changed": true, "item": ["slurm", "slurm-devel", "slurm-perlapi", "slurm-contribs", "slurm-torque"], "msg": "https://copr-be.cloud.fedoraproject.org/results/verdurin/slurm-17.02/epel-6-x86_64/repodata/repomd.xml: [Errno 12] Timeout on https://copr-be.cloud.fedoraproject.org/results/verdurin/slurm-17.02/epel-6-x86_64/repodata/repomd.xml: (28, 'connect() timed out!')\nTrying other mirror.\n", "rc": 0, "results": ["slurm-17.02.7-1.el6.x86_64 providing slurm is already installed", "Loaded plugins: fastestmirror, security\nSetting up Install Process\nLoading mirror speeds from cached hostfile\nResolving Dependencies\n--> Running transaction check\n---> Package slurm-contribs.x86_64 0:17.02.7-1.el6 will be installed\n---> Package slurm-devel.x86_64 0:17.02.7-1.el6 will be installed\n---> Package slurm-perlapi.x86_64 0:17.02.7-1.el6 will be installed\n---> Package slurm-torque.x86_64 0:17.02.7-1.el6 will be installed\n--> Finished Dependency Resolution\n\nDependencies Resolved\n\n================================================================================\n Package              Arch         Version              Repository         Size\n================================================================================\nInstalling:\n slurm-contribs       x86_64       17.02.7-1.el6        local-slurm        17 k\n slurm-devel          x86_64       17.02.7-1.el6        local-slurm       143 k\n slurm-perlapi        x86_64       17.02.7-1.el6        local-slurm       456 k\n slurm-torque         x86_64       17.02.7-1.el6        local-slurm        37 k\n\nTransaction Summary\n================================================================================\nInstall       4 Package(s)\n\nTotal download size: 653 k\nInstalled size: 2.5 M\nDownloading Packages:\n--------------------------------------------------------------------------------\nTotal                                            38 MB/s | 653 kB     00:00     \nRunning rpm_check_debug\nRunning Transaction Test\nTransaction Test Succeeded\nRunning Transaction\n\r  Installing : slurm-perlapi-17.02.7-1.el6.x86_64                           1/4 \n\r  Installing : slurm-torque-17.02.7-1.el6.x86_64                            2/4 \n\r  Installing : slurm-contribs-17.02.7-1.el6.x86_64                          3/4 \n\r  Installing : slurm-devel-17.02.7-1.el6.x86_64                             4/4 \n\r  Verifying  : slurm-perlapi-17.02.7-1.el6.x86_64                           1/4 \n\r  Verifying  : slurm-devel-17.02.7-1.el6.x86_64                             2/4 \n\r  Verifying  : slurm-torque-17.02.7-1.el6.x86_64                            3/4 \n\r  Verifying  : slurm-contribs-17.02.7-1.el6.x86_64                          4/4 \n\nInstalled:\n  slurm-contribs.x86_64 0:17.02.7-1.el6   slurm-devel.x86_64 0:17.02.7-1.el6   \n  slurm-perlapi.x86_64 0:17.02.7-1.el6    slurm-torque.x86_64 0:17.02.7-1.el6  \n\nComplete!\n"]}

TASK [slurm-client : Stop SLURM services (RHEL/CentOS 6.x)] ********************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-client/tasks/main.yml:59
changed: [worker001] => {"changed": true, "enabled": false, "name": "slurm", "state": "stopped"}
changed: [master001] => {"changed": true, "enabled": false, "name": "slurm", "state": "stopped"}
changed: [worker002] => {"changed": true, "enabled": false, "name": "slurm", "state": "stopped"}

TASK [slurm-client : Stop SLURM services (RHEL/CentOS 7.x)] ********************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-client/tasks/main.yml:66
skipping: [master001] => (item=slurmd)  => {"changed": false, "item": "slurmd", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [master001] => (item=slurmctld)  => {"changed": false, "item": "slurmctld", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => (item=slurmd)  => {"changed": false, "item": "slurmd", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => (item=slurmctld)  => {"changed": false, "item": "slurmctld", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item=slurmd)  => {"changed": false, "item": "slurmd", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item=slurmctld)  => {"changed": false, "item": "slurmctld", "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-worker : Load distribution-specific parameters] ********************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/main.yml:4
included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/init-RedHat.yml for master001, worker001, worker002

TASK [slurm-worker : Set SLURM worker playbook params (RHEL compatible)] *******
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/init-RedHat.yml:3
ok: [master001] => {"ansible_facts": {"slurmd_packages": ["slurm", "slurm-plugins", "slurm-sql"]}, "changed": false}
ok: [worker001] => {"ansible_facts": {"slurmd_packages": ["slurm", "slurm-plugins", "slurm-sql"]}, "changed": false}
ok: [worker002] => {"ansible_facts": {"slurmd_packages": ["slurm", "slurm-plugins", "slurm-sql"]}, "changed": false}

TASK [slurm-worker : Set SLURM worker service name (RHEL 7.x compatible)] ******
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/init-RedHat.yml:11
skipping: [master001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-worker : Set SLURM worker service name (RHEL 6.x compatible)] ******
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/init-RedHat.yml:16
ok: [master001] => {"ansible_facts": {"slurmd_service": "slurm"}, "changed": false}
ok: [worker001] => {"ansible_facts": {"slurmd_service": "slurm"}, "changed": false}
ok: [worker002] => {"ansible_facts": {"slurmd_service": "slurm"}, "changed": false}

TASK [slurm-worker : Deploy SLURM configuration files] *************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/main.yml:7
changed: [master001] => (item=gres.conf) => {"changed": true, "checksum": "afc9ac4c3247ee557700305700c9743a79f0dfd0", "dest": "/etc/slurm/gres.conf", "gid": 0, "group": "root", "item": "gres.conf", "md5sum": "a178d15a40bf70261e38f1cd729a8d56", "mode": "0444", "owner": "root", "size": 297, "src": "/root/.ansible/tmp/ansible-tmp-1525351404.98-256596653634727/source", "state": "file", "uid": 0}
changed: [worker001] => (item=gres.conf) => {"changed": true, "checksum": "afc9ac4c3247ee557700305700c9743a79f0dfd0", "dest": "/etc/slurm/gres.conf", "gid": 0, "group": "root", "item": "gres.conf", "md5sum": "a178d15a40bf70261e38f1cd729a8d56", "mode": "0444", "owner": "root", "size": 297, "src": "/root/.ansible/tmp/ansible-tmp-1525351405.0-169705334624630/source", "state": "file", "uid": 0}
changed: [worker002] => (item=gres.conf) => {"changed": true, "checksum": "afc9ac4c3247ee557700305700c9743a79f0dfd0", "dest": "/etc/slurm/gres.conf", "gid": 0, "group": "root", "item": "gres.conf", "md5sum": "a178d15a40bf70261e38f1cd729a8d56", "mode": "0444", "owner": "root", "size": 297, "src": "/root/.ansible/tmp/ansible-tmp-1525351405.03-271318129532787/source", "state": "file", "uid": 0}

TASK [slurm-worker : Deploy kernel config check script] ************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/cgroup.yml:3
skipping: [master001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-worker : Check that kernel is configured for cgroups] **************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/cgroup.yml:12
skipping: [master001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-worker : Ensure cgroup filesystems are mounted] ********************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/cgroup.yml:18
skipping: [master001] => (item=blkio)  => {"changed": false, "item": "blkio", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [master001] => (item=cpuacct)  => {"changed": false, "item": "cpuacct", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => (item=blkio)  => {"changed": false, "item": "blkio", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => (item=cpuacct)  => {"changed": false, "item": "cpuacct", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [master001] => (item=cpuset)  => {"changed": false, "item": "cpuset", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item=blkio)  => {"changed": false, "item": "blkio", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => (item=cpuset)  => {"changed": false, "item": "cpuset", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item=cpuacct)  => {"changed": false, "item": "cpuacct", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [master001] => (item=devices)  => {"changed": false, "item": "devices", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => (item=devices)  => {"changed": false, "item": "devices", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item=cpuset)  => {"changed": false, "item": "cpuset", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [master001] => (item=freezer)  => {"changed": false, "item": "freezer", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => (item=freezer)  => {"changed": false, "item": "freezer", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item=devices)  => {"changed": false, "item": "devices", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [master001] => (item=memory)  => {"changed": false, "item": "memory", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => (item=memory)  => {"changed": false, "item": "memory", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item=freezer)  => {"changed": false, "item": "freezer", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item=memory)  => {"changed": false, "item": "memory", "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-worker : Ensure kernel is booted with swap accounting enabled] *****
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/cgroup.yml:44
skipping: [master001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-worker : Check if swap accouting is enabled (may fail!)] ***********
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/cgroup.yml:53
skipping: [master001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-worker : Reboot to enable swap accounting] *************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/cgroup.yml:59
skipping: [master001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-worker : Wait for server to come up again] *************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/cgroup.yml:72
skipping: [master001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-worker : Ensure release agent directory exists] ********************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/cgroup.yml:84
skipping: [master001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-worker : Deploy cgroup-specific release agent script] **************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/cgroup.yml:93
skipping: [master001] => (item=blkio)  => {"changed": false, "item": "blkio", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [master001] => (item=cpuacct)  => {"changed": false, "item": "cpuacct", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => (item=blkio)  => {"changed": false, "item": "blkio", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => (item=cpuacct)  => {"changed": false, "item": "cpuacct", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [master001] => (item=cpuset)  => {"changed": false, "item": "cpuset", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item=blkio)  => {"changed": false, "item": "blkio", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => (item=cpuset)  => {"changed": false, "item": "cpuset", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [master001] => (item=devices)  => {"changed": false, "item": "devices", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item=cpuacct)  => {"changed": false, "item": "cpuacct", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => (item=devices)  => {"changed": false, "item": "devices", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [master001] => (item=freezer)  => {"changed": false, "item": "freezer", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item=cpuset)  => {"changed": false, "item": "cpuset", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => (item=freezer)  => {"changed": false, "item": "freezer", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [master001] => (item=memory)  => {"changed": false, "item": "memory", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item=devices)  => {"changed": false, "item": "devices", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => (item=memory)  => {"changed": false, "item": "memory", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item=freezer)  => {"changed": false, "item": "freezer", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item=memory)  => {"changed": false, "item": "memory", "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-worker : Deploy SLURM cgroup configuration file (main)] ************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/cgroup.yml:103
skipping: [master001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-worker : Deploy SLURM cgroup configuration file (devices)] *********
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/cgroup.yml:112
skipping: [master001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-worker : Install SLURM worker packages] ****************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/main.yml:27
changed: [master001] => (item=[u'slurm', u'slurm-plugins', u'slurm-sql']) => {"changed": true, "item": ["slurm", "slurm-plugins", "slurm-sql"], "msg": "https://copr-be.cloud.fedoraproject.org/results/verdurin/slurm-17.02/epel-6-x86_64/repodata/repomd.xml: [Errno 12] Timeout on https://copr-be.cloud.fedoraproject.org/results/verdurin/slurm-17.02/epel-6-x86_64/repodata/repomd.xml: (28, 'connect() timed out!')\nTrying other mirror.\n", "rc": 0, "results": ["slurm-17.02.7-1.el6.x86_64 providing slurm is already installed", "slurm-plugins-17.02.7-1.el6.x86_64 providing slurm-plugins is already installed", "Loaded plugins: fastestmirror, security\nSetting up Install Process\nLoading mirror speeds from cached hostfile\nResolving Dependencies\n--> Running transaction check\n---> Package slurm-sql.x86_64 0:17.02.7-1.el6 will be installed\n--> Finished Dependency Resolution\n\nDependencies Resolved\n\n================================================================================\n Package          Arch          Version                Repository          Size\n================================================================================\nInstalling:\n slurm-sql        x86_64        17.02.7-1.el6          local-slurm        306 k\n\nTransaction Summary\n================================================================================\nInstall       1 Package(s)\n\nTotal download size: 306 k\nInstalled size: 1.1 M\nDownloading Packages:\nRunning rpm_check_debug\nRunning Transaction Test\nTransaction Test Succeeded\nRunning Transaction\n\r  Installing : slurm-sql-17.02.7-1.el6.x86_64                               1/1 \n\r  Verifying  : slurm-sql-17.02.7-1.el6.x86_64                               1/1 \n\nInstalled:\n  slurm-sql.x86_64 0:17.02.7-1.el6                                              \n\nComplete!\n"]}
changed: [worker001] => (item=[u'slurm', u'slurm-plugins', u'slurm-sql']) => {"changed": true, "item": ["slurm", "slurm-plugins", "slurm-sql"], "msg": "https://copr-be.cloud.fedoraproject.org/results/verdurin/slurm-17.02/epel-6-x86_64/repodata/repomd.xml: [Errno 12] Timeout on https://copr-be.cloud.fedoraproject.org/results/verdurin/slurm-17.02/epel-6-x86_64/repodata/repomd.xml: (28, 'connect() timed out!')\nTrying other mirror.\n", "rc": 0, "results": ["slurm-17.02.7-1.el6.x86_64 providing slurm is already installed", "slurm-plugins-17.02.7-1.el6.x86_64 providing slurm-plugins is already installed", "Loaded plugins: fastestmirror, security\nSetting up Install Process\nLoading mirror speeds from cached hostfile\nResolving Dependencies\n--> Running transaction check\n---> Package slurm-sql.x86_64 0:17.02.7-1.el6 will be installed\n--> Finished Dependency Resolution\n\nDependencies Resolved\n\n================================================================================\n Package          Arch          Version                Repository          Size\n================================================================================\nInstalling:\n slurm-sql        x86_64        17.02.7-1.el6          local-slurm        306 k\n\nTransaction Summary\n================================================================================\nInstall       1 Package(s)\n\nTotal download size: 306 k\nInstalled size: 1.1 M\nDownloading Packages:\nRunning rpm_check_debug\nRunning Transaction Test\nTransaction Test Succeeded\nRunning Transaction\n\r  Installing : slurm-sql-17.02.7-1.el6.x86_64                               1/1 \n\r  Verifying  : slurm-sql-17.02.7-1.el6.x86_64                               1/1 \n\nInstalled:\n  slurm-sql.x86_64 0:17.02.7-1.el6                                              \n\nComplete!\n"]}
changed: [worker002] => (item=[u'slurm', u'slurm-plugins', u'slurm-sql']) => {"changed": true, "item": ["slurm", "slurm-plugins", "slurm-sql"], "msg": "https://copr-be.cloud.fedoraproject.org/results/verdurin/slurm-17.02/epel-6-x86_64/repodata/repomd.xml: [Errno 12] Timeout on https://copr-be.cloud.fedoraproject.org/results/verdurin/slurm-17.02/epel-6-x86_64/repodata/repomd.xml: (28, 'connect() timed out!')\nTrying other mirror.\n", "rc": 0, "results": ["slurm-17.02.7-1.el6.x86_64 providing slurm is already installed", "slurm-plugins-17.02.7-1.el6.x86_64 providing slurm-plugins is already installed", "Loaded plugins: fastestmirror, security\nSetting up Install Process\nLoading mirror speeds from cached hostfile\nResolving Dependencies\n--> Running transaction check\n---> Package slurm-sql.x86_64 0:17.02.7-1.el6 will be installed\n--> Finished Dependency Resolution\n\nDependencies Resolved\n\n================================================================================\n Package          Arch          Version                Repository          Size\n================================================================================\nInstalling:\n slurm-sql        x86_64        17.02.7-1.el6          local-slurm        306 k\n\nTransaction Summary\n================================================================================\nInstall       1 Package(s)\n\nTotal download size: 306 k\nInstalled size: 1.1 M\nDownloading Packages:\nRunning rpm_check_debug\nRunning Transaction Test\nTransaction Test Succeeded\nRunning Transaction\n\r  Installing : slurm-sql-17.02.7-1.el6.x86_64                               1/1 \n\r  Verifying  : slurm-sql-17.02.7-1.el6.x86_64                               1/1 \n\nInstalled:\n  slurm-sql.x86_64 0:17.02.7-1.el6                                              \n\nComplete!\n"]}

TASK [slurm-worker : Ensure SLURMd starts at boot] *****************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/main.yml:33
changed: [worker002] => {"changed": true, "enabled": true, "name": "slurm"}
changed: [master001] => {"changed": true, "enabled": true, "name": "slurm"}
changed: [worker001] => {"changed": true, "enabled": true, "name": "slurm"}

PLAY [Setup OBS-initial mount directory and password file] *********************

TASK [setup] *******************************************************************
ok: [master001]
ok: [worker001]
ok: [worker002]

TASK [Create the obs directory] ************************************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/s3fs.yml:8
skipping: [master001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [Create the obs password file] ********************************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/s3fs.yml:13
skipping: [master001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [Setup OBS-insert into password file] *************************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/s3fs.yml:19
skipping: [master001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [Setup OBS-umount obs] ****************************************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/s3fs.yml:22
skipping: [master001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [Setup OBS-mount obs] *****************************************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/s3fs.yml:29
skipping: [master001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [Remove item from /etc/fstaba] ********************************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/s3fs.yml:32
skipping: [master001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [Add to /etc/fstab] *******************************************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/s3fs.yml:39
skipping: [master001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

PLAY [Slurm master Playbook] ***************************************************

TASK [setup] *******************************************************************
ok: [master001]

TASK [nis : Load distribution-specific parameters] *****************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/main.yml:3
included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/init-RedHat.yml for master001

TASK [nis : Set NIS common playbook params (CentOS/RHEL)] **********************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/init-RedHat.yml:3
ok: [master001] => {"ansible_facts": {"nis_client_packages": ["ypbind"], "nis_client_services": ["rpcbind", "ypbind"], "nis_common_packages": ["yp-tools"], "nis_master_packages": ["ypserv"], "nis_master_services": ["rpcbind", "yppasswdd", "ypserv"], "nis_securenets_path": "/var/yp/securenets"}, "changed": false}

TASK [nis : Pre-load debconf answer to questions (Debian/Ubuntu)] **************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/main.yml:7
skipping: [master001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [nis : Deploy additional NIS configuration (Debian/Ubuntu)] ***************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/main.yml:18
skipping: [master001] => (item=etc/default/nis)  => {"changed": false, "item": "etc/default/nis", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [master001] => (item=etc/defaultdomain)  => {"changed": false, "item": "etc/defaultdomain", "skip_reason": "Conditional check failed", "skipped": true}

TASK [nis : Set NIS domain (CentOS/RHEL)] **************************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/main.yml:29
changed: [master001] => {"backup": "", "changed": true, "msg": "line added"}

TASK [nis : Install NIS common packages] ***************************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/main.yml:39
ok: [master001] => (item=[u'yp-tools']) => {"changed": false, "item": ["yp-tools"], "msg": "", "rc": 0, "results": ["yp-tools-2.9-12.el6.x86_64 providing yp-tools is already installed"]}

TASK [nis : Deploy `ypserv` configuration files] *******************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml:3
ok: [master001] => {"changed": false, "gid": 0, "group": "root", "mode": "0400", "owner": "root", "path": "/var/yp/securenets", "size": 706, "state": "file", "uid": 0}

TASK [nis : Deploy `yppasswdd` configuration file (CentOS/RHEL)] ***************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml:12
ok: [master001] => {"changed": false, "checksum": "18392b4e3ecc4f9e7832c274663b5f6cee00d3f1", "dest": "/etc/sysconfig/yppasswdd", "gid": 0, "group": "root", "mode": "0444", "owner": "root", "path": "/etc/sysconfig/yppasswdd", "size": 681, "state": "file", "uid": 0}

TASK [nis : Install NIS master server packages] ********************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml:22
ok: [master001] => (item=[u'ypserv']) => {"changed": false, "item": ["ypserv"], "msg": "", "rc": 0, "results": ["ypserv-2.19-31.el6.x86_64 providing ypserv is already installed"]}

TASK [nis : Ensure `ypserv` starts at boot] ************************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml:33
ok: [master001] => (item=rpcbind) => {"changed": false, "enabled": true, "item": "rpcbind", "name": "rpcbind", "state": "started"}
ok: [master001] => (item=yppasswdd) => {"changed": false, "enabled": true, "item": "yppasswdd", "name": "yppasswdd", "state": "started"}
ok: [master001] => (item=ypserv) => {"changed": false, "enabled": true, "item": "ypserv", "name": "ypserv", "state": "started"}

TASK [nis : Update NIS/YP databases (NIS master server)] ***********************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml:41
changed: [master001] => {"changed": true, "cmd": ["make"], "delta": "0:00:00.050952", "end": "2018-05-03 20:44:33.648597", "rc": 0, "start": "2018-05-03 20:44:33.597645", "stderr": "", "stdout": "gmake[1]: Entering directory `/var/yp/elasticluster'\nUpdating netid.byname...\ngmake[1]: Leaving directory `/var/yp/elasticluster'", "stdout_lines": ["gmake[1]: Entering directory `/var/yp/elasticluster'", "Updating netid.byname...", "gmake[1]: Leaving directory `/var/yp/elasticluster'"], "warnings": []}

TASK [nis : Deploy `ypbind` configuration files] *******************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypbind.yml:3
skipping: [master001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [nis : Ensure NIS/YP is used as a name service] ***************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypbind.yml:12
skipping: [master001] => (item=passwd)  => {"changed": false, "item": "passwd", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [master001] => (item=group)  => {"changed": false, "item": "group", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [master001] => (item=shadow)  => {"changed": false, "item": "shadow", "skip_reason": "Conditional check failed", "skipped": true}

TASK [nis : Replace `compat` in `/etc/nsswitch.conf` with `files nis`] *********
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypbind.yml:26
skipping: [master001] => (item=passwd)  => {"changed": false, "item": "passwd", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [master001] => (item=group)  => {"changed": false, "item": "group", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [master001] => (item=shadow)  => {"changed": false, "item": "shadow", "skip_reason": "Conditional check failed", "skipped": true}

TASK [nis : Ensure `ypbind` starts at boot] ************************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypbind.yml:37
skipping: [master001] => (item=rpcbind)  => {"changed": false, "item": "rpcbind", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [master001] => (item=ypbind)  => {"changed": false, "item": "ypbind", "skip_reason": "Conditional check failed", "skipped": true}

TASK [nfs-server : Load distribution-specific parameters] **********************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-server/tasks/main.yml:6
included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-server/tasks/init-RedHat.yml for master001

TASK [nfs-server : Set NFS server variables (RHEL/CentOS 7.x)] *****************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-server/tasks/init-RedHat.yml:6
skipping: [master001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [nfs-server : Set NFS server variables (RHEL/CentOS 6.x)] *****************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-server/tasks/init-RedHat.yml:20
ok: [master001] => {"ansible_facts": {"_nfs_server_started_state": "started", "nfs_server_packages": ["nfs-utils"], "nfs_server_services": ["rpcbind", "nfslock", "nfs"]}, "changed": false}

TASK [nfs-server : install NFS server software] ********************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-server/tasks/main.yml:12
ok: [master001] => (item=[u'nfs-utils']) => {"changed": false, "item": ["nfs-utils"], "msg": "", "rc": 0, "results": ["nfs-utils-1:1.2.3-75.el6_9.x86_64 providing nfs-utils is already installed"]}

TASK [nfs-server : Export directories] *****************************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-server/tasks/main.yml:25
changed: [master001] => (item={u'path': u'/home', u'clients': [u'worker001', u'worker002']}) => {"changed": true, "item": {"clients": ["worker001", "worker002"], "path": "/home"}}

TASK [nfs-server : Ensure portmapper is running] *******************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-server/tasks/main.yml:38
ok: [master001] => {"changed": false, "enabled": true, "name": "rpcbind", "state": "started"}

TASK [nfs-server : Ensure NFS server is running (Debian 8 "jessie")] ***********
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-server/tasks/main.yml:48
skipping: [master001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [nfs-server : Ensure NFS server is running] *******************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-server/tasks/main.yml:59
ok: [master001] => (item=rpcbind) => {"changed": false, "enabled": true, "item": "rpcbind", "name": "rpcbind", "state": "started"}
ok: [master001] => (item=nfslock) => {"changed": false, "enabled": true, "item": "nfslock", "name": "nfslock", "state": "started"}
ok: [master001] => (item=nfs) => {"changed": false, "enabled": true, "item": "nfs", "name": "nfs", "state": "started"}

TASK [nfs-server : Reload NFS exports file] ************************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-server/tasks/main.yml:71
changed: [master001] => {"changed": true, "cmd": ["exportfs", "-r"], "delta": "0:00:00.003028", "end": "2018-05-03 20:44:35.619568", "rc": 0, "start": "2018-05-03 20:44:35.616540", "stderr": "", "stdout": "", "stdout_lines": [], "warnings": []}

TASK [nfs-server : Restart NFS server services] ********************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-server/tasks/main.yml:79
changed: [master001] => {"changed": true, "name": "nfs", "state": "started"}

TASK [slurm-common : Load distribution-specific parameters] ********************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:3
included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/init-RedHat.yml for master001

TASK [slurm-common : Set SLURM common playbook params (RHEL 7.x compatible)] ***
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/init-RedHat.yml:3
ok: [master001] => {"ansible_facts": {"slurm_pid_dir": "/var/run"}, "changed": false}

TASK [slurm-common : Create `slurm` system group] ******************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:16
ok: [master001] => {"changed": false, "gid": 498, "name": "slurm", "state": "present", "system": true}

TASK [slurm-common : Create `slurm` system user] *******************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:24
ok: [master001] => {"append": false, "changed": false, "comment": "", "group": 498, "home": "/home/slurm", "move_home": false, "name": "slurm", "shell": "/bin/bash", "state": "present", "uid": 498}

TASK [slurm-common : Create work directory {{item}}] ***************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:33
changed: [master001] => (item=/etc/slurm) => {"changed": true, "gid": 498, "group": "slurm", "item": "/etc/slurm", "mode": "0755", "owner": "slurm", "path": "/etc/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [master001] => (item=/var/lib/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/lib/slurm", "mode": "0755", "owner": "slurm", "path": "/var/lib/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [master001] => (item=/var/lib/slurm/checkpoint) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/lib/slurm/checkpoint", "mode": "0755", "owner": "slurm", "path": "/var/lib/slurm/checkpoint", "size": 4096, "state": "directory", "uid": 498}
ok: [master001] => (item=/var/lib/slurm/slurmctld) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/lib/slurm/slurmctld", "mode": "0755", "owner": "slurm", "path": "/var/lib/slurm/slurmctld", "size": 4096, "state": "directory", "uid": 498}
ok: [master001] => (item=/var/lib/slurm/slurmd) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/lib/slurm/slurmd", "mode": "0755", "owner": "slurm", "path": "/var/lib/slurm/slurmd", "size": 4096, "state": "directory", "uid": 498}
ok: [master001] => (item=/var/log/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/log/slurm", "mode": "0755", "owner": "slurm", "path": "/var/log/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [master001] => (item=/var/run/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/run/slurm", "mode": "0755", "owner": "slurm", "path": "/var/run/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [master001] => (item=/var/spool/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/spool/slurm", "mode": "0755", "owner": "slurm", "path": "/var/spool/slurm", "size": 4096, "state": "directory", "uid": 498}

TASK [slurm-common : Make compatibility symlinks] ******************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:56
skipping: [master001] => (item={u'to': u'/var/lib/slurm-llnl', u'from': u'/var/lib/slurm'})  => {"changed": false, "item": {"from": "/var/lib/slurm", "to": "/var/lib/slurm-llnl"}, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [master001] => (item={u'to': u'/etc/slurm-llnl', u'from': u'/etc/slurm'})  => {"changed": false, "item": {"from": "/etc/slurm", "to": "/etc/slurm-llnl"}, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [master001] => (item={u'to': u'/var/log/slurm-llnl', u'from': u'/var/log/slurm'})  => {"changed": false, "item": {"from": "/var/log/slurm", "to": "/var/log/slurm-llnl"}, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [master001] => (item={u'to': u'/var/run/slurm-llnl', u'from': u'/var/run/slurm'})  => {"changed": false, "item": {"from": "/var/run/slurm", "to": "/var/run/slurm-llnl"}, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-common : Enable Copr SLURM repo by verdurin] ***********************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:73
ok: [master001] => {"changed": false, "gid": 0, "group": "root", "mode": "0444", "owner": "root", "path": "/etc/yum.repos.d/copr-slurm.repo", "size": 615, "state": "file", "uid": 0}

TASK [slurm-common : Prevent MUNGE from starting] ******************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:20
skipping: [master001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-common : Install MUNGE (Debian/Ubuntu)] ****************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:31
skipping: [master001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-common : Install MUNGE (RHEL-compatible)] **************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:43
ok: [master001] => {"changed": false, "msg": "", "rc": 0, "results": ["slurm-munge-17.02.7-1.el6.x86_64 providing slurm-munge is already installed"]}

TASK [slurm-common : Allow MUNGE to start] *************************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:57
skipping: [master001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-common : Add `--syslog` option to `munged` startup] ****************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:66
skipping: [master001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-common : Reload systemd configuration] *****************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:77
skipping: [master001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-common : Add `--syslog` startup options for `munged` (Ubuntu 14.04)] ***
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:86
skipping: [master001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-common : Configure MUNGE] ******************************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:101
ok: [master001] => {"changed": false, "checksum": "222afe887f907039b799b394715664ffa584fec5", "dest": "/etc/munge/munge.key", "gid": 497, "group": "munge", "mode": "0400", "owner": "munge", "path": "/etc/munge/munge.key", "size": 1024, "state": "file", "uid": 497}

TASK [slurm-common : Ensure the MUNGE service is running] **********************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:114
ok: [master001] => {"changed": false, "enabled": true, "name": "munge", "state": "started"}

TASK [slurm-common : Deploy SLURM configuration file] **************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:86
ok: [master001] => {"changed": false, "gid": 0, "group": "root", "mode": "0444", "owner": "root", "path": "/etc/slurm/slurm.conf", "size": 4154, "state": "file", "uid": 0}

TASK [slurm-common : Deploy script to suspend cloud instance] ******************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:96
ok: [master001] => {"changed": false, "gid": 0, "group": "root", "mode": "0555", "owner": "root", "path": "/etc/slurm/slurm.suspend.sh", "size": 602, "state": "file", "uid": 0}

TASK [slurm-common : Deploy script to resume cloud instance] *******************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:106
ok: [master001] => {"changed": false, "gid": 0, "group": "root", "mode": "0555", "owner": "root", "path": "/etc/slurm/slurm.resume.sh", "size": 629, "state": "file", "uid": 0}

TASK [slurm-common : Install support packages (Debian/Ubuntu)] *****************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:116
skipping: [master001] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-common : Install SLURM packages (RHEL-compatible)] *****************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:127
skipping: [master001] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-client : Install required SLURM packages (Debian/Ubuntu)] **********
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-client/tasks/main.yml:4
skipping: [master001] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-client : Install required SLURM packages (older Debian/Ubuntu)] ****
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-client/tasks/main.yml:23
skipping: [master001] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-client : service] **************************************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-client/tasks/main.yml:35
skipping: [master001] => (item=slurm-llnl)  => {"changed": false, "item": "slurm-llnl", "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-client : Install SLURM packages (RHEL-compatible)] *****************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-client/tasks/main.yml:47
ok: [master001] => (item=[u'slurm', u'slurm-devel', u'slurm-perlapi', u'slurm-contribs', u'slurm-torque']) => {"changed": false, "item": ["slurm", "slurm-devel", "slurm-perlapi", "slurm-contribs", "slurm-torque"], "msg": "", "rc": 0, "results": ["slurm-17.02.7-1.el6.x86_64 providing slurm is already installed", "slurm-devel-17.02.7-1.el6.x86_64 providing slurm-devel is already installed", "slurm-perlapi-17.02.7-1.el6.x86_64 providing slurm-perlapi is already installed", "slurm-contribs-17.02.7-1.el6.x86_64 providing slurm-contribs is already installed", "slurm-torque-17.02.7-1.el6.x86_64 providing slurm-torque is already installed"]}

TASK [slurm-client : Stop SLURM services (RHEL/CentOS 6.x)] ********************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-client/tasks/main.yml:59
changed: [master001] => {"changed": true, "enabled": false, "name": "slurm", "state": "stopped"}

TASK [slurm-client : Stop SLURM services (RHEL/CentOS 7.x)] ********************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-client/tasks/main.yml:66
skipping: [master001] => (item=slurmd)  => {"changed": false, "item": "slurmd", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [master001] => (item=slurmctld)  => {"changed": false, "item": "slurmctld", "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-master : Load distribution-specific parameters] ********************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-master/tasks/main.yml:4
included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-master/tasks/init-RedHat.yml for master001

TASK [slurm-master : Set SLURM master playbook params (RHEL 7.x compatible)] ***
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-master/tasks/init-RedHat.yml:3
skipping: [master001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-master : Set SLURM playbook params (RHEL 6.x compatible)] **********
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-master/tasks/init-RedHat.yml:18
ok: [master001] => {"ansible_facts": {"slurmctld_packages": ["mailx", "slurm-plugins", "slurm"], "slurmctld_service_name": "slurm", "slurmdbd_packages": ["slurm-sql", "slurm-slurmdbd"], "slurmdbd_service_name": "slurmdbd"}, "changed": false}

TASK [slurm-master : Set variables (Debian/Ubuntu)] ****************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-master/tasks/db.yml:3
skipping: [master001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-master : Set variables (older Debian/Ubuntu)] **********************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-master/tasks/db.yml:11
skipping: [master001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-master : Set variables (RHEL 7.x)] *********************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-master/tasks/db.yml:19
skipping: [master001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-master : Set variables (RHEL 6.x)] *********************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-master/tasks/db.yml:27
ok: [master001] => {"ansible_facts": {"slurm_db_python_pkg": "MySQL-python", "slurm_db_server_name": "MySQL", "slurm_db_server_pkg": "mysql-server", "slurm_db_service_name": "mysqld"}, "changed": false}

TASK [slurm-master : Install MySQL, used for SLURM accounting] *****************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-master/tasks/db.yml:36
ok: [master001] => (item=[u'mysql-server', u'MySQL-python']) => {"changed": false, "item": ["mysql-server", "MySQL-python"], "msg": "", "rc": 0, "results": ["mysql-server-5.1.73-8.el6_8.x86_64 providing mysql-server is already installed", "MySQL-python-1.2.3-0.3.c1.1.el6.x86_64 providing MySQL-python is already installed"]}

TASK [slurm-master : Ensure MySQL daemon is up] ********************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-master/tasks/db.yml:48
ok: [master001] => {"changed": false, "enabled": true, "name": "mysqld", "state": "started"}

TASK [slurm-master : Create DB for SLURMDBD] ***********************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-master/tasks/db.yml:58
ok: [master001] => {"changed": false, "db": "slurm"}

TASK [slurm-master : Create DB user for SLURMDBD] ******************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-master/tasks/db.yml:68
changed: [master001] => (item=master001) => {"changed": true, "item": "master001", "user": "slurm"}
ok: [master001] => (item=localhost) => {"changed": false, "item": "localhost", "user": "slurm"}

TASK [slurm-master : Deploy SLURMDBD configuration] ****************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-master/tasks/install-slurmdbd.yml:5
NOTIFIED HANDLER restart slurmdbd
changed: [master001] => {"changed": true, "checksum": "2123863c7b7980504ada69584fa7d5b2c37eb76f", "dest": "/etc/slurm/slurmdbd.conf", "gid": 0, "group": "root", "md5sum": "8fb226da68b994d3893f33200dd75069", "mode": "0444", "owner": "root", "size": 2862, "src": "/root/.ansible/tmp/ansible-tmp-1525351483.32-30666816444395/source", "state": "file", "uid": 0}

TASK [slurm-master : Install SLURM DBD packages] *******************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-master/tasks/install-slurmdbd.yml:19
changed: [master001] => (item=[u'slurm-sql', u'slurm-slurmdbd']) => {"changed": true, "item": ["slurm-sql", "slurm-slurmdbd"], "msg": "https://copr-be.cloud.fedoraproject.org/results/verdurin/slurm-17.02/epel-6-x86_64/repodata/repomd.xml: [Errno 12] Timeout on https://copr-be.cloud.fedoraproject.org/results/verdurin/slurm-17.02/epel-6-x86_64/repodata/repomd.xml: (28, 'connect() timed out!')\nTrying other mirror.\n", "rc": 0, "results": ["slurm-sql-17.02.7-1.el6.x86_64 providing slurm-sql is already installed", "Loaded plugins: fastestmirror, security\nSetting up Install Process\nLoading mirror speeds from cached hostfile\nResolving Dependencies\n--> Running transaction check\n---> Package slurm-slurmdbd.x86_64 0:17.02.7-1.el6 will be installed\n--> Finished Dependency Resolution\n\nDependencies Resolved\n\n================================================================================\n Package              Arch         Version              Repository         Size\n================================================================================\nInstalling:\n slurm-slurmdbd       x86_64       17.02.7-1.el6        local-slurm       1.1 M\n\nTransaction Summary\n================================================================================\nInstall       1 Package(s)\n\nTotal download size: 1.1 M\nInstalled size: 3.9 M\nDownloading Packages:\nRunning rpm_check_debug\nRunning Transaction Test\nTransaction Test Succeeded\nRunning Transaction\n\r  Installing : slurm-slurmdbd-17.02.7-1.el6.x86_64                          1/1 \n\r  Verifying  : slurm-slurmdbd-17.02.7-1.el6.x86_64                          1/1 \n\nInstalled:\n  slurm-slurmdbd.x86_64 0:17.02.7-1.el6                                         \n\nComplete!\n"]}

TASK [slurm-master : Ensure `slurmdbd` is running] *****************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-master/tasks/install-slurmdbd.yml:26
changed: [master001] => (item=slurmdbd) => {"changed": true, "enabled": true, "item": "slurmdbd", "name": "slurmdbd", "state": "started"}

TASK [slurm-master : Install SLURM master packages] ****************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-master/tasks/install-slurmctld.yml:3
ok: [master001] => (item=[u'mailx', u'slurm-plugins', u'slurm']) => {"changed": false, "item": ["mailx", "slurm-plugins", "slurm"], "msg": "", "rc": 0, "results": ["mailx-12.4-8.el6_6.x86_64 providing mailx is already installed", "slurm-plugins-17.02.7-1.el6.x86_64 providing slurm-plugins is already installed", "slurm-17.02.7-1.el6.x86_64 providing slurm is already installed"]}

TASK [slurm-master : Create cluster in accounting database] ********************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-master/tasks/main.yml:13
changed: [master001] => {"changed": true, "cmd": "sacctmgr --parsable --noheader list cluster | grep '^elasticluster|' || sacctmgr -i -Q add cluster elasticluster", "delta": "0:00:00.327309", "end": "2018-05-03 20:45:48.205583", "rc": 0, "start": "2018-05-03 20:45:47.878274", "stderr": "", "stdout": "elasticluster|192.168.0.171|6817|7936|1||||||||normal||", "stdout_lines": ["elasticluster|192.168.0.171|6817|7936|1||||||||normal||"], "warnings": []}

TASK [slurm-master : Create an account for default cluster] ********************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-master/tasks/main.yml:20
changed: [master001] => {"changed": true, "cmd": "sacctmgr --immediate --parsable --noheader list account Cluster=elasticluster | grep '^root|' || sacctmgr -i --quiet add account root Cluster=elasticluster", "delta": "0:00:00.167897", "end": "2018-05-03 20:45:48.526665", "rc": 0, "start": "2018-05-03 20:45:48.358768", "stderr": "", "stdout": "root|default root account|root|", "stdout_lines": ["root|default root account|root|"], "warnings": []}

TASK [slurm-master : Add default user to cluster] ******************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-master/tasks/main.yml:27
changed: [master001] => (item=root) => {"changed": true, "cmd": "sacctmgr --immediate --parsable --noheader list user Account=root | grep '^root|' || sacctmgr --immediate --quiet add user 'root' DefaultAccount=root", "delta": "0:00:00.168469", "end": "2018-05-03 20:45:48.854594", "item": "root", "rc": 0, "start": "2018-05-03 20:45:48.686125", "stderr": "", "stdout": "root|root|Administrator|", "stdout_lines": ["root|root|Administrator|"], "warnings": []}

TASK [slurm-master : Ensure `slurmctld` is running] ****************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-master/tasks/main.yml:35
changed: [master001] => (item=slurm) => {"changed": true, "enabled": true, "item": "slurm", "name": "slurm", "state": "started"}

RUNNING HANDLER [slurm-master : restart slurmdbd] ******************************
changed: [master001] => {"changed": true, "name": "slurmdbd", "state": "started"}

PLAY [Slurm worker nodes Playbook] *********************************************

TASK [setup] *******************************************************************
ok: [worker002]
ok: [worker001]

TASK [nis : Load distribution-specific parameters] *****************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/main.yml:3
included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/init-RedHat.yml for worker001, worker002

TASK [nis : Set NIS common playbook params (CentOS/RHEL)] **********************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/init-RedHat.yml:3
ok: [worker001] => {"ansible_facts": {"nis_client_packages": ["ypbind"], "nis_client_services": ["rpcbind", "ypbind"], "nis_common_packages": ["yp-tools"], "nis_master_packages": ["ypserv"], "nis_master_services": ["rpcbind", "yppasswdd", "ypserv"], "nis_securenets_path": "/var/yp/securenets"}, "changed": false}
ok: [worker002] => {"ansible_facts": {"nis_client_packages": ["ypbind"], "nis_client_services": ["rpcbind", "ypbind"], "nis_common_packages": ["yp-tools"], "nis_master_packages": ["ypserv"], "nis_master_services": ["rpcbind", "yppasswdd", "ypserv"], "nis_securenets_path": "/var/yp/securenets"}, "changed": false}

TASK [nis : Pre-load debconf answer to questions (Debian/Ubuntu)] **************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/main.yml:7
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [nis : Deploy additional NIS configuration (Debian/Ubuntu)] ***************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/main.yml:18
skipping: [worker001] => (item=etc/default/nis)  => {"changed": false, "item": "etc/default/nis", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => (item=etc/defaultdomain)  => {"changed": false, "item": "etc/defaultdomain", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item=etc/default/nis)  => {"changed": false, "item": "etc/default/nis", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item=etc/defaultdomain)  => {"changed": false, "item": "etc/defaultdomain", "skip_reason": "Conditional check failed", "skipped": true}

TASK [nis : Set NIS domain (CentOS/RHEL)] **************************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/main.yml:29
changed: [worker001] => {"backup": "", "changed": true, "msg": "line added"}
changed: [worker002] => {"backup": "", "changed": true, "msg": "line added"}

TASK [nis : Install NIS common packages] ***************************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/main.yml:39
ok: [worker001] => (item=[u'yp-tools']) => {"changed": false, "item": ["yp-tools"], "msg": "", "rc": 0, "results": ["yp-tools-2.9-12.el6.x86_64 providing yp-tools is already installed"]}
ok: [worker002] => (item=[u'yp-tools']) => {"changed": false, "item": ["yp-tools"], "msg": "", "rc": 0, "results": ["yp-tools-2.9-12.el6.x86_64 providing yp-tools is already installed"]}

TASK [nis : Deploy `ypserv` configuration files] *******************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml:3
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [nis : Deploy `yppasswdd` configuration file (CentOS/RHEL)] ***************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml:12
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [nis : Install NIS master server packages] ********************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml:22
skipping: [worker001] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}

TASK [nis : Ensure `ypserv` starts at boot] ************************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml:33
skipping: [worker001] => (item=rpcbind)  => {"changed": false, "item": "rpcbind", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item=rpcbind)  => {"changed": false, "item": "rpcbind", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => (item=yppasswdd)  => {"changed": false, "item": "yppasswdd", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item=yppasswdd)  => {"changed": false, "item": "yppasswdd", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => (item=ypserv)  => {"changed": false, "item": "ypserv", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item=ypserv)  => {"changed": false, "item": "ypserv", "skip_reason": "Conditional check failed", "skipped": true}

TASK [nis : Update NIS/YP databases (NIS master server)] ***********************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml:41
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [nis : Deploy `ypbind` configuration files] *******************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypbind.yml:3
ok: [worker001] => {"changed": false, "gid": 0, "group": "root", "mode": "0400", "owner": "root", "path": "/etc/yp.conf", "size": 723, "state": "file", "uid": 0}
ok: [worker002] => {"changed": false, "gid": 0, "group": "root", "mode": "0400", "owner": "root", "path": "/etc/yp.conf", "size": 723, "state": "file", "uid": 0}

TASK [nis : Ensure NIS/YP is used as a name service] ***************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypbind.yml:12
ok: [worker001] => (item=passwd) => {"backup": "", "changed": false, "item": "passwd", "msg": ""}
ok: [worker002] => (item=passwd) => {"backup": "", "changed": false, "item": "passwd", "msg": ""}
ok: [worker001] => (item=group) => {"backup": "", "changed": false, "item": "group", "msg": ""}
ok: [worker002] => (item=group) => {"backup": "", "changed": false, "item": "group", "msg": ""}
ok: [worker001] => (item=shadow) => {"backup": "", "changed": false, "item": "shadow", "msg": ""}
ok: [worker002] => (item=shadow) => {"backup": "", "changed": false, "item": "shadow", "msg": ""}

TASK [nis : Replace `compat` in `/etc/nsswitch.conf` with `files nis`] *********
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypbind.yml:26
ok: [worker001] => (item=passwd) => {"changed": false, "item": "passwd", "msg": ""}
ok: [worker002] => (item=passwd) => {"changed": false, "item": "passwd", "msg": ""}
ok: [worker001] => (item=group) => {"changed": false, "item": "group", "msg": ""}
ok: [worker002] => (item=group) => {"changed": false, "item": "group", "msg": ""}
ok: [worker001] => (item=shadow) => {"changed": false, "item": "shadow", "msg": ""}
ok: [worker002] => (item=shadow) => {"changed": false, "item": "shadow", "msg": ""}

TASK [nis : Ensure `ypbind` starts at boot] ************************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypbind.yml:37
ok: [worker001] => (item=rpcbind) => {"changed": false, "enabled": true, "item": "rpcbind", "name": "rpcbind", "state": "started"}
ok: [worker002] => (item=rpcbind) => {"changed": false, "enabled": true, "item": "rpcbind", "name": "rpcbind", "state": "started"}
ok: [worker001] => (item=ypbind) => {"changed": false, "enabled": true, "item": "ypbind", "name": "ypbind", "state": "started"}
ok: [worker002] => (item=ypbind) => {"changed": false, "enabled": true, "item": "ypbind", "name": "ypbind", "state": "started"}

TASK [nfs-client : install NFS client software (Debian/Ubuntu)] ****************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-client/tasks/main.yml:3
skipping: [worker001] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}

TASK [nfs-client : install NFS client software (RHEL-compatible)] **************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-client/tasks/main.yml:15
ok: [worker001] => (item=[u'nfs-utils', u'nfs4-acl-tools']) => {"changed": false, "item": ["nfs-utils", "nfs4-acl-tools"], "msg": "", "rc": 0, "results": ["nfs-utils-1:1.2.3-75.el6_9.x86_64 providing nfs-utils is already installed", "nfs4-acl-tools-0.3.3-8.el6.x86_64 providing nfs4-acl-tools is already installed"]}
ok: [worker002] => (item=[u'nfs-utils', u'nfs4-acl-tools']) => {"changed": false, "item": ["nfs-utils", "nfs4-acl-tools"], "msg": "", "rc": 0, "results": ["nfs-utils-1:1.2.3-75.el6_9.x86_64 providing nfs-utils is already installed", "nfs4-acl-tools-0.3.3-8.el6.x86_64 providing nfs4-acl-tools is already installed"]}

TASK [nfs-client : Ensure `rpcbind` is running (Debian)] ***********************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-client/tasks/main.yml:28
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [nfs-client : Ensure `rpcbind` is running (RHEL-compatible)] **************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-client/tasks/main.yml:35
ok: [worker001] => {"changed": false, "enabled": true, "name": "rpcbind", "state": "started"}
ok: [worker002] => {"changed": false, "enabled": true, "name": "rpcbind", "state": "started"}

TASK [nfs-client : Ensure `portmap` is running (Ubuntu prior to 14.04)] ********
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-client/tasks/main.yml:42
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [nfs-client : Ensure `rpcbind` is running (Ubuntu 14.04 or newer)] ********
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-client/tasks/main.yml:49
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [nfs-client : Mount NFS filesystems] **************************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-client/tasks/main.yml:57
included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-client/tasks/nfsmount.yml for worker001, worker002

TASK [nfs-client : ensure /home directory exists] ******************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-client/tasks/nfsmount.yml:3
ok: [worker001] => {"changed": false, "gid": 0, "group": "root", "mode": "0755", "owner": "root", "path": "/home", "size": 4096, "state": "directory", "uid": 0}
ok: [worker002] => {"changed": false, "gid": 0, "group": "root", "mode": "0755", "owner": "root", "path": "/home", "size": 4096, "state": "directory", "uid": 0}

TASK [nfs-client : add to /etc/fstab] ******************************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-client/tasks/nfsmount.yml:8
changed: [worker001] => {"changed": true, "dump": "0", "fstab": "/etc/fstab", "fstype": "nfs", "name": "/home", "opts": "rw,async", "passno": "0", "src": "master001:/home"}
changed: [worker002] => {"changed": true, "dump": "0", "fstab": "/etc/fstab", "fstype": "nfs", "name": "/home", "opts": "rw,async", "passno": "0", "src": "master001:/home"}

TASK [autofs : Load distribution-specific parameters] **************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/autofs/tasks/main.yml:8
included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/autofs/tasks/init-RedHat.yml for worker001, worker002

TASK [autofs : Provide RHEL-specific values] ***********************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/autofs/tasks/init-RedHat.yml:6
ok: [worker001] => {"ansible_facts": {"autofs_packages": ["autofs"], "autofs_service": "autofs", "autofs_username": "root"}, "changed": false}
ok: [worker002] => {"ansible_facts": {"autofs_packages": ["autofs"], "autofs_service": "autofs", "autofs_username": "root"}, "changed": false}

TASK [autofs : Deploy autofs configuration] ************************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/autofs/tasks/main.yml:12
ok: [worker002] => (item=etc/auto.master) => {"changed": false, "gid": 0, "group": "root", "item": "etc/auto.master", "mode": "0444", "owner": "root", "path": "/etc/auto.master", "size": 540, "state": "file", "uid": 0}
ok: [worker001] => (item=etc/auto.master) => {"changed": false, "gid": 0, "group": "root", "item": "etc/auto.master", "mode": "0444", "owner": "root", "path": "/etc/auto.master", "size": 540, "state": "file", "uid": 0}
ok: [worker002] => (item=etc/auto.home) => {"changed": false, "gid": 0, "group": "root", "item": "etc/auto.home", "mode": "0444", "owner": "root", "path": "/etc/auto.home", "size": 852, "state": "file", "uid": 0}
ok: [worker001] => (item=etc/auto.home) => {"changed": false, "gid": 0, "group": "root", "item": "etc/auto.home", "mode": "0444", "owner": "root", "path": "/etc/auto.home", "size": 852, "state": "file", "uid": 0}

TASK [autofs : Deploy autofs mount script for NFSv4 (Debian/Ubuntu)] ***********
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/autofs/tasks/main.yml:24
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [autofs : Install Autofs] *************************************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/autofs/tasks/main.yml:34
ok: [worker001] => (item=[u'autofs']) => {"changed": false, "item": ["autofs"], "msg": "", "rc": 0, "results": ["autofs-1:5.0.5-133.el6_9.x86_64 providing autofs is already installed"]}
ok: [worker002] => (item=[u'autofs']) => {"changed": false, "item": ["autofs"], "msg": "", "rc": 0, "results": ["autofs-1:5.0.5-133.el6_9.x86_64 providing autofs is already installed"]}

TASK [autofs : Ensure autofs is running and starts at boot] ********************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/autofs/tasks/main.yml:41
ok: [worker001] => {"changed": false, "enabled": true, "name": "autofs", "state": "started"}
ok: [worker002] => {"changed": false, "enabled": true, "name": "autofs", "state": "started"}

TASK [slurm-common : Load distribution-specific parameters] ********************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:3
included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/init-RedHat.yml for worker001, worker002

TASK [slurm-common : Set SLURM common playbook params (RHEL 7.x compatible)] ***
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/init-RedHat.yml:3
ok: [worker001] => {"ansible_facts": {"slurm_pid_dir": "/var/run"}, "changed": false}
ok: [worker002] => {"ansible_facts": {"slurm_pid_dir": "/var/run"}, "changed": false}

TASK [slurm-common : Create `slurm` system group] ******************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:16
ok: [worker001] => {"changed": false, "gid": 498, "name": "slurm", "state": "present", "system": true}
ok: [worker002] => {"changed": false, "gid": 498, "name": "slurm", "state": "present", "system": true}

TASK [slurm-common : Create `slurm` system user] *******************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:24
ok: [worker001] => {"append": false, "changed": false, "comment": "", "group": 498, "home": "/home/slurm", "move_home": false, "name": "slurm", "shell": "/bin/bash", "state": "present", "uid": 498}
ok: [worker002] => {"append": false, "changed": false, "comment": "", "group": 498, "home": "/home/slurm", "move_home": false, "name": "slurm", "shell": "/bin/bash", "state": "present", "uid": 498}

TASK [slurm-common : Create work directory {{item}}] ***************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:33
changed: [worker001] => (item=/etc/slurm) => {"changed": true, "gid": 498, "group": "slurm", "item": "/etc/slurm", "mode": "0755", "owner": "slurm", "path": "/etc/slurm", "size": 4096, "state": "directory", "uid": 498}
changed: [worker002] => (item=/etc/slurm) => {"changed": true, "gid": 498, "group": "slurm", "item": "/etc/slurm", "mode": "0755", "owner": "slurm", "path": "/etc/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [worker001] => (item=/var/lib/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/lib/slurm", "mode": "0755", "owner": "slurm", "path": "/var/lib/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [worker002] => (item=/var/lib/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/lib/slurm", "mode": "0755", "owner": "slurm", "path": "/var/lib/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [worker001] => (item=/var/lib/slurm/checkpoint) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/lib/slurm/checkpoint", "mode": "0755", "owner": "slurm", "path": "/var/lib/slurm/checkpoint", "size": 4096, "state": "directory", "uid": 498}
ok: [worker002] => (item=/var/lib/slurm/checkpoint) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/lib/slurm/checkpoint", "mode": "0755", "owner": "slurm", "path": "/var/lib/slurm/checkpoint", "size": 4096, "state": "directory", "uid": 498}
ok: [worker001] => (item=/var/lib/slurm/slurmctld) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/lib/slurm/slurmctld", "mode": "0755", "owner": "slurm", "path": "/var/lib/slurm/slurmctld", "size": 4096, "state": "directory", "uid": 498}
ok: [worker002] => (item=/var/lib/slurm/slurmctld) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/lib/slurm/slurmctld", "mode": "0755", "owner": "slurm", "path": "/var/lib/slurm/slurmctld", "size": 4096, "state": "directory", "uid": 498}
ok: [worker001] => (item=/var/lib/slurm/slurmd) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/lib/slurm/slurmd", "mode": "0755", "owner": "slurm", "path": "/var/lib/slurm/slurmd", "size": 4096, "state": "directory", "uid": 498}
ok: [worker002] => (item=/var/lib/slurm/slurmd) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/lib/slurm/slurmd", "mode": "0755", "owner": "slurm", "path": "/var/lib/slurm/slurmd", "size": 4096, "state": "directory", "uid": 498}
ok: [worker001] => (item=/var/log/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/log/slurm", "mode": "0755", "owner": "slurm", "path": "/var/log/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [worker002] => (item=/var/log/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/log/slurm", "mode": "0755", "owner": "slurm", "path": "/var/log/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [worker001] => (item=/var/run/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/run/slurm", "mode": "0755", "owner": "slurm", "path": "/var/run/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [worker002] => (item=/var/run/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/run/slurm", "mode": "0755", "owner": "slurm", "path": "/var/run/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [worker001] => (item=/var/spool/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/spool/slurm", "mode": "0755", "owner": "slurm", "path": "/var/spool/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [worker002] => (item=/var/spool/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/spool/slurm", "mode": "0755", "owner": "slurm", "path": "/var/spool/slurm", "size": 4096, "state": "directory", "uid": 498}

TASK [slurm-common : Make compatibility symlinks] ******************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:56
skipping: [worker001] => (item={u'to': u'/etc/slurm-llnl', u'from': u'/etc/slurm'})  => {"changed": false, "item": {"from": "/etc/slurm", "to": "/etc/slurm-llnl"}, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => (item={u'to': u'/var/lib/slurm-llnl', u'from': u'/var/lib/slurm'})  => {"changed": false, "item": {"from": "/var/lib/slurm", "to": "/var/lib/slurm-llnl"}, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => (item={u'to': u'/var/log/slurm-llnl', u'from': u'/var/log/slurm'})  => {"changed": false, "item": {"from": "/var/log/slurm", "to": "/var/log/slurm-llnl"}, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item={u'to': u'/var/lib/slurm-llnl', u'from': u'/var/lib/slurm'})  => {"changed": false, "item": {"from": "/var/lib/slurm", "to": "/var/lib/slurm-llnl"}, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item={u'to': u'/etc/slurm-llnl', u'from': u'/etc/slurm'})  => {"changed": false, "item": {"from": "/etc/slurm", "to": "/etc/slurm-llnl"}, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => (item={u'to': u'/var/run/slurm-llnl', u'from': u'/var/run/slurm'})  => {"changed": false, "item": {"from": "/var/run/slurm", "to": "/var/run/slurm-llnl"}, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item={u'to': u'/var/log/slurm-llnl', u'from': u'/var/log/slurm'})  => {"changed": false, "item": {"from": "/var/log/slurm", "to": "/var/log/slurm-llnl"}, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item={u'to': u'/var/run/slurm-llnl', u'from': u'/var/run/slurm'})  => {"changed": false, "item": {"from": "/var/run/slurm", "to": "/var/run/slurm-llnl"}, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-common : Enable Copr SLURM repo by verdurin] ***********************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:73
ok: [worker002] => {"changed": false, "gid": 0, "group": "root", "mode": "0444", "owner": "root", "path": "/etc/yum.repos.d/copr-slurm.repo", "size": 615, "state": "file", "uid": 0}
ok: [worker001] => {"changed": false, "gid": 0, "group": "root", "mode": "0444", "owner": "root", "path": "/etc/yum.repos.d/copr-slurm.repo", "size": 615, "state": "file", "uid": 0}

TASK [slurm-common : Prevent MUNGE from starting] ******************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:20
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-common : Install MUNGE (Debian/Ubuntu)] ****************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:31
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-common : Install MUNGE (RHEL-compatible)] **************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:43
ok: [worker001] => {"changed": false, "msg": "", "rc": 0, "results": ["slurm-munge-17.02.7-1.el6.x86_64 providing slurm-munge is already installed"]}
ok: [worker002] => {"changed": false, "msg": "", "rc": 0, "results": ["slurm-munge-17.02.7-1.el6.x86_64 providing slurm-munge is already installed"]}

TASK [slurm-common : Allow MUNGE to start] *************************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:57
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-common : Add `--syslog` option to `munged` startup] ****************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:66
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-common : Reload systemd configuration] *****************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:77
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-common : Add `--syslog` startup options for `munged` (Ubuntu 14.04)] ***
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:86
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-common : Configure MUNGE] ******************************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:101
ok: [worker001] => {"changed": false, "checksum": "222afe887f907039b799b394715664ffa584fec5", "dest": "/etc/munge/munge.key", "gid": 497, "group": "munge", "mode": "0400", "owner": "munge", "path": "/etc/munge/munge.key", "size": 1024, "state": "file", "uid": 497}
ok: [worker002] => {"changed": false, "checksum": "222afe887f907039b799b394715664ffa584fec5", "dest": "/etc/munge/munge.key", "gid": 497, "group": "munge", "mode": "0400", "owner": "munge", "path": "/etc/munge/munge.key", "size": 1024, "state": "file", "uid": 497}

TASK [slurm-common : Ensure the MUNGE service is running] **********************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:114
ok: [worker001] => {"changed": false, "enabled": true, "name": "munge", "state": "started"}
ok: [worker002] => {"changed": false, "enabled": true, "name": "munge", "state": "started"}

TASK [slurm-common : Deploy SLURM configuration file] **************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:86
ok: [worker002] => {"changed": false, "gid": 0, "group": "root", "mode": "0444", "owner": "root", "path": "/etc/slurm/slurm.conf", "size": 4154, "state": "file", "uid": 0}
ok: [worker001] => {"changed": false, "gid": 0, "group": "root", "mode": "0444", "owner": "root", "path": "/etc/slurm/slurm.conf", "size": 4154, "state": "file", "uid": 0}

TASK [slurm-common : Deploy script to suspend cloud instance] ******************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:96
ok: [worker001] => {"changed": false, "gid": 0, "group": "root", "mode": "0555", "owner": "root", "path": "/etc/slurm/slurm.suspend.sh", "size": 602, "state": "file", "uid": 0}
ok: [worker002] => {"changed": false, "gid": 0, "group": "root", "mode": "0555", "owner": "root", "path": "/etc/slurm/slurm.suspend.sh", "size": 602, "state": "file", "uid": 0}

TASK [slurm-common : Deploy script to resume cloud instance] *******************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:106
ok: [worker001] => {"changed": false, "gid": 0, "group": "root", "mode": "0555", "owner": "root", "path": "/etc/slurm/slurm.resume.sh", "size": 629, "state": "file", "uid": 0}
ok: [worker002] => {"changed": false, "gid": 0, "group": "root", "mode": "0555", "owner": "root", "path": "/etc/slurm/slurm.resume.sh", "size": 629, "state": "file", "uid": 0}

TASK [slurm-common : Install support packages (Debian/Ubuntu)] *****************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:116
skipping: [worker001] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-common : Install SLURM packages (RHEL-compatible)] *****************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:127
skipping: [worker001] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-client : Install required SLURM packages (Debian/Ubuntu)] **********
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-client/tasks/main.yml:4
skipping: [worker001] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-client : Install required SLURM packages (older Debian/Ubuntu)] ****
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-client/tasks/main.yml:23
skipping: [worker001] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-client : service] **************************************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-client/tasks/main.yml:35
skipping: [worker001] => (item=slurm-llnl)  => {"changed": false, "item": "slurm-llnl", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item=slurm-llnl)  => {"changed": false, "item": "slurm-llnl", "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-client : Install SLURM packages (RHEL-compatible)] *****************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-client/tasks/main.yml:47
ok: [worker001] => (item=[u'slurm', u'slurm-devel', u'slurm-perlapi', u'slurm-contribs', u'slurm-torque']) => {"changed": false, "item": ["slurm", "slurm-devel", "slurm-perlapi", "slurm-contribs", "slurm-torque"], "msg": "", "rc": 0, "results": ["slurm-17.02.7-1.el6.x86_64 providing slurm is already installed", "slurm-devel-17.02.7-1.el6.x86_64 providing slurm-devel is already installed", "slurm-perlapi-17.02.7-1.el6.x86_64 providing slurm-perlapi is already installed", "slurm-contribs-17.02.7-1.el6.x86_64 providing slurm-contribs is already installed", "slurm-torque-17.02.7-1.el6.x86_64 providing slurm-torque is already installed"]}
ok: [worker002] => (item=[u'slurm', u'slurm-devel', u'slurm-perlapi', u'slurm-contribs', u'slurm-torque']) => {"changed": false, "item": ["slurm", "slurm-devel", "slurm-perlapi", "slurm-contribs", "slurm-torque"], "msg": "", "rc": 0, "results": ["slurm-17.02.7-1.el6.x86_64 providing slurm is already installed", "slurm-devel-17.02.7-1.el6.x86_64 providing slurm-devel is already installed", "slurm-perlapi-17.02.7-1.el6.x86_64 providing slurm-perlapi is already installed", "slurm-contribs-17.02.7-1.el6.x86_64 providing slurm-contribs is already installed", "slurm-torque-17.02.7-1.el6.x86_64 providing slurm-torque is already installed"]}

TASK [slurm-client : Stop SLURM services (RHEL/CentOS 6.x)] ********************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-client/tasks/main.yml:59
changed: [worker001] => {"changed": true, "enabled": false, "name": "slurm", "state": "stopped"}
changed: [worker002] => {"changed": true, "enabled": false, "name": "slurm", "state": "stopped"}

TASK [slurm-client : Stop SLURM services (RHEL/CentOS 7.x)] ********************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-client/tasks/main.yml:66
skipping: [worker001] => (item=slurmd)  => {"changed": false, "item": "slurmd", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => (item=slurmctld)  => {"changed": false, "item": "slurmctld", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item=slurmd)  => {"changed": false, "item": "slurmd", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item=slurmctld)  => {"changed": false, "item": "slurmctld", "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-worker : Load distribution-specific parameters] ********************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/main.yml:4
included: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/init-RedHat.yml for worker001, worker002

TASK [slurm-worker : Set SLURM worker playbook params (RHEL compatible)] *******
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/init-RedHat.yml:3
ok: [worker001] => {"ansible_facts": {"slurmd_packages": ["slurm", "slurm-plugins", "slurm-sql"]}, "changed": false}
ok: [worker002] => {"ansible_facts": {"slurmd_packages": ["slurm", "slurm-plugins", "slurm-sql"]}, "changed": false}

TASK [slurm-worker : Set SLURM worker service name (RHEL 7.x compatible)] ******
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/init-RedHat.yml:11
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-worker : Set SLURM worker service name (RHEL 6.x compatible)] ******
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/init-RedHat.yml:16
ok: [worker001] => {"ansible_facts": {"slurmd_service": "slurm"}, "changed": false}
ok: [worker002] => {"ansible_facts": {"slurmd_service": "slurm"}, "changed": false}

TASK [slurm-worker : Deploy SLURM configuration files] *************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/main.yml:7
ok: [worker001] => (item=gres.conf) => {"changed": false, "gid": 0, "group": "root", "item": "gres.conf", "mode": "0444", "owner": "root", "path": "/etc/slurm/gres.conf", "size": 297, "state": "file", "uid": 0}
ok: [worker002] => (item=gres.conf) => {"changed": false, "gid": 0, "group": "root", "item": "gres.conf", "mode": "0444", "owner": "root", "path": "/etc/slurm/gres.conf", "size": 297, "state": "file", "uid": 0}

TASK [slurm-worker : Deploy kernel config check script] ************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/cgroup.yml:3
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-worker : Check that kernel is configured for cgroups] **************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/cgroup.yml:12
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-worker : Ensure cgroup filesystems are mounted] ********************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/cgroup.yml:18
skipping: [worker001] => (item=blkio)  => {"changed": false, "item": "blkio", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => (item=cpuacct)  => {"changed": false, "item": "cpuacct", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item=blkio)  => {"changed": false, "item": "blkio", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item=cpuacct)  => {"changed": false, "item": "cpuacct", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => (item=cpuset)  => {"changed": false, "item": "cpuset", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item=cpuset)  => {"changed": false, "item": "cpuset", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => (item=devices)  => {"changed": false, "item": "devices", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item=devices)  => {"changed": false, "item": "devices", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => (item=freezer)  => {"changed": false, "item": "freezer", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item=freezer)  => {"changed": false, "item": "freezer", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => (item=memory)  => {"changed": false, "item": "memory", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item=memory)  => {"changed": false, "item": "memory", "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-worker : Ensure kernel is booted with swap accounting enabled] *****
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/cgroup.yml:44
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-worker : Check if swap accouting is enabled (may fail!)] ***********
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/cgroup.yml:53
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-worker : Reboot to enable swap accounting] *************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/cgroup.yml:59
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-worker : Wait for server to come up again] *************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/cgroup.yml:72
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-worker : Ensure release agent directory exists] ********************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/cgroup.yml:84
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-worker : Deploy cgroup-specific release agent script] **************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/cgroup.yml:93
skipping: [worker001] => (item=blkio)  => {"changed": false, "item": "blkio", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => (item=cpuacct)  => {"changed": false, "item": "cpuacct", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item=blkio)  => {"changed": false, "item": "blkio", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item=cpuacct)  => {"changed": false, "item": "cpuacct", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => (item=cpuset)  => {"changed": false, "item": "cpuset", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item=cpuset)  => {"changed": false, "item": "cpuset", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => (item=devices)  => {"changed": false, "item": "devices", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item=devices)  => {"changed": false, "item": "devices", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => (item=freezer)  => {"changed": false, "item": "freezer", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item=freezer)  => {"changed": false, "item": "freezer", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => (item=memory)  => {"changed": false, "item": "memory", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item=memory)  => {"changed": false, "item": "memory", "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-worker : Deploy SLURM cgroup configuration file (main)] ************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/cgroup.yml:103
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-worker : Deploy SLURM cgroup configuration file (devices)] *********
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/cgroup.yml:112
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-worker : Install SLURM worker packages] ****************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/main.yml:27
ok: [worker001] => (item=[u'slurm', u'slurm-plugins', u'slurm-sql']) => {"changed": false, "item": ["slurm", "slurm-plugins", "slurm-sql"], "msg": "", "rc": 0, "results": ["slurm-17.02.7-1.el6.x86_64 providing slurm is already installed", "slurm-plugins-17.02.7-1.el6.x86_64 providing slurm-plugins is already installed", "slurm-sql-17.02.7-1.el6.x86_64 providing slurm-sql is already installed"]}
ok: [worker002] => (item=[u'slurm', u'slurm-plugins', u'slurm-sql']) => {"changed": false, "item": ["slurm", "slurm-plugins", "slurm-sql"], "msg": "", "rc": 0, "results": ["slurm-17.02.7-1.el6.x86_64 providing slurm is already installed", "slurm-plugins-17.02.7-1.el6.x86_64 providing slurm-plugins is already installed", "slurm-sql-17.02.7-1.el6.x86_64 providing slurm-sql is already installed"]}

TASK [slurm-worker : Ensure SLURMd starts at boot] *****************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/main.yml:33
changed: [worker001] => {"changed": true, "enabled": true, "name": "slurm"}
changed: [worker002] => {"changed": true, "enabled": true, "name": "slurm"}

PLAY [Restart SLURMd after all config is done] *********************************

TASK [setup] *******************************************************************
ok: [worker001]
ok: [worker002]

TASK [service] *****************************************************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm.yml:47
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [service] *****************************************************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm.yml:51
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [service] *****************************************************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm.yml:55
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [service] *****************************************************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm.yml:59
changed: [worker001] => {"changed": true, "name": "slurm", "state": "started"}
changed: [worker002] => {"changed": true, "name": "slurm", "state": "started"}

PLAY [Apply local customizations (after)] **************************************

TASK [setup] *******************************************************************
ok: [worker001]
ok: [worker002]
ok: [master001]

PLAY [Report success on cluster creation] **************************************

TASK [Mark host as successfully configured] ************************************
task path: /root/.cache/Python-Eggs/hwcc-0.1.0-py2.7.egg-tmp/elasticluster/share/playbooks/site.yml:78
changed: [master001 -> localhost] => {"backup": "", "changed": true, "msg": "line added"}
changed: [worker001 -> localhost] => {"backup": "", "changed": true, "msg": "line added"}
changed: [worker002 -> localhost] => {"backup": "", "changed": true, "msg": "line added"}

PLAY RECAP *********************************************************************
master001                  : ok=124  changed=45   unreachable=0    failed=0   
worker001                  : ok=116  changed=36   unreachable=0    failed=0   
worker002                  : ok=116  changed=36   unreachable=0    failed=0   

2018-05-03 20:46:09 hwc hwcc[4151] INFO Cluster correctly configured.
Starting cluster `slurm` with:
* 2 worker nodes.
* 1 master nodes.
(This may take a while...)
Configuring the cluster ...
(this too may take a while)

Your cluster `slurm` is ready!

Cluster name:     slurm
Cluster template: slurm
Default ssh to node: master001
- worker nodes: 2
- master nodes: 1

To login on the frontend node, run the command:

    elasticluster ssh slurm

To upload or download files to the cluster, use the command:

    elasticluster sftp slurm


2018-04-28 18:31:08 hwc hwc[5245] DEBUG Checking section `cluster/slurm` ...
2018-04-28 18:31:08 hwc hwc[5245] DEBUG Checking section `login/centos` ...
2018-04-28 18:31:08 hwc hwc[5245] DEBUG Checking section `setup/ansible-slurm` ...
2018-04-28 18:31:08 hwc hwc[5245] DEBUG Checking section `cloud/catalyst` ...
2018-04-28 18:31:09 hwc hwc[5245] DEBUG Using class <class 'elasticluster.providers.openstack.OpenStackCloudProvider'> from module <module 'elasticluster.providers.openstack' from '/usr/local/lib/python2.7/site-packages/elasticluster-1.3.dev1-py2.7.egg/elasticluster/providers/openstack.pyc'> to instanciate provider 'openstack'
2018-04-28 18:31:09 hwc hwc[5245] DEBUG OpenStack auth URL taken from env variable OS_AUTH_URL
2018-04-28 18:31:09 hwc hwc[5245] DEBUG OpenStack user name taken from env variable OS_USERNAME
2018-04-28 18:31:09 hwc hwc[5245] DEBUG OpenStack user domain name taken from env variable OS_USER_DOMAIN_NAME
2018-04-28 18:31:09 hwc hwc[5245] DEBUG OpenStack password taken from env variable OS_PASSWORD
2018-04-28 18:31:09 hwc hwc[5245] DEBUG OpenStack project name taken from env variable OS_PROJECT_NAME
2018-04-28 18:31:09 hwc hwc[5245] DEBUG OpenStack project domain name taken from env variable OS_PROJECT_DOMAIN_NAME
2018-04-28 18:31:09 hwc hwc[5245] DEBUG Using class <class 'elasticluster.providers.ansible_provider.AnsibleSetupProvider'> from module <module 'elasticluster.providers.ansible_provider' from '/usr/local/lib/python2.7/site-packages/elasticluster-1.3.dev1-py2.7.egg/elasticluster/providers/ansible_provider.pyc'> to instanciate provider 'ansible'
2018-04-28 18:31:09 hwc hwc[5245] DEBUG setting variable user_obs_url=http://obs.cn-north-1.myhwclouds.com for node kind compute
2018-04-28 18:31:09 hwc hwc[5245] DEBUG setting variable upgrade_packages=no for node kind compute
2018-04-28 18:31:09 hwc hwc[5245] DEBUG setting variable user_obs_endpoint=cn-north-1 for node kind compute
2018-04-28 18:31:09 hwc hwc[5245] DEBUG setting variable user_client_home=/root/hwc for node kind compute
2018-04-28 18:31:09 hwc hwc[5245] DEBUG setting variable user_obs_name=hwc-obs for node kind compute
2018-04-28 18:31:09 hwc hwc[5245] DEBUG setting variable user_client_ip=192.168.0.230 for node kind compute
2018-04-28 18:31:09 hwc hwc[5245] DEBUG setting variable user_sfs_url=sfs-nas1.cn-north-1.myhuaweicloud.com:/share-480fca1b for node kind compute
2018-04-28 18:31:09 hwc hwc[5245] DEBUG setting variable user_ak=CNYVGDKK2PPBQEEMUYQG for node kind compute
2018-04-28 18:31:09 hwc hwc[5245] DEBUG setting variable user_sk=6imAmmCVVzMqR1dm5x0AKJcL1BILA7TlOsSMhu4G for node kind compute
2018-04-28 18:31:09 hwc hwc[5245] DEBUG setting variable user_obs_url=http://obs.cn-north-1.myhwclouds.com for node kind frontend
2018-04-28 18:31:09 hwc hwc[5245] DEBUG setting variable upgrade_packages=no for node kind frontend
2018-04-28 18:31:09 hwc hwc[5245] DEBUG setting variable user_obs_endpoint=cn-north-1 for node kind frontend
2018-04-28 18:31:09 hwc hwc[5245] DEBUG setting variable user_client_home=/root/hwc for node kind frontend
2018-04-28 18:31:09 hwc hwc[5245] DEBUG setting variable user_obs_name=hwc-obs for node kind frontend
2018-04-28 18:31:09 hwc hwc[5245] DEBUG setting variable user_client_ip=192.168.0.230 for node kind frontend
2018-04-28 18:31:09 hwc hwc[5245] DEBUG setting variable user_sfs_url=sfs-nas1.cn-north-1.myhuaweicloud.com:/share-480fca1b for node kind frontend
2018-04-28 18:31:09 hwc hwc[5245] DEBUG setting variable user_ak=CNYVGDKK2PPBQEEMUYQG for node kind frontend
2018-04-28 18:31:09 hwc hwc[5245] DEBUG setting variable user_sk=6imAmmCVVzMqR1dm5x0AKJcL1BILA7TlOsSMhu4G for node kind frontend
2018-04-28 18:31:09 hwc hwc[5245] INFO Starting cluster nodes (timeout: 600 seconds) ...
2018-04-28 18:31:09 hwc hwc[5245] DEBUG Note: starting 3 nodes concurrently.
2018-04-28 18:31:09 hwc hwc[5245] DEBUG _start_node: working on node `frontend001`
2018-04-28 18:31:09 hwc hwc[5245] INFO Starting node `frontend001` from image `befbd7ca-dd7a-4737-bc60-33a1cdf6ea8b` with flavor c1.medium ...
2018-04-28 18:31:09 hwc hwc[5245] DEBUG Initializing OpenStack API clients: OS_AUTH_URL='https://iam.cn-north-1.myhwclouds.com/v3' OS_USERNAME='mschyj' OS_USER_DOMAIN_NAME='mschyj' OS_PROJECT_NAME='cn-north-1' OS_PROJECT_DOMAIN_NAME='mschyj' OS_REGION_NAME='cn-north-1'
2018-04-28 18:31:09 hwc hwc[5245] INFO Using Keystone API v3 session to authenticate to OpenStack
2018-04-28 18:31:09 hwc hwc[5245] DEBUG Creating OpenStack Compute API (Nova) v2 client ...
2018-04-28 18:31:09 hwc hwc[5245] DEBUG _start_node: working on node `compute001`
2018-04-28 18:31:09 hwc hwc[5245] INFO Starting node `compute001` from image `befbd7ca-dd7a-4737-bc60-33a1cdf6ea8b` with flavor c1.medium ...
2018-04-28 18:31:09 hwc hwc[5245] DEBUG Initializing OpenStack API clients: OS_AUTH_URL='https://iam.cn-north-1.myhwclouds.com/v3' OS_USERNAME='mschyj' OS_USER_DOMAIN_NAME='mschyj' OS_PROJECT_NAME='cn-north-1' OS_PROJECT_DOMAIN_NAME='mschyj' OS_REGION_NAME='cn-north-1'
2018-04-28 18:31:09 hwc hwc[5245] DEBUG _start_node: working on node `compute002`
2018-04-28 18:31:09 hwc hwc[5245] INFO Starting node `compute002` from image `befbd7ca-dd7a-4737-bc60-33a1cdf6ea8b` with flavor c1.medium ...
2018-04-28 18:31:09 hwc hwc[5245] DEBUG Initializing OpenStack API clients: OS_AUTH_URL='https://iam.cn-north-1.myhwclouds.com/v3' OS_USERNAME='mschyj' OS_USER_DOMAIN_NAME='mschyj' OS_PROJECT_NAME='cn-north-1' OS_PROJECT_DOMAIN_NAME='mschyj' OS_REGION_NAME='cn-north-1'
2018-04-28 18:31:09 hwc hwc[5245] INFO Using Keystone API v3 session to authenticate to OpenStack
2018-04-28 18:31:09 hwc hwc[5245] DEBUG Creating OpenStack Compute API (Nova) v2 client ...
2018-04-28 18:31:09 hwc hwc[5245] DEBUG Creating OpenStack Network API (Neutron) client ...
2018-04-28 18:31:09 hwc hwc[5245] INFO Using Keystone API v3 session to authenticate to OpenStack
2018-04-28 18:31:09 hwc hwc[5245] DEBUG Creating OpenStack Compute API (Nova) v2 client ...
2018-04-28 18:31:09 hwc hwc[5245] DEBUG Creating OpenStack Network API (Neutron) client ...
2018-04-28 18:31:09 hwc hwc[5245] DEBUG Creating OpenStack Network API (Neutron) client ...
2018-04-28 18:31:09 hwc hwc[5245] DEBUG Creating OpenStack Image API (Glance) v2 client ...
2018-04-28 18:31:09 hwc hwc[5245] DEBUG Creating OpenStack Image API (Glance) v2 client ...
2018-04-28 18:31:09 hwc hwc[5245] DEBUG Creating OpenStack Image API (Glance) v2 client ...
2018-04-28 18:31:09 hwc hwc[5245] DEBUG Creating OpenStack Volume API (Cinder) v2 client ...
2018-04-28 18:31:09 hwc hwc[5245] DEBUG Creating OpenStack Volume API (Cinder) v2 client ...
2018-04-28 18:31:09 hwc hwc[5245] DEBUG Creating OpenStack Volume API (Cinder) v2 client ...
2018-04-28 18:31:09 hwc hwc[5245] DEBUG Checking keypair `hwc-key` ...
2018-04-28 18:31:09 hwc hwc[5245] DEBUG Checking keypair `hwc-key` ...
2018-04-28 18:31:09 hwc hwc[5245] DEBUG Checking keypair `hwc-key` ...
2018-04-28 18:31:09 hwc hwc[5245] DEBUG Checking existence of security group(s) ['security-mschyj'] ...
2018-04-28 18:31:09 hwc hwc[5245] DEBUG Checking existence of security group(s) ['security-mschyj'] ...
2018-04-28 18:31:09 hwc hwc[5245] DEBUG Checking existence of security group(s) ['security-mschyj'] ...
2018-04-28 18:31:20 hwc hwc[5245] DEBUG Specifying networks for node slurm-compute001: 6c7d662f-eb15-4a44-b2ef-2e71ff7fce15
2018-04-28 18:31:20 hwc hwc[5245] DEBUG Specifying networks for node slurm-frontend001: 6c7d662f-eb15-4a44-b2ef-2e71ff7fce15
2018-04-28 18:31:21 hwc hwc[5245] DEBUG Specifying networks for node slurm-compute002: 6c7d662f-eb15-4a44-b2ef-2e71ff7fce15
2018-04-28 18:31:25 hwc hwc[5245] DEBUG Node `frontend001` has instance ID `3a55ed3c-0fdf-40d7-8fae-19036a789c72`
2018-04-28 18:31:25 hwc hwc[5245] INFO Node `frontend001` has been started.
2018-04-28 18:31:25 hwc hwc[5245] DEBUG Node `compute001` has instance ID `65dacfda-cbb8-44c0-a0b0-154b2c0bd7b9`
2018-04-28 18:31:25 hwc hwc[5245] INFO Node `compute001` has been started.
2018-04-28 18:31:26 hwc hwc[5245] DEBUG Node `compute002` has instance ID `5293a27c-3a9a-4d00-b170-e43c14fad521`
2018-04-28 18:31:26 hwc hwc[5245] INFO Node `compute002` has been started.
2018-04-28 18:31:26 hwc hwc[5245] DEBUG Getting information for instance 3a55ed3c-0fdf-40d7-8fae-19036a789c72
2018-04-28 18:31:27 hwc hwc[5245] DEBUG node `frontend001` (instance id `3a55ed3c-0fdf-40d7-8fae-19036a789c72`) still building...
2018-04-28 18:31:27 hwc hwc[5245] DEBUG Getting information for instance 5293a27c-3a9a-4d00-b170-e43c14fad521
2018-04-28 18:31:27 hwc hwc[5245] DEBUG node `compute002` (instance id `5293a27c-3a9a-4d00-b170-e43c14fad521`) still building...
2018-04-28 18:31:27 hwc hwc[5245] DEBUG Getting information for instance 65dacfda-cbb8-44c0-a0b0-154b2c0bd7b9
2018-04-28 18:31:28 hwc hwc[5245] DEBUG node `compute001` (instance id `65dacfda-cbb8-44c0-a0b0-154b2c0bd7b9`) still building...
2018-04-28 18:31:28 hwc hwc[5245] DEBUG Waiting for 3 more nodes to come up ...
2018-04-28 18:31:38 hwc hwc[5245] DEBUG Getting information for instance 3a55ed3c-0fdf-40d7-8fae-19036a789c72
2018-04-28 18:31:39 hwc hwc[5245] DEBUG node `frontend001` (instance id `3a55ed3c-0fdf-40d7-8fae-19036a789c72`) still building...
2018-04-28 18:31:39 hwc hwc[5245] DEBUG Getting information for instance 5293a27c-3a9a-4d00-b170-e43c14fad521
2018-04-28 18:31:40 hwc hwc[5245] DEBUG node `compute002` (instance id `5293a27c-3a9a-4d00-b170-e43c14fad521`) still building...
2018-04-28 18:31:40 hwc hwc[5245] DEBUG Getting information for instance 65dacfda-cbb8-44c0-a0b0-154b2c0bd7b9
2018-04-28 18:31:40 hwc hwc[5245] DEBUG node `compute001` (instance id `65dacfda-cbb8-44c0-a0b0-154b2c0bd7b9`) still building...
2018-04-28 18:31:40 hwc hwc[5245] DEBUG Waiting for 3 more nodes to come up ...
2018-04-28 18:31:50 hwc hwc[5245] DEBUG Getting information for instance 3a55ed3c-0fdf-40d7-8fae-19036a789c72
2018-04-28 18:31:51 hwc hwc[5245] DEBUG node `frontend001` (instance id `3a55ed3c-0fdf-40d7-8fae-19036a789c72`) still building...
2018-04-28 18:31:51 hwc hwc[5245] DEBUG Getting information for instance 5293a27c-3a9a-4d00-b170-e43c14fad521
2018-04-28 18:31:51 hwc hwc[5245] DEBUG node `compute002` (instance id `5293a27c-3a9a-4d00-b170-e43c14fad521`) still building...
2018-04-28 18:31:51 hwc hwc[5245] DEBUG Getting information for instance 65dacfda-cbb8-44c0-a0b0-154b2c0bd7b9
2018-04-28 18:31:51 hwc hwc[5245] DEBUG node `compute001` (instance id `65dacfda-cbb8-44c0-a0b0-154b2c0bd7b9`) still building...
2018-04-28 18:31:51 hwc hwc[5245] DEBUG Waiting for 3 more nodes to come up ...
2018-04-28 18:32:01 hwc hwc[5245] DEBUG Getting information for instance 3a55ed3c-0fdf-40d7-8fae-19036a789c72
2018-04-28 18:32:02 hwc hwc[5245] DEBUG node `frontend001` (instance id `3a55ed3c-0fdf-40d7-8fae-19036a789c72`) still building...
2018-04-28 18:32:02 hwc hwc[5245] DEBUG Getting information for instance 5293a27c-3a9a-4d00-b170-e43c14fad521
2018-04-28 18:32:03 hwc hwc[5245] DEBUG node `compute002` (instance id `5293a27c-3a9a-4d00-b170-e43c14fad521`) still building...
2018-04-28 18:32:03 hwc hwc[5245] DEBUG Getting information for instance 65dacfda-cbb8-44c0-a0b0-154b2c0bd7b9
2018-04-28 18:32:04 hwc hwc[5245] DEBUG node `compute001` (instance id `65dacfda-cbb8-44c0-a0b0-154b2c0bd7b9`) still building...
2018-04-28 18:32:04 hwc hwc[5245] DEBUG Waiting for 3 more nodes to come up ...
2018-04-28 18:32:14 hwc hwc[5245] DEBUG Getting information for instance 3a55ed3c-0fdf-40d7-8fae-19036a789c72
2018-04-28 18:32:14 hwc hwc[5245] DEBUG node `frontend001` (instance id `3a55ed3c-0fdf-40d7-8fae-19036a789c72`) still building...
2018-04-28 18:32:14 hwc hwc[5245] DEBUG Getting information for instance 5293a27c-3a9a-4d00-b170-e43c14fad521
2018-04-28 18:32:15 hwc hwc[5245] DEBUG node `compute002` (instance id `5293a27c-3a9a-4d00-b170-e43c14fad521`) still building...
2018-04-28 18:32:15 hwc hwc[5245] DEBUG Getting information for instance 65dacfda-cbb8-44c0-a0b0-154b2c0bd7b9
2018-04-28 18:32:15 hwc hwc[5245] DEBUG node `compute001` (instance id `65dacfda-cbb8-44c0-a0b0-154b2c0bd7b9`) still building...
2018-04-28 18:32:15 hwc hwc[5245] DEBUG Waiting for 3 more nodes to come up ...
2018-04-28 18:32:25 hwc hwc[5245] DEBUG Getting information for instance 3a55ed3c-0fdf-40d7-8fae-19036a789c72
2018-04-28 18:32:25 hwc hwc[5245] DEBUG node `frontend001` (instance id `3a55ed3c-0fdf-40d7-8fae-19036a789c72`) still building...
2018-04-28 18:32:25 hwc hwc[5245] DEBUG Getting information for instance 5293a27c-3a9a-4d00-b170-e43c14fad521
2018-04-28 18:32:26 hwc hwc[5245] DEBUG node `compute002` (instance id `5293a27c-3a9a-4d00-b170-e43c14fad521`) still building...
2018-04-28 18:32:26 hwc hwc[5245] DEBUG Getting information for instance 65dacfda-cbb8-44c0-a0b0-154b2c0bd7b9
2018-04-28 18:32:27 hwc hwc[5245] DEBUG node `compute001` (instance id `65dacfda-cbb8-44c0-a0b0-154b2c0bd7b9`) still building...
2018-04-28 18:32:27 hwc hwc[5245] DEBUG Waiting for 3 more nodes to come up ...
2018-04-28 18:32:37 hwc hwc[5245] DEBUG Getting information for instance 3a55ed3c-0fdf-40d7-8fae-19036a789c72
2018-04-28 18:32:37 hwc hwc[5245] DEBUG node `frontend001` (instance id `3a55ed3c-0fdf-40d7-8fae-19036a789c72`) still building...
2018-04-28 18:32:37 hwc hwc[5245] DEBUG Getting information for instance 5293a27c-3a9a-4d00-b170-e43c14fad521
2018-04-28 18:32:37 hwc hwc[5245] DEBUG node `compute002` (instance id `5293a27c-3a9a-4d00-b170-e43c14fad521`) still building...
2018-04-28 18:32:37 hwc hwc[5245] DEBUG Getting information for instance 65dacfda-cbb8-44c0-a0b0-154b2c0bd7b9
2018-04-28 18:32:38 hwc hwc[5245] DEBUG node `compute001` (instance id `65dacfda-cbb8-44c0-a0b0-154b2c0bd7b9`) still building...
2018-04-28 18:32:38 hwc hwc[5245] DEBUG Waiting for 3 more nodes to come up ...
2018-04-28 18:32:48 hwc hwc[5245] DEBUG Getting information for instance 3a55ed3c-0fdf-40d7-8fae-19036a789c72
2018-04-28 18:32:48 hwc hwc[5245] DEBUG node `frontend001` (instance id `3a55ed3c-0fdf-40d7-8fae-19036a789c72`) still building...
2018-04-28 18:32:48 hwc hwc[5245] DEBUG Getting information for instance 5293a27c-3a9a-4d00-b170-e43c14fad521
2018-04-28 18:32:49 hwc hwc[5245] DEBUG node `compute002` (instance id `5293a27c-3a9a-4d00-b170-e43c14fad521`) still building...
2018-04-28 18:32:49 hwc hwc[5245] DEBUG Getting information for instance 65dacfda-cbb8-44c0-a0b0-154b2c0bd7b9
2018-04-28 18:32:49 hwc hwc[5245] DEBUG node `compute001` (instance id `65dacfda-cbb8-44c0-a0b0-154b2c0bd7b9`) still building...
2018-04-28 18:32:49 hwc hwc[5245] DEBUG Waiting for 3 more nodes to come up ...
2018-04-28 18:32:59 hwc hwc[5245] DEBUG Getting information for instance 3a55ed3c-0fdf-40d7-8fae-19036a789c72
2018-04-28 18:32:59 hwc hwc[5245] DEBUG node `frontend001` (instance id `3a55ed3c-0fdf-40d7-8fae-19036a789c72`) still building...
2018-04-28 18:32:59 hwc hwc[5245] DEBUG Getting information for instance 5293a27c-3a9a-4d00-b170-e43c14fad521
2018-04-28 18:33:00 hwc hwc[5245] DEBUG node `compute002` (instance id `5293a27c-3a9a-4d00-b170-e43c14fad521`) still building...
2018-04-28 18:33:00 hwc hwc[5245] DEBUG Getting information for instance 65dacfda-cbb8-44c0-a0b0-154b2c0bd7b9
2018-04-28 18:33:00 hwc hwc[5245] DEBUG node `compute001` (instance id `65dacfda-cbb8-44c0-a0b0-154b2c0bd7b9`) still building...
2018-04-28 18:33:00 hwc hwc[5245] DEBUG Waiting for 3 more nodes to come up ...
2018-04-28 18:33:10 hwc hwc[5245] DEBUG Getting information for instance 3a55ed3c-0fdf-40d7-8fae-19036a789c72
2018-04-28 18:33:11 hwc hwc[5245] DEBUG node `frontend001` (instance id `3a55ed3c-0fdf-40d7-8fae-19036a789c72`) still building...
2018-04-28 18:33:11 hwc hwc[5245] DEBUG Getting information for instance 5293a27c-3a9a-4d00-b170-e43c14fad521
2018-04-28 18:33:11 hwc hwc[5245] DEBUG node `compute002` (instance id `5293a27c-3a9a-4d00-b170-e43c14fad521`) still building...
2018-04-28 18:33:11 hwc hwc[5245] DEBUG Getting information for instance 65dacfda-cbb8-44c0-a0b0-154b2c0bd7b9
2018-04-28 18:33:11 hwc hwc[5245] DEBUG node `compute001` (instance id `65dacfda-cbb8-44c0-a0b0-154b2c0bd7b9`) still building...
2018-04-28 18:33:11 hwc hwc[5245] DEBUG Waiting for 3 more nodes to come up ...
2018-04-28 18:33:21 hwc hwc[5245] DEBUG Getting information for instance 3a55ed3c-0fdf-40d7-8fae-19036a789c72
2018-04-28 18:33:22 hwc hwc[5245] DEBUG node `frontend001` (instance id `3a55ed3c-0fdf-40d7-8fae-19036a789c72`) still building...
2018-04-28 18:33:22 hwc hwc[5245] DEBUG Getting information for instance 5293a27c-3a9a-4d00-b170-e43c14fad521
2018-04-28 18:33:23 hwc hwc[5245] DEBUG node `compute002` (instance id `5293a27c-3a9a-4d00-b170-e43c14fad521`) still building...
2018-04-28 18:33:23 hwc hwc[5245] DEBUG Getting information for instance 65dacfda-cbb8-44c0-a0b0-154b2c0bd7b9
2018-04-28 18:33:23 hwc hwc[5245] DEBUG node `compute001` (instance id `65dacfda-cbb8-44c0-a0b0-154b2c0bd7b9`) still building...
2018-04-28 18:33:23 hwc hwc[5245] DEBUG Waiting for 3 more nodes to come up ...
2018-04-28 18:33:33 hwc hwc[5245] DEBUG Getting information for instance 3a55ed3c-0fdf-40d7-8fae-19036a789c72
2018-04-28 18:33:34 hwc hwc[5245] DEBUG node `frontend001` (instance id `3a55ed3c-0fdf-40d7-8fae-19036a789c72`) still building...
2018-04-28 18:33:34 hwc hwc[5245] DEBUG Getting information for instance 5293a27c-3a9a-4d00-b170-e43c14fad521
2018-04-28 18:33:34 hwc hwc[5245] DEBUG node `compute002` (instance id `5293a27c-3a9a-4d00-b170-e43c14fad521`) still building...
2018-04-28 18:33:34 hwc hwc[5245] DEBUG Getting information for instance 65dacfda-cbb8-44c0-a0b0-154b2c0bd7b9
2018-04-28 18:33:35 hwc hwc[5245] DEBUG node `compute001` (instance id `65dacfda-cbb8-44c0-a0b0-154b2c0bd7b9`) still building...
2018-04-28 18:33:35 hwc hwc[5245] DEBUG Waiting for 3 more nodes to come up ...
2018-04-28 18:33:45 hwc hwc[5245] DEBUG Getting information for instance 3a55ed3c-0fdf-40d7-8fae-19036a789c72
2018-04-28 18:33:45 hwc hwc[5245] DEBUG node `frontend001` (instance id `3a55ed3c-0fdf-40d7-8fae-19036a789c72`) still building...
2018-04-28 18:33:45 hwc hwc[5245] DEBUG Getting information for instance 5293a27c-3a9a-4d00-b170-e43c14fad521
2018-04-28 18:33:45 hwc hwc[5245] DEBUG node `compute002` (instance id `5293a27c-3a9a-4d00-b170-e43c14fad521`) still building...
2018-04-28 18:33:45 hwc hwc[5245] DEBUG Getting information for instance 65dacfda-cbb8-44c0-a0b0-154b2c0bd7b9
2018-04-28 18:33:46 hwc hwc[5245] DEBUG node `compute001` (instance id `65dacfda-cbb8-44c0-a0b0-154b2c0bd7b9`) still building...
2018-04-28 18:33:46 hwc hwc[5245] DEBUG Waiting for 3 more nodes to come up ...
2018-04-28 18:33:56 hwc hwc[5245] DEBUG Getting information for instance 3a55ed3c-0fdf-40d7-8fae-19036a789c72
2018-04-28 18:33:57 hwc hwc[5245] DEBUG node `frontend001` (instance id `3a55ed3c-0fdf-40d7-8fae-19036a789c72`) still building...
2018-04-28 18:33:57 hwc hwc[5245] DEBUG Getting information for instance 5293a27c-3a9a-4d00-b170-e43c14fad521
2018-04-28 18:33:58 hwc hwc[5245] DEBUG node `compute002` (instance id `5293a27c-3a9a-4d00-b170-e43c14fad521`) still building...
2018-04-28 18:33:58 hwc hwc[5245] DEBUG Getting information for instance 65dacfda-cbb8-44c0-a0b0-154b2c0bd7b9
2018-04-28 18:33:59 hwc hwc[5245] DEBUG node `compute001` (instance id `65dacfda-cbb8-44c0-a0b0-154b2c0bd7b9`) still building...
2018-04-28 18:33:59 hwc hwc[5245] DEBUG Waiting for 3 more nodes to come up ...
2018-04-28 18:34:09 hwc hwc[5245] DEBUG Getting information for instance 3a55ed3c-0fdf-40d7-8fae-19036a789c72
2018-04-28 18:34:09 hwc hwc[5245] DEBUG node `frontend001` (instance id `3a55ed3c-0fdf-40d7-8fae-19036a789c72`) still building...
2018-04-28 18:34:09 hwc hwc[5245] DEBUG Getting information for instance 5293a27c-3a9a-4d00-b170-e43c14fad521
2018-04-28 18:34:10 hwc hwc[5245] DEBUG node `compute002` (instance id `5293a27c-3a9a-4d00-b170-e43c14fad521`) still building...
2018-04-28 18:34:10 hwc hwc[5245] DEBUG Getting information for instance 65dacfda-cbb8-44c0-a0b0-154b2c0bd7b9
2018-04-28 18:34:10 hwc hwc[5245] DEBUG node `compute001` (instance id `65dacfda-cbb8-44c0-a0b0-154b2c0bd7b9`) still building...
2018-04-28 18:34:10 hwc hwc[5245] DEBUG Waiting for 3 more nodes to come up ...
2018-04-28 18:34:20 hwc hwc[5245] DEBUG Getting information for instance 3a55ed3c-0fdf-40d7-8fae-19036a789c72
2018-04-28 18:34:20 hwc hwc[5245] DEBUG node `frontend001` (instance id `3a55ed3c-0fdf-40d7-8fae-19036a789c72`) still building...
2018-04-28 18:34:20 hwc hwc[5245] DEBUG Getting information for instance 5293a27c-3a9a-4d00-b170-e43c14fad521
2018-04-28 18:34:21 hwc hwc[5245] DEBUG node `compute002` (instance id `5293a27c-3a9a-4d00-b170-e43c14fad521`) still building...
2018-04-28 18:34:21 hwc hwc[5245] DEBUG Getting information for instance 65dacfda-cbb8-44c0-a0b0-154b2c0bd7b9
2018-04-28 18:34:22 hwc hwc[5245] DEBUG node `compute001` (instance id `65dacfda-cbb8-44c0-a0b0-154b2c0bd7b9`) still building...
2018-04-28 18:34:22 hwc hwc[5245] DEBUG Waiting for 3 more nodes to come up ...
2018-04-28 18:34:32 hwc hwc[5245] DEBUG Getting information for instance 3a55ed3c-0fdf-40d7-8fae-19036a789c72
2018-04-28 18:34:32 hwc hwc[5245] DEBUG node `frontend001` (instance id `3a55ed3c-0fdf-40d7-8fae-19036a789c72`) still building...
2018-04-28 18:34:32 hwc hwc[5245] DEBUG Getting information for instance 5293a27c-3a9a-4d00-b170-e43c14fad521
2018-04-28 18:34:33 hwc hwc[5245] DEBUG node `compute002` (instance id `5293a27c-3a9a-4d00-b170-e43c14fad521`) still building...
2018-04-28 18:34:33 hwc hwc[5245] DEBUG Getting information for instance 65dacfda-cbb8-44c0-a0b0-154b2c0bd7b9
2018-04-28 18:34:34 hwc hwc[5245] DEBUG node `compute001` (instance id 65dacfda-cbb8-44c0-a0b0-154b2c0bd7b9) is up.
2018-04-28 18:34:34 hwc hwc[5245] DEBUG Waiting for 2 more nodes to come up ...
2018-04-28 18:34:44 hwc hwc[5245] DEBUG Getting information for instance 3a55ed3c-0fdf-40d7-8fae-19036a789c72
2018-04-28 18:34:45 hwc hwc[5245] DEBUG node `frontend001` (instance id 3a55ed3c-0fdf-40d7-8fae-19036a789c72) is up.
2018-04-28 18:34:45 hwc hwc[5245] DEBUG Getting information for instance 5293a27c-3a9a-4d00-b170-e43c14fad521
2018-04-28 18:34:46 hwc hwc[5245] DEBUG node `compute002` (instance id 5293a27c-3a9a-4d00-b170-e43c14fad521) is up.
2018-04-28 18:34:47 hwc hwc[5245] INFO Checking SSH connection to nodes (timeout: 600 seconds) ...
2018-04-28 18:34:47 hwc hwc[5245] DEBUG Trying to connect to host frontend001 (192.168.0.52) ...
2018-04-28 18:34:52 hwc hwc[5245] DEBUG Host frontend001 (192.168.0.52) not reachable within 5 seconds: timed out -- <class 'socket.timeout'>
2018-04-28 18:34:52 hwc hwc[5245] DEBUG Trying to connect to host compute002 (192.168.0.171) ...
2018-04-28 18:34:57 hwc hwc[5245] DEBUG Host compute002 (192.168.0.171) not reachable within 5 seconds: timed out -- <class 'socket.timeout'>
2018-04-28 18:34:57 hwc hwc[5245] DEBUG Trying to connect to host compute001 (192.168.0.100) ...
2018-04-28 18:34:57 hwc hwc[5245] DEBUG Host compute001 (192.168.0.100) not reachable within 5 seconds: [Errno None] Unable to connect to port 22 on 192.168.0.100 -- <class 'paramiko.ssh_exception.NoValidConnectionsError'>
2018-04-28 18:35:07 hwc hwc[5245] DEBUG Trying to connect to host frontend001 (192.168.0.52) ...
2018-04-28 18:35:07 hwc hwc[5245] DEBUG Host frontend001 (192.168.0.52) not reachable within 5 seconds: [Errno None] Unable to connect to port 22 on 192.168.0.52 -- <class 'paramiko.ssh_exception.NoValidConnectionsError'>
2018-04-28 18:35:07 hwc hwc[5245] DEBUG Trying to connect to host compute002 (192.168.0.171) ...
2018-04-28 18:35:07 hwc hwc[5245] DEBUG Host compute002 (192.168.0.171) not reachable within 5 seconds: [Errno None] Unable to connect to port 22 on 192.168.0.171 -- <class 'paramiko.ssh_exception.NoValidConnectionsError'>
2018-04-28 18:35:07 hwc hwc[5245] DEBUG Trying to connect to host compute001 (192.168.0.100) ...
2018-04-28 18:35:07 hwc hwc[5245] DEBUG Host compute001 (192.168.0.100) not reachable within 5 seconds: [Errno None] Unable to connect to port 22 on 192.168.0.100 -- <class 'paramiko.ssh_exception.NoValidConnectionsError'>
2018-04-28 18:35:17 hwc hwc[5245] DEBUG Trying to connect to host frontend001 (192.168.0.52) ...
2018-04-28 18:35:17 hwc hwc[5245] DEBUG Host frontend001 (192.168.0.52) not reachable within 5 seconds: [Errno None] Unable to connect to port 22 on 192.168.0.52 -- <class 'paramiko.ssh_exception.NoValidConnectionsError'>
2018-04-28 18:35:17 hwc hwc[5245] DEBUG Trying to connect to host compute002 (192.168.0.171) ...
2018-04-28 18:35:17 hwc hwc[5245] DEBUG Host compute002 (192.168.0.171) not reachable within 5 seconds: [Errno None] Unable to connect to port 22 on 192.168.0.171 -- <class 'paramiko.ssh_exception.NoValidConnectionsError'>
2018-04-28 18:35:17 hwc hwc[5245] DEBUG Trying to connect to host compute001 (192.168.0.100) ...
2018-04-28 18:35:17 hwc hwc[5245] DEBUG Host compute001 (192.168.0.100) not reachable within 5 seconds: [Errno None] Unable to connect to port 22 on 192.168.0.100 -- <class 'paramiko.ssh_exception.NoValidConnectionsError'>
2018-04-28 18:35:27 hwc hwc[5245] DEBUG Trying to connect to host frontend001 (192.168.0.52) ...
2018-04-28 18:35:27 hwc hwc[5245] DEBUG Connection to 192.168.0.52 succeeded on port 22, will use this IP address for future connections.
2018-04-28 18:35:27 hwc hwc[5245] INFO Connection to node `frontend001` successful, using IP address 192.168.0.52 to connect.
2018-04-28 18:35:27 hwc hwc[5245] DEBUG Trying to connect to host compute002 (192.168.0.171) ...
2018-04-28 18:35:27 hwc hwc[5245] DEBUG Connection to 192.168.0.171 succeeded on port 22, will use this IP address for future connections.
2018-04-28 18:35:27 hwc hwc[5245] INFO Connection to node `compute002` successful, using IP address 192.168.0.171 to connect.
2018-04-28 18:35:27 hwc hwc[5245] DEBUG Trying to connect to host compute001 (192.168.0.100) ...
2018-04-28 18:35:28 hwc hwc[5245] DEBUG Connection to 192.168.0.100 succeeded on port 22, will use this IP address for future connections.
2018-04-28 18:35:28 hwc hwc[5245] INFO Connection to node `compute001` successful, using IP address 192.168.0.100 to connect.
2018-04-28 18:35:28 hwc hwc[5245] DEBUG Writing Ansible inventory to file `/root/hwc/cfg/storage/slurm.inventory` ...
2018-04-28 18:35:28 hwc hwc[5245] DEBUG Calling `ansible-playbook` with the following environment:
2018-04-28 18:35:28 hwc hwc[5245] DEBUG - ANSIBLE_ANY_ERRORS_FATAL='yes'
2018-04-28 18:35:28 hwc hwc[5245] DEBUG - ANSIBLE_FORKS='10'
2018-04-28 18:35:28 hwc hwc[5245] DEBUG - ANSIBLE_HOST_KEY_CHECKING='no'
2018-04-28 18:35:28 hwc hwc[5245] DEBUG - ANSIBLE_RETRY_FILES_ENABLED='no'
2018-04-28 18:35:28 hwc hwc[5245] DEBUG - ANSIBLE_ROLES_PATH='/root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles:/root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks:/etc/ansible/roles'
2018-04-28 18:35:28 hwc hwc[5245] DEBUG - ANSIBLE_SSH_PIPELINING='yes'
2018-04-28 18:35:28 hwc hwc[5245] DEBUG - ANSIBLE_TIMEOUT='120'
2018-04-28 18:35:28 hwc hwc[5245] DEBUG - CINDER_ENDPOINT_TYPE='publicURL'
2018-04-28 18:35:28 hwc hwc[5245] DEBUG - CLUSTER_HOME='/root/elasticluster'
2018-04-28 18:35:28 hwc hwc[5245] DEBUG - CVS_RSH='ssh'
2018-04-28 18:35:28 hwc hwc[5245] DEBUG - G_BROKEN_FILENAMES='1'
2018-04-28 18:35:28 hwc hwc[5245] DEBUG - HISTCONTROL='ignoredups'
2018-04-28 18:35:28 hwc hwc[5245] DEBUG - HISTSIZE='1000'
2018-04-28 18:35:28 hwc hwc[5245] DEBUG - HISTTIMEFORMAT='%F %T root '
2018-04-28 18:35:28 hwc hwc[5245] DEBUG - HOME='/root'
2018-04-28 18:35:28 hwc hwc[5245] DEBUG - HOSTNAME='hwc'
2018-04-28 18:35:28 hwc hwc[5245] DEBUG - LANG='en_US.UTF-8'
2018-04-28 18:35:28 hwc hwc[5245] DEBUG - LESSOPEN='||/usr/bin/lesspipe.sh %s'
2018-04-28 18:35:28 hwc hwc[5245] DEBUG - LOGNAME='root'
2018-04-28 18:35:28 hwc hwc[5245] DEBUG - LS_COLORS='rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=01;05;37;41:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arj=01;31:*.taz=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.dz=01;31:*.gz=01;31:*.lz=01;31:*.xz=01;31:*.bz2=01;31:*.tbz=01;31:*.tbz2=01;31:*.bz=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.rar=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.jpg=01;35:*.jpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.axv=01;35:*.anx=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=01;36:*.au=01;36:*.flac=01;36:*.mid=01;36:*.midi=01;36:*.mka=01;36:*.mp3=01;36:*.mpc=01;36:*.ogg=01;36:*.ra=01;36:*.wav=01;36:*.axa=01;36:*.oga=01;36:*.spx=01;36:*.xspf=01;36:'
2018-04-28 18:35:28 hwc hwc[5245] DEBUG - MAIL='/var/spool/mail/root'
2018-04-28 18:35:28 hwc hwc[5245] DEBUG - NOVA_ENDPOINT_TYPE='publicURL'
2018-04-28 18:35:28 hwc hwc[5245] DEBUG - OS_AUTH_URL='https://iam.cn-north-1.myhwclouds.com/v3'
2018-04-28 18:35:28 hwc hwc[5245] DEBUG - OS_AVAILABILITY='cn-north-1a'
2018-04-28 18:35:28 hwc hwc[5245] DEBUG - OS_ENDPOINT_TYPE='publicURL'
2018-04-28 18:35:28 hwc hwc[5245] DEBUG - OS_IDENTITY_API_VERSION='3'
2018-04-28 18:35:28 hwc hwc[5245] DEBUG - OS_IMAGE_API_VERSION='2'
2018-04-28 18:35:28 hwc hwc[5245] DEBUG - OS_PASSWORD='1980813c'
2018-04-28 18:35:28 hwc hwc[5245] DEBUG - OS_PROJECT_DOMAIN_NAME='mschyj'
2018-04-28 18:35:28 hwc hwc[5245] DEBUG - OS_PROJECT_NAME='cn-north-1'
2018-04-28 18:35:28 hwc hwc[5245] DEBUG - OS_TENANT_NAME='cn-north-1'
2018-04-28 18:35:28 hwc hwc[5245] DEBUG - OS_USERNAME='mschyj'
2018-04-28 18:35:28 hwc hwc[5245] DEBUG - OS_USER_DOMAIN_NAME='mschyj'
2018-04-28 18:35:28 hwc hwc[5245] DEBUG - OS_VOLUME_API_VERSION='2'
2018-04-28 18:35:28 hwc hwc[5245] DEBUG - PATH='/usr/lib64/qt-3.3/bin:/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin:/root/bin:/root/bin'
2018-04-28 18:35:28 hwc hwc[5245] DEBUG - PWD='/root/hwc'
2018-04-28 18:35:28 hwc hwc[5245] DEBUG - QTDIR='/usr/lib64/qt-3.3'
2018-04-28 18:35:28 hwc hwc[5245] DEBUG - QTINC='/usr/lib64/qt-3.3/include'
2018-04-28 18:35:28 hwc hwc[5245] DEBUG - QTLIB='/usr/lib64/qt-3.3/lib'
2018-04-28 18:35:28 hwc hwc[5245] DEBUG - SHELL='/bin/bash'
2018-04-28 18:35:28 hwc hwc[5245] DEBUG - SHLVL='3'
2018-04-28 18:35:28 hwc hwc[5245] DEBUG - SSH_CLIENT='58.213.108.56 42151 22'
2018-04-28 18:35:28 hwc hwc[5245] DEBUG - SSH_CONNECTION='58.213.108.56 42151 192.168.0.230 22'
2018-04-28 18:35:28 hwc hwc[5245] DEBUG - SSH_TTY='/dev/pts/0'
2018-04-28 18:35:28 hwc hwc[5245] DEBUG - TERM='screen'
2018-04-28 18:35:28 hwc hwc[5245] DEBUG - TMUX='/tmp/tmux-0/default,2468,0'
2018-04-28 18:35:28 hwc hwc[5245] DEBUG - TMUX_PANE='%2'
2018-04-28 18:35:28 hwc hwc[5245] DEBUG - USER='root'
2018-04-28 18:35:28 hwc hwc[5245] DEBUG - _='/usr/local/bin/elasticluster'
2018-04-28 18:35:28 hwc hwc[5245] DEBUG Using playbook file /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/site.yml.
2018-04-28 18:35:28 hwc hwc[5245] DEBUG Running Ansible command `ansible-playbook --private-key=/root/hwc/auth/id_rsa /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/site.yml --inventory=/root/hwc/cfg/storage/slurm.inventory --become --become-user=root -vv -e elasticluster_output_dir=/tmp/elasticluster.fnRDac.d` ...
No config file found; using defaults
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/hosts.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/hostname.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/netgroup.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/ssh_auth.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/ssh_keysign_rhel7.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/cuda/tasks/_check_nvidia_dev.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/cuda/tasks/_reboot_and_wait.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/cuda/tasks/_check_nvidia_dev.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/ceph/tasks/mon.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/ceph/tasks/mgr.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/ceph/tasks/_create_key.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/ceph/tasks/osd.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/ceph/tasks/mds.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/ceph/tasks/fs.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/ceph/tasks/client.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/glusterfs-common/tasks/debian.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/glusterfs-common/tasks/rhel.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/glusterfs-common/tasks/debian.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/glusterfs-common/tasks/rhel.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/ssh_auth.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/ssh_keysign_rhel7.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/handlers/main.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/handlers/main.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypbind.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/cgroup.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypbind.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypbind.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/lmod/tasks/build.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/lmod/tasks/post-install.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypbind.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/lmod/tasks/build.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/lmod/tasks/post-install.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypbind.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/postgresql/tasks/install.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/postgresql/tasks/install_yum.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/postgresql/tasks/extensions.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/postgresql/tasks/extensions/contrib.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/postgresql/tasks/extensions/dev_headers.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/postgresql/tasks/extensions/postgis.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/postgresql/tasks/configure.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/postgresql/tasks/users.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/postgresql/tasks/databases.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/postgresql/tasks/users_privileges.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/postgresql/tasks/monit.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/lmod/tasks/build.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/lmod/tasks/post-install.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/r/tasks/rmpi.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypbind.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/lmod/tasks/build.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/lmod/tasks/post-install.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/r/tasks/rmpi.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/lmod/tasks/build.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/lmod/tasks/post-install.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypbind.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/handlers/main.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/handlers/main.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/ssh_auth.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/ssh_keysign_rhel7.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/jenkins/tasks/main.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/jenkins/handlers/main.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/kubernetes-common/tasks/debian.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/kubernetes-common/tasks/redhat.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/kubernetes-common/tasks/debian.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/kubernetes-common/tasks/redhat.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypbind.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/lmod/tasks/build.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/lmod/tasks/post-install.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/pbs+maui/tasks/master.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/handlers/main.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/pbs+maui/handlers/main.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/pbs+maui/tasks/maui.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/pbs+maui/handlers/main.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypbind.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/lmod/tasks/build.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/lmod/tasks/post-install.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/pbs+maui/tasks/clients.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/pbs+maui/handlers/main.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypbind.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-master/tasks/db.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-master/tasks/install-slurmdbd.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-master/tasks/install-slurmctld.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypbind.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/cgroup.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypbind.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/lmod/tasks/build.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/lmod/tasks/post-install.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/lmod/tasks/build.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/lmod/tasks/post-install.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/r/tasks/rmpi.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/smb-server/tasks/ctdb.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/lmod/tasks/build.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/lmod/tasks/post-install.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/r/tasks/rmpi.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/jupyter/tasks/bash.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/jupyter/tasks/python.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/jupyter/tasks/python.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/jupyter/tasks/pyspark.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/jupyter/tasks/pyspark.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/jupyter/tasks/irkernel.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/jupyter/tasks/matlab.yml

PLAYBOOK: site.yml *************************************************************
44 plays in /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/site.yml

PLAY [Prepare VM for running Ansible] ******************************************

TASK [Ensure Python is installed] **********************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/site.yml:6
skipping: [compute001] => {"changed": false, "msg": "skipped, since /usr/bin/python exists", "skipped": true}
skipping: [frontend001] => {"changed": false, "msg": "skipped, since /usr/bin/python exists", "skipped": true}
skipping: [compute002] => {"changed": false, "msg": "skipped, since /usr/bin/python exists", "skipped": true}

PLAY [Apply local customizations (before)] *************************************

TASK [setup] *******************************************************************
ok: [compute002]
ok: [frontend001]
ok: [compute001]

PLAY [Common setup for all hosts] **********************************************

TASK [setup] *******************************************************************
ok: [compute002]
ok: [compute001]
ok: [frontend001]

TASK [Ensure apt-daily is *not* running] ***************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/site.yml:20
skipping: [frontend001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [common : Provide workaround for YAML syntax error in lines containing colon+space] ***
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/main.yml:3
ok: [frontend001] => {"ansible_facts": {"__colon__": ":"}, "changed": false}
ok: [compute001] => {"ansible_facts": {"__colon__": ":"}, "changed": false}
ok: [compute002] => {"ansible_facts": {"__colon__": ":"}, "changed": false}

TASK [common : Allow package updates] ******************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/main.yml:9
skipping: [frontend001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [common : Disallow package updates] ***************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/main.yml:14
ok: [frontend001] => {"ansible_facts": {"pkg_install_state": "present"}, "changed": false}
ok: [compute001] => {"ansible_facts": {"pkg_install_state": "present"}, "changed": false}
ok: [compute002] => {"ansible_facts": {"pkg_install_state": "present"}, "changed": false}

TASK [common : Set /etc/hosts from Ansible hostgroups] *************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/hosts.yml:2
changed: [frontend001] => {"changed": true, "checksum": "360c4fb84e9694a05121c0c2be099ccee008baaf", "dest": "/etc/hosts", "gid": 0, "group": "root", "md5sum": "36fe95c2a7174b3410bf7d54e24b3f42", "mode": "0644", "owner": "root", "size": 431, "src": "/root/.ansible/tmp/ansible-tmp-1524911733.68-121662671878918/source", "state": "file", "uid": 0}
changed: [compute001] => {"changed": true, "checksum": "360c4fb84e9694a05121c0c2be099ccee008baaf", "dest": "/etc/hosts", "gid": 0, "group": "root", "md5sum": "36fe95c2a7174b3410bf7d54e24b3f42", "mode": "0644", "owner": "root", "size": 431, "src": "/root/.ansible/tmp/ansible-tmp-1524911733.69-49875121560947/source", "state": "file", "uid": 0}
changed: [compute002] => {"changed": true, "checksum": "360c4fb84e9694a05121c0c2be099ccee008baaf", "dest": "/etc/hosts", "gid": 0, "group": "root", "md5sum": "36fe95c2a7174b3410bf7d54e24b3f42", "mode": "0644", "owner": "root", "size": 431, "src": "/root/.ansible/tmp/ansible-tmp-1524911733.7-81869170815505/source", "state": "file", "uid": 0}

TASK [common : Patch `/etc/redhat-release`] ************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/hostname.yml:8
skipping: [frontend001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [common : Set host name to Ansible "inventory name"] **********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/hostname.yml:16
changed: [compute002] => {"ansible_facts": {"ansible_domain": "", "ansible_fqdn": "compute002", "ansible_hostname": "compute002", "ansible_nodename": "compute002"}, "changed": true, "name": "compute002"}
ok: [frontend001] => {"ansible_facts": {"ansible_domain": "", "ansible_fqdn": "frontend001", "ansible_hostname": "frontend001", "ansible_nodename": "frontend001"}, "changed": false, "name": "frontend001"}
changed: [compute001] => {"ansible_facts": {"ansible_domain": "", "ansible_fqdn": "compute001", "ansible_hostname": "compute001", "ansible_nodename": "compute001"}, "changed": true, "name": "compute001"}

TASK [common : Undo patch to `/etc/redhat-release`] ****************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/hostname.yml:22
skipping: [frontend001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [common : Check for cloud-init conf file] *********************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/hostname.yml:30
ok: [frontend001] => {"changed": false, "stat": {"atime": 1524911720.586, "checksum": "ce75755e6aabf7995730f54951ecb5da27974667", "ctime": 1523322248.64, "dev": 51714, "executable": false, "exists": true, "gid": 0, "gr_name": "root", "inode": 1572909, "isblk": false, "ischr": false, "isdir": false, "isfifo": false, "isgid": false, "islnk": false, "isreg": true, "issock": false, "isuid": false, "md5": "bba22929886688d6488b6b8b85e33d74", "mode": "0664", "mtime": 1523322248.64, "nlink": 1, "path": "/etc/cloud/cloud.cfg", "pw_name": "root", "readable": true, "rgrp": true, "roth": true, "rusr": true, "size": 1627, "uid": 0, "wgrp": true, "woth": false, "writeable": true, "wusr": true, "xgrp": false, "xoth": false, "xusr": false}}
ok: [compute001] => {"changed": false, "stat": {"atime": 1524911720.723, "checksum": "ce75755e6aabf7995730f54951ecb5da27974667", "ctime": 1523322248.64, "dev": 51714, "executable": false, "exists": true, "gid": 0, "gr_name": "root", "inode": 1572909, "isblk": false, "ischr": false, "isdir": false, "isfifo": false, "isgid": false, "islnk": false, "isreg": true, "issock": false, "isuid": false, "md5": "bba22929886688d6488b6b8b85e33d74", "mode": "0664", "mtime": 1523322248.64, "nlink": 1, "path": "/etc/cloud/cloud.cfg", "pw_name": "root", "readable": true, "rgrp": true, "roth": true, "rusr": true, "size": 1627, "uid": 0, "wgrp": true, "woth": false, "writeable": true, "wusr": true, "xgrp": false, "xoth": false, "xusr": false}}
ok: [compute002] => {"changed": false, "stat": {"atime": 1524911720.2380002, "checksum": "ce75755e6aabf7995730f54951ecb5da27974667", "ctime": 1523322248.64, "dev": 51714, "executable": false, "exists": true, "gid": 0, "gr_name": "root", "inode": 1572909, "isblk": false, "ischr": false, "isdir": false, "isfifo": false, "isgid": false, "islnk": false, "isreg": true, "issock": false, "isuid": false, "md5": "bba22929886688d6488b6b8b85e33d74", "mode": "0664", "mtime": 1523322248.64, "nlink": 1, "path": "/etc/cloud/cloud.cfg", "pw_name": "root", "readable": true, "rgrp": true, "roth": true, "rusr": true, "size": 1627, "uid": 0, "wgrp": true, "woth": false, "writeable": true, "wusr": true, "xgrp": false, "xoth": false, "xusr": false}}

TASK [common : Ensure changes to hostname are not overwritten by cloud-init] ***
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/hostname.yml:41
ok: [compute002] => {"backup": "", "changed": false, "msg": ""}
ok: [compute001] => {"backup": "", "changed": false, "msg": ""}
ok: [frontend001] => {"backup": "", "changed": false, "msg": ""}

TASK [common : Deploy `/etc/netgroup` file.] ***********************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/netgroup.yml:3
changed: [compute002] => {"changed": true, "checksum": "ee2d3dfaf1af7ca9fa96b282c07b66cf57d2ecb6", "dest": "/etc/netgroup", "gid": 0, "group": "root", "md5sum": "5fa85341d712d505a56fb9a43e938e53", "mode": "0444", "owner": "root", "size": 274, "src": "/root/.ansible/tmp/ansible-tmp-1524911736.21-180984755936075/source", "state": "file", "uid": 0}
changed: [compute001] => {"changed": true, "checksum": "ee2d3dfaf1af7ca9fa96b282c07b66cf57d2ecb6", "dest": "/etc/netgroup", "gid": 0, "group": "root", "md5sum": "5fa85341d712d505a56fb9a43e938e53", "mode": "0444", "owner": "root", "size": 274, "src": "/root/.ansible/tmp/ansible-tmp-1524911736.19-225904203168400/source", "state": "file", "uid": 0}
changed: [frontend001] => {"changed": true, "checksum": "ee2d3dfaf1af7ca9fa96b282c07b66cf57d2ecb6", "dest": "/etc/netgroup", "gid": 0, "group": "root", "md5sum": "5fa85341d712d505a56fb9a43e938e53", "mode": "0444", "owner": "root", "size": 274, "src": "/root/.ansible/tmp/ansible-tmp-1524911736.12-107992028764520/source", "state": "file", "uid": 0}

TASK [common : Add `files` databases to `netgroup` service (I)] ****************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/netgroup.yml:21
ok: [compute002] => {"changed": false, "msg": ""}
ok: [compute001] => {"changed": false, "msg": ""}
ok: [frontend001] => {"changed": false, "msg": ""}

TASK [common : Add `files` databases to `netgroup` service (II)] ***************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/netgroup.yml:32
skipping: [frontend001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [common : Load distribution-dependent values (Debian/Ubuntu)] *************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/ssh_auth.yml:6
skipping: [frontend001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [common : Load distribution-dependent values (CentOS/RHEL)] ***************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/ssh_auth.yml:12
ok: [frontend001] => {"ansible_facts": {"ssh_keysign_path": "/usr/libexec/openssh/ssh-keysign"}, "changed": false}
ok: [compute001] => {"ansible_facts": {"ssh_keysign_path": "/usr/libexec/openssh/ssh-keysign"}, "changed": false}
ok: [compute002] => {"ansible_facts": {"ssh_keysign_path": "/usr/libexec/openssh/ssh-keysign"}, "changed": false}

TASK [common : Setup SSH known hosts file] *************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/ssh_auth.yml:18
changed: [frontend001] => {"changed": true, "checksum": "b78a8dbbd5c29ec6b916b062cb22b2e5cc3643ee", "dest": "/etc/ssh/ssh_known_hosts", "gid": 0, "group": "root", "md5sum": "251006d140856f8f139c5e12c103ec3a", "mode": "0644", "owner": "root", "size": 3060, "src": "/root/.ansible/tmp/ansible-tmp-1524911738.17-194016532303057/source", "state": "file", "uid": 0}
changed: [compute002] => {"changed": true, "checksum": "b78a8dbbd5c29ec6b916b062cb22b2e5cc3643ee", "dest": "/etc/ssh/ssh_known_hosts", "gid": 0, "group": "root", "md5sum": "251006d140856f8f139c5e12c103ec3a", "mode": "0644", "owner": "root", "size": 3060, "src": "/root/.ansible/tmp/ansible-tmp-1524911738.2-210231795106256/source", "state": "file", "uid": 0}
changed: [compute001] => {"changed": true, "checksum": "b78a8dbbd5c29ec6b916b062cb22b2e5cc3643ee", "dest": "/etc/ssh/ssh_known_hosts", "gid": 0, "group": "root", "md5sum": "251006d140856f8f139c5e12c103ec3a", "mode": "0644", "owner": "root", "size": 3060, "src": "/root/.ansible/tmp/ansible-tmp-1524911738.18-149098994163370/source", "state": "file", "uid": 0}

TASK [common : Setup /etc/ssh/shosts.equiv file] *******************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/ssh_auth.yml:27
changed: [compute001] => {"changed": true, "checksum": "ef446af1e5a5ab07cd5d7be9ba45605e44a18095", "dest": "/etc/ssh/shosts.equiv", "gid": 0, "group": "root", "md5sum": "c4b5f211f721a914eaab5012d3ff1a17", "mode": "0644", "owner": "root", "size": 75, "src": "/root/.ansible/tmp/ansible-tmp-1524911739.44-258299160360524/source", "state": "file", "uid": 0}
changed: [compute002] => {"changed": true, "checksum": "ef446af1e5a5ab07cd5d7be9ba45605e44a18095", "dest": "/etc/ssh/shosts.equiv", "gid": 0, "group": "root", "md5sum": "c4b5f211f721a914eaab5012d3ff1a17", "mode": "0644", "owner": "root", "size": 75, "src": "/root/.ansible/tmp/ansible-tmp-1524911739.45-74704111087875/source", "state": "file", "uid": 0}
changed: [frontend001] => {"changed": true, "checksum": "ef446af1e5a5ab07cd5d7be9ba45605e44a18095", "dest": "/etc/ssh/shosts.equiv", "gid": 0, "group": "root", "md5sum": "c4b5f211f721a914eaab5012d3ff1a17", "mode": "0644", "owner": "root", "size": 75, "src": "/root/.ansible/tmp/ansible-tmp-1524911739.42-82102144888143/source", "state": "file", "uid": 0}

TASK [common : Setup /root/.shosts file] ***************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/ssh_auth.yml:35
changed: [compute001] => {"changed": true, "checksum": "ef446af1e5a5ab07cd5d7be9ba45605e44a18095", "dest": "/root/.shosts", "gid": 0, "group": "root", "md5sum": "c4b5f211f721a914eaab5012d3ff1a17", "mode": "0644", "owner": "root", "size": 75, "src": "/root/.ansible/tmp/ansible-tmp-1524911740.68-1832519808670/source", "state": "file", "uid": 0}
changed: [frontend001] => {"changed": true, "checksum": "ef446af1e5a5ab07cd5d7be9ba45605e44a18095", "dest": "/root/.shosts", "gid": 0, "group": "root", "md5sum": "c4b5f211f721a914eaab5012d3ff1a17", "mode": "0644", "owner": "root", "size": 75, "src": "/root/.ansible/tmp/ansible-tmp-1524911740.67-53720960179661/source", "state": "file", "uid": 0}
changed: [compute002] => {"changed": true, "checksum": "ef446af1e5a5ab07cd5d7be9ba45605e44a18095", "dest": "/root/.shosts", "gid": 0, "group": "root", "md5sum": "c4b5f211f721a914eaab5012d3ff1a17", "mode": "0644", "owner": "root", "size": 75, "src": "/root/.ansible/tmp/ansible-tmp-1524911740.72-199777717882740/source", "state": "file", "uid": 0}

TASK [common : Setup SSH authentication (server configuration file)] ***********
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/ssh_auth.yml:43
ok: [compute001] => (item={u'value': u'yes', u'key': u'HostbasedAuthentication'}) => {"backup": "", "changed": false, "item": {"key": "HostbasedAuthentication", "value": "yes"}, "msg": ""}
ok: [frontend001] => (item={u'value': u'yes', u'key': u'HostbasedAuthentication'}) => {"backup": "", "changed": false, "item": {"key": "HostbasedAuthentication", "value": "yes"}, "msg": ""}
ok: [compute002] => (item={u'value': u'yes', u'key': u'HostbasedAuthentication'}) => {"backup": "", "changed": false, "item": {"key": "HostbasedAuthentication", "value": "yes"}, "msg": ""}
ok: [compute001] => (item={u'value': u'no', u'key': u'IgnoreRhosts'}) => {"backup": "", "changed": false, "item": {"key": "IgnoreRhosts", "value": "no"}, "msg": ""}
ok: [frontend001] => (item={u'value': u'no', u'key': u'IgnoreRhosts'}) => {"backup": "", "changed": false, "item": {"key": "IgnoreRhosts", "value": "no"}, "msg": ""}
ok: [compute002] => (item={u'value': u'no', u'key': u'IgnoreRhosts'}) => {"backup": "", "changed": false, "item": {"key": "IgnoreRhosts", "value": "no"}, "msg": ""}
ok: [compute001] => (item={u'value': u'yes', u'key': u'PasswordAuthentication'}) => {"backup": "", "changed": false, "item": {"key": "PasswordAuthentication", "value": "yes"}, "msg": ""}
ok: [frontend001] => (item={u'value': u'yes', u'key': u'PasswordAuthentication'}) => {"backup": "", "changed": false, "item": {"key": "PasswordAuthentication", "value": "yes"}, "msg": ""}
ok: [compute002] => (item={u'value': u'yes', u'key': u'PasswordAuthentication'}) => {"backup": "", "changed": false, "item": {"key": "PasswordAuthentication", "value": "yes"}, "msg": ""}

TASK [common : Setup SSH authentication (client configuration file)] ***********
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/ssh_auth.yml:55
ok: [frontend001] => (item={u'value': u'yes', u'key': u'HostbasedAuthentication'}) => {"backup": "", "changed": false, "item": {"key": "HostbasedAuthentication", "value": "yes"}, "msg": ""}
ok: [compute001] => (item={u'value': u'yes', u'key': u'HostbasedAuthentication'}) => {"backup": "", "changed": false, "item": {"key": "HostbasedAuthentication", "value": "yes"}, "msg": ""}
ok: [compute002] => (item={u'value': u'yes', u'key': u'HostbasedAuthentication'}) => {"backup": "", "changed": false, "item": {"key": "HostbasedAuthentication", "value": "yes"}, "msg": ""}
ok: [frontend001] => (item={u'value': u'yes', u'key': u'EnableSSHKeysign'}) => {"backup": "", "changed": false, "item": {"key": "EnableSSHKeysign", "value": "yes"}, "msg": ""}
ok: [compute002] => (item={u'value': u'yes', u'key': u'EnableSSHKeysign'}) => {"backup": "", "changed": false, "item": {"key": "EnableSSHKeysign", "value": "yes"}, "msg": ""}
ok: [compute001] => (item={u'value': u'yes', u'key': u'EnableSSHKeysign'}) => {"backup": "", "changed": false, "item": {"key": "EnableSSHKeysign", "value": "yes"}, "msg": ""}

TASK [common : Load info about the `ssh-keysign` executable] *******************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/ssh_keysign_rhel7.yml:9
skipping: [frontend001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [common : List all SSH host keys] *****************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/ssh_keysign_rhel7.yml:15
skipping: [frontend001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [common : Ensure SSH host keys can be read by `ssh-keysign`] **************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/ssh_keysign_rhel7.yml:23
skipping: [frontend001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [iptables : Load distribution-specific data] ******************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/iptables/tasks/main.yml:18
included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/iptables/tasks/init-RedHat.yml for frontend001, compute001, compute002

TASK [iptables : Load configuration and service names (RHEL-compatible)] *******
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/iptables/tasks/init-RedHat.yml:3
ok: [frontend001] => {"ansible_facts": {"configfile": {"etc/iptables/rules.v4": "/etc/sysconfig/iptables", "etc/iptables/rules.v6": "/etc/sysconfig/ip6tables"}, "reload": "reload iptables", "service": {"ip6tables": "ip6tables", "iptables": "iptables"}}, "changed": false}
ok: [compute001] => {"ansible_facts": {"configfile": {"etc/iptables/rules.v4": "/etc/sysconfig/iptables", "etc/iptables/rules.v6": "/etc/sysconfig/ip6tables"}, "reload": "reload iptables", "service": {"ip6tables": "ip6tables", "iptables": "iptables"}}, "changed": false}
ok: [compute002] => {"ansible_facts": {"configfile": {"etc/iptables/rules.v4": "/etc/sysconfig/iptables", "etc/iptables/rules.v6": "/etc/sysconfig/ip6tables"}, "reload": "reload iptables", "service": {"ip6tables": "ip6tables", "iptables": "iptables"}}, "changed": false}

TASK [iptables : Load configuration and service names (RHEL6-compatible)] ******
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/iptables/tasks/init-RedHat.yml:17
ok: [frontend001] => {"ansible_facts": {"packages": ["iptables", "iptables-ipv6"]}, "changed": false}
ok: [compute001] => {"ansible_facts": {"packages": ["iptables", "iptables-ipv6"]}, "changed": false}
ok: [compute002] => {"ansible_facts": {"packages": ["iptables", "iptables-ipv6"]}, "changed": false}

TASK [iptables : Load configuration and service names (RHEL7-compatible)] ******
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/iptables/tasks/init-RedHat.yml:25
skipping: [frontend001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [iptables : Install iptables packages] ************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/iptables/tasks/main.yml:25
ok: [compute001] => (item=[u'iptables', u'iptables-ipv6']) => {"changed": false, "item": ["iptables", "iptables-ipv6"], "msg": "", "rc": 0, "results": ["iptables-1.4.7-16.el6.x86_64 providing iptables is already installed", "iptables-ipv6-1.4.7-16.el6.x86_64 providing iptables-ipv6 is already installed"]}
ok: [compute002] => (item=[u'iptables', u'iptables-ipv6']) => {"changed": false, "item": ["iptables", "iptables-ipv6"], "msg": "", "rc": 0, "results": ["iptables-1.4.7-16.el6.x86_64 providing iptables is already installed", "iptables-ipv6-1.4.7-16.el6.x86_64 providing iptables-ipv6 is already installed"]}
ok: [frontend001] => (item=[u'iptables', u'iptables-ipv6']) => {"changed": false, "item": ["iptables", "iptables-ipv6"], "msg": "", "rc": 0, "results": ["iptables-1.4.7-16.el6.x86_64 providing iptables is already installed", "iptables-ipv6-1.4.7-16.el6.x86_64 providing iptables-ipv6 is already installed"]}

TASK [iptables : Deploy netfilter rules] ***************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/iptables/tasks/main.yml:33
changed: [frontend001] => (item=etc/iptables/rules.v4) => {"changed": true, "checksum": "343641a53f128660be7113487862d1ff9c90560e", "dest": "/etc/sysconfig/iptables", "gid": 0, "group": "root", "item": "etc/iptables/rules.v4", "md5sum": "4fb5b55ae4593c52bf9915c4750819d3", "mode": "0444", "owner": "root", "size": 1115, "src": "/root/.ansible/tmp/ansible-tmp-1524911745.15-157225411803855/source", "state": "file", "uid": 0}
changed: [compute001] => (item=etc/iptables/rules.v4) => {"changed": true, "checksum": "343641a53f128660be7113487862d1ff9c90560e", "dest": "/etc/sysconfig/iptables", "gid": 0, "group": "root", "item": "etc/iptables/rules.v4", "md5sum": "4fb5b55ae4593c52bf9915c4750819d3", "mode": "0444", "owner": "root", "size": 1115, "src": "/root/.ansible/tmp/ansible-tmp-1524911745.15-51007817822333/source", "state": "file", "uid": 0}
changed: [compute002] => (item=etc/iptables/rules.v4) => {"changed": true, "checksum": "343641a53f128660be7113487862d1ff9c90560e", "dest": "/etc/sysconfig/iptables", "gid": 0, "group": "root", "item": "etc/iptables/rules.v4", "md5sum": "4fb5b55ae4593c52bf9915c4750819d3", "mode": "0444", "owner": "root", "size": 1115, "src": "/root/.ansible/tmp/ansible-tmp-1524911745.17-207288938260249/source", "state": "file", "uid": 0}
NOTIFIED HANDLER reload iptables
NOTIFIED HANDLER reload iptables
ok: [compute002] => (item=etc/iptables/rules.v6) => {"changed": false, "gid": 0, "group": "root", "item": "etc/iptables/rules.v6", "mode": "0444", "owner": "root", "path": "/etc/sysconfig/ip6tables", "size": 1180, "state": "file", "uid": 0}
NOTIFIED HANDLER reload iptables
ok: [frontend001] => (item=etc/iptables/rules.v6) => {"changed": false, "gid": 0, "group": "root", "item": "etc/iptables/rules.v6", "mode": "0444", "owner": "root", "path": "/etc/sysconfig/ip6tables", "size": 1180, "state": "file", "uid": 0}
ok: [compute001] => (item=etc/iptables/rules.v6) => {"changed": false, "gid": 0, "group": "root", "item": "etc/iptables/rules.v6", "mode": "0444", "owner": "root", "path": "/etc/sysconfig/ip6tables", "size": 1180, "state": "file", "uid": 0}

TASK [iptables : Ensure netfilter rules are loaded at boot] ********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/iptables/tasks/main.yml:50
ok: [compute002] => (item=iptables) => {"changed": false, "enabled": true, "item": "iptables", "name": "iptables", "state": "started"}
ok: [compute001] => (item=iptables) => {"changed": false, "enabled": true, "item": "iptables", "name": "iptables", "state": "started"}
ok: [frontend001] => (item=iptables) => {"changed": false, "enabled": true, "item": "iptables", "name": "iptables", "state": "started"}
ok: [compute001] => (item=ip6tables) => {"changed": false, "enabled": true, "item": "ip6tables", "name": "ip6tables", "state": "started"}
ok: [compute002] => (item=ip6tables) => {"changed": false, "enabled": true, "item": "ip6tables", "name": "ip6tables", "state": "started"}
ok: [frontend001] => (item=ip6tables) => {"changed": false, "enabled": true, "item": "ip6tables", "name": "ip6tables", "state": "started"}

RUNNING HANDLER [iptables : reload iptables] ***********************************
changed: [compute001] => (item=iptables) => {"changed": true, "item": "iptables", "name": "iptables", "state": "started"}
changed: [compute002] => (item=iptables) => {"changed": true, "item": "iptables", "name": "iptables", "state": "started"}
changed: [frontend001] => (item=iptables) => {"changed": true, "item": "iptables", "name": "iptables", "state": "started"}
changed: [compute001] => (item=ip6tables) => {"changed": true, "item": "ip6tables", "name": "ip6tables", "state": "started"}
changed: [compute002] => (item=ip6tables) => {"changed": true, "item": "ip6tables", "name": "ip6tables", "state": "started"}
changed: [frontend001] => (item=ip6tables) => {"changed": true, "item": "ip6tables", "name": "ip6tables", "state": "started"}

TASK [ntpd : Load distribution-specific parameters] ****************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/ntpd/tasks/main.yml:3
included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/ntpd/tasks/init-RedHat.yml for frontend001, compute001, compute002

TASK [ntpd : Set NTPd common playbook params (RHEL/CentOS)] ********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/ntpd/tasks/init-RedHat.yml:3
ok: [frontend001] => {"ansible_facts": {"ntpd_package": "ntp", "ntpd_service": "ntpd"}, "changed": false}
ok: [compute001] => {"ansible_facts": {"ntpd_package": "ntp", "ntpd_service": "ntpd"}, "changed": false}
ok: [compute002] => {"ansible_facts": {"ntpd_package": "ntp", "ntpd_service": "ntpd"}, "changed": false}

TASK [ntpd : Deploy NTP configuration file] ************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/ntpd/tasks/main.yml:7
NOTIFIED HANDLER restart ntpd
changed: [frontend001] => {"changed": true, "checksum": "6a2291b1f1fd168748d1db7953cbe66ed2904151", "dest": "/etc/ntp.conf", "gid": 0, "group": "root", "md5sum": "e7b8052f8f4375213ed726d36b7a96f8", "mode": "0444", "owner": "root", "size": 1981, "src": "/root/.ansible/tmp/ansible-tmp-1524911748.38-31294978398625/source", "state": "file", "uid": 0}
NOTIFIED HANDLER restart ntpd
changed: [compute002] => {"changed": true, "checksum": "af804a75af9d7acdc73f35ac807c870bc1c4b119", "dest": "/etc/ntp.conf", "gid": 0, "group": "root", "md5sum": "6de30d188e852b806b3b6fe902673286", "mode": "0444", "owner": "root", "size": 1983, "src": "/root/.ansible/tmp/ansible-tmp-1524911748.4-67805857964392/source", "state": "file", "uid": 0}
NOTIFIED HANDLER restart ntpd
changed: [compute001] => {"changed": true, "checksum": "1cff101d1a86b3b2e39151809d17b232abf8eaa5", "dest": "/etc/ntp.conf", "gid": 0, "group": "root", "md5sum": "508b82e89163e2c216a396161f4e8f23", "mode": "0444", "owner": "root", "size": 1983, "src": "/root/.ansible/tmp/ansible-tmp-1524911748.39-20184395878787/source", "state": "file", "uid": 0}

TASK [ntpd : Install NTP packages] *********************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/ntpd/tasks/main.yml:19
ok: [frontend001] => {"changed": false, "msg": "", "rc": 0, "results": ["ntp-4.2.6p5-12.el6.centos.2.x86_64 providing ntp is already installed"]}
ok: [compute001] => {"changed": false, "msg": "", "rc": 0, "results": ["ntp-4.2.6p5-12.el6.centos.2.x86_64 providing ntp is already installed"]}
ok: [compute002] => {"changed": false, "msg": "", "rc": 0, "results": ["ntp-4.2.6p5-12.el6.centos.2.x86_64 providing ntp is already installed"]}

TASK [ntpd : Enable NTP service at boot] ***************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/ntpd/tasks/main.yml:42
ok: [frontend001] => {"changed": false, "enabled": true, "name": "ntpd", "state": "started"}
ok: [compute001] => {"changed": false, "enabled": true, "name": "ntpd", "state": "started"}
ok: [compute002] => {"changed": false, "enabled": true, "name": "ntpd", "state": "started"}

TASK [pdsh : Install pdsh packages (Debian-family)] ****************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/pdsh/tasks/main.yml:3
skipping: [frontend001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [pdsh : Install pdsh packages (RHEL-family)] ******************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/pdsh/tasks/main.yml:11
ok: [frontend001] => (item=[u'pdsh', u'pdsh-rcmd-ssh', u'pdsh-mod-genders']) => {"changed": false, "item": ["pdsh", "pdsh-rcmd-ssh", "pdsh-mod-genders"], "msg": "", "rc": 0, "results": ["pdsh-2.26-4.el6.x86_64 providing pdsh is already installed", "pdsh-rcmd-ssh-2.26-4.el6.x86_64 providing pdsh-rcmd-ssh is already installed", "pdsh-mod-genders-2.26-4.el6.x86_64 providing pdsh-mod-genders is already installed"]}
ok: [compute002] => (item=[u'pdsh', u'pdsh-rcmd-ssh', u'pdsh-mod-genders']) => {"changed": false, "item": ["pdsh", "pdsh-rcmd-ssh", "pdsh-mod-genders"], "msg": "", "rc": 0, "results": ["pdsh-2.26-4.el6.x86_64 providing pdsh is already installed", "pdsh-rcmd-ssh-2.26-4.el6.x86_64 providing pdsh-rcmd-ssh is already installed", "pdsh-mod-genders-2.26-4.el6.x86_64 providing pdsh-mod-genders is already installed"]}
ok: [compute001] => (item=[u'pdsh', u'pdsh-rcmd-ssh', u'pdsh-mod-genders']) => {"changed": false, "item": ["pdsh", "pdsh-rcmd-ssh", "pdsh-mod-genders"], "msg": "", "rc": 0, "results": ["pdsh-2.26-4.el6.x86_64 providing pdsh is already installed", "pdsh-rcmd-ssh-2.26-4.el6.x86_64 providing pdsh-rcmd-ssh is already installed", "pdsh-mod-genders-2.26-4.el6.x86_64 providing pdsh-mod-genders is already installed"]}

TASK [pdsh : Create genders file for PDSH] *************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/pdsh/tasks/main.yml:21
changed: [frontend001] => {"changed": true, "checksum": "4d9cb5086244d2a2e7b0cfe22261aeb8b7b88bcc", "dest": "/etc/genders", "gid": 0, "group": "root", "md5sum": "ea66ce19ee3c74de41bcc6d9958081a9", "mode": "0444", "owner": "root", "size": 206, "src": "/root/.ansible/tmp/ansible-tmp-1524911751.24-240497364628712/source", "state": "file", "uid": 0}
changed: [compute002] => {"changed": true, "checksum": "4d9cb5086244d2a2e7b0cfe22261aeb8b7b88bcc", "dest": "/etc/genders", "gid": 0, "group": "root", "md5sum": "ea66ce19ee3c74de41bcc6d9958081a9", "mode": "0444", "owner": "root", "size": 206, "src": "/root/.ansible/tmp/ansible-tmp-1524911751.27-105402192484034/source", "state": "file", "uid": 0}
changed: [compute001] => {"changed": true, "checksum": "4d9cb5086244d2a2e7b0cfe22261aeb8b7b88bcc", "dest": "/etc/genders", "gid": 0, "group": "root", "md5sum": "ea66ce19ee3c74de41bcc6d9958081a9", "mode": "0444", "owner": "root", "size": 206, "src": "/root/.ansible/tmp/ansible-tmp-1524911751.25-156933933699012/source", "state": "file", "uid": 0}

TASK [pdsh : Make SSH the default exec method for PDSH] ************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/pdsh/tasks/main.yml:29
ok: [frontend001] => {"changed": false, "checksum": "5929e07c9e9a681ab81ae9da0e1a4c17630509ab", "dest": "/etc/profile.d/pdsh.sh", "gid": 0, "group": "root", "mode": "0444", "owner": "root", "path": "/etc/profile.d/pdsh.sh", "size": 278, "state": "file", "uid": 0}
ok: [compute002] => {"changed": false, "checksum": "5929e07c9e9a681ab81ae9da0e1a4c17630509ab", "dest": "/etc/profile.d/pdsh.sh", "gid": 0, "group": "root", "mode": "0444", "owner": "root", "path": "/etc/profile.d/pdsh.sh", "size": 278, "state": "file", "uid": 0}
ok: [compute001] => {"changed": false, "checksum": "5929e07c9e9a681ab81ae9da0e1a4c17630509ab", "dest": "/etc/profile.d/pdsh.sh", "gid": 0, "group": "root", "mode": "0444", "owner": "root", "path": "/etc/profile.d/pdsh.sh", "size": 278, "state": "file", "uid": 0}

RUNNING HANDLER [ntpd : restart ntpd] ******************************************
changed: [compute001] => {"changed": true, "name": "ntpd", "state": "started"}
changed: [frontend001] => {"changed": true, "name": "ntpd", "state": "started"}
changed: [compute002] => {"changed": true, "name": "ntpd", "state": "started"}

PLAY [Slurm worker nodes Playbook] *********************************************

TASK [setup] *******************************************************************
ok: [frontend001]
ok: [compute001]
ok: [compute002]

TASK [nis : Load distribution-specific parameters] *****************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/main.yml:3
included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/init-RedHat.yml for frontend001, compute001, compute002

TASK [nis : Set NIS common playbook params (CentOS/RHEL)] **********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/init-RedHat.yml:3
ok: [frontend001] => {"ansible_facts": {"nis_client_packages": ["ypbind"], "nis_client_services": ["rpcbind", "ypbind"], "nis_common_packages": ["yp-tools"], "nis_master_packages": ["ypserv"], "nis_master_services": ["rpcbind", "yppasswdd", "ypserv"], "nis_securenets_path": "/var/yp/securenets"}, "changed": false}
ok: [compute001] => {"ansible_facts": {"nis_client_packages": ["ypbind"], "nis_client_services": ["rpcbind", "ypbind"], "nis_common_packages": ["yp-tools"], "nis_master_packages": ["ypserv"], "nis_master_services": ["rpcbind", "yppasswdd", "ypserv"], "nis_securenets_path": "/var/yp/securenets"}, "changed": false}
ok: [compute002] => {"ansible_facts": {"nis_client_packages": ["ypbind"], "nis_client_services": ["rpcbind", "ypbind"], "nis_common_packages": ["yp-tools"], "nis_master_packages": ["ypserv"], "nis_master_services": ["rpcbind", "yppasswdd", "ypserv"], "nis_securenets_path": "/var/yp/securenets"}, "changed": false}

TASK [nis : Pre-load debconf answer to questions (Debian/Ubuntu)] **************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/main.yml:7
skipping: [frontend001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [nis : Deploy additional NIS configuration (Debian/Ubuntu)] ***************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/main.yml:18
skipping: [frontend001] => (item=etc/default/nis)  => {"changed": false, "item": "etc/default/nis", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => (item=etc/default/nis)  => {"changed": false, "item": "etc/default/nis", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [frontend001] => (item=etc/defaultdomain)  => {"changed": false, "item": "etc/defaultdomain", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => (item=etc/defaultdomain)  => {"changed": false, "item": "etc/defaultdomain", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item=etc/default/nis)  => {"changed": false, "item": "etc/default/nis", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item=etc/defaultdomain)  => {"changed": false, "item": "etc/defaultdomain", "skip_reason": "Conditional check failed", "skipped": true}

TASK [nis : Set NIS domain (CentOS/RHEL)] **************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/main.yml:29
changed: [frontend001] => {"backup": "", "changed": true, "msg": "line added"}
changed: [compute001] => {"backup": "", "changed": true, "msg": "line added"}
changed: [compute002] => {"backup": "", "changed": true, "msg": "line added"}

TASK [nis : Install NIS common packages] ***************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/main.yml:39
ok: [compute001] => (item=[u'yp-tools']) => {"changed": false, "item": ["yp-tools"], "msg": "", "rc": 0, "results": ["yp-tools-2.9-12.el6.x86_64 providing yp-tools is already installed"]}
ok: [compute002] => (item=[u'yp-tools']) => {"changed": false, "item": ["yp-tools"], "msg": "", "rc": 0, "results": ["yp-tools-2.9-12.el6.x86_64 providing yp-tools is already installed"]}
ok: [frontend001] => (item=[u'yp-tools']) => {"changed": false, "item": ["yp-tools"], "msg": "", "rc": 0, "results": ["yp-tools-2.9-12.el6.x86_64 providing yp-tools is already installed"]}

TASK [nis : Deploy `ypserv` configuration files] *******************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml:3
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
NOTIFIED HANDLER restart NIS master services
changed: [frontend001] => {"changed": true, "checksum": "49e0f17752c096d141697e720ac380eacbfc933b", "dest": "/var/yp/securenets", "gid": 0, "group": "root", "md5sum": "5d00e4b0591a2f57639e91b7944d7564", "mode": "0400", "owner": "root", "size": 708, "src": "/root/.ansible/tmp/ansible-tmp-1524911755.38-102420917943727/source", "state": "file", "uid": 0}

TASK [nis : Deploy `yppasswdd` configuration file (CentOS/RHEL)] ***************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml:12
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
ok: [frontend001] => {"changed": false, "checksum": "18392b4e3ecc4f9e7832c274663b5f6cee00d3f1", "dest": "/etc/sysconfig/yppasswdd", "gid": 0, "group": "root", "mode": "0444", "owner": "root", "path": "/etc/sysconfig/yppasswdd", "size": 681, "state": "file", "uid": 0}

TASK [nis : Install NIS master server packages] ********************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml:22
skipping: [compute001] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}
ok: [frontend001] => (item=[u'ypserv']) => {"changed": false, "item": ["ypserv"], "msg": "", "rc": 0, "results": ["ypserv-2.19-31.el6.x86_64 providing ypserv is already installed"]}

RUNNING HANDLER [nis : restart NIS master services] ****************************
changed: [frontend001] => (item=rpcbind) => {"changed": true, "item": "rpcbind", "name": "rpcbind", "state": "started"}
changed: [frontend001] => (item=yppasswdd) => {"changed": true, "item": "yppasswdd", "name": "yppasswdd", "state": "started"}
changed: [frontend001] => (item=ypserv) => {"changed": true, "item": "ypserv", "name": "ypserv", "state": "started"}

TASK [nis : Ensure `ypserv` starts at boot] ************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml:33
skipping: [compute001] => (item=rpcbind)  => {"changed": false, "item": "rpcbind", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item=rpcbind)  => {"changed": false, "item": "rpcbind", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => (item=yppasswdd)  => {"changed": false, "item": "yppasswdd", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item=yppasswdd)  => {"changed": false, "item": "yppasswdd", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => (item=ypserv)  => {"changed": false, "item": "ypserv", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item=ypserv)  => {"changed": false, "item": "ypserv", "skip_reason": "Conditional check failed", "skipped": true}
ok: [frontend001] => (item=rpcbind) => {"changed": false, "enabled": true, "item": "rpcbind", "name": "rpcbind", "state": "started"}
ok: [frontend001] => (item=yppasswdd) => {"changed": false, "enabled": true, "item": "yppasswdd", "name": "yppasswdd", "state": "started"}
ok: [frontend001] => (item=ypserv) => {"changed": false, "enabled": true, "item": "ypserv", "name": "ypserv", "state": "started"}

TASK [nis : Update NIS/YP databases (NIS master server)] ***********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml:41
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
changed: [frontend001] => {"changed": true, "cmd": ["make"], "delta": "0:00:00.123732", "end": "2018-04-28 18:35:59.160226", "rc": 0, "start": "2018-04-28 18:35:59.036494", "stderr": "", "stdout": "gmake[1]: Entering directory `/var/yp/elasticluster'\nUpdating passwd.byname...\nUpdating passwd.byuid...\nUpdating group.byname...\nUpdating group.bygid...\nUpdating hosts.byname...\nUpdating hosts.byaddr...\nUpdating netid.byname...\ngmake[1]: Leaving directory `/var/yp/elasticluster'", "stdout_lines": ["gmake[1]: Entering directory `/var/yp/elasticluster'", "Updating passwd.byname...", "Updating passwd.byuid...", "Updating group.byname...", "Updating group.bygid...", "Updating hosts.byname...", "Updating hosts.byaddr...", "Updating netid.byname...", "gmake[1]: Leaving directory `/var/yp/elasticluster'"], "warnings": []}

TASK [nis : Deploy `ypbind` configuration files] *******************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypbind.yml:3
skipping: [frontend001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
NOTIFIED HANDLER restart ypbind
changed: [compute002] => {"changed": true, "checksum": "ccafbafdf6830b9f4c1fc505dda7c510da9d48c6", "dest": "/etc/yp.conf", "gid": 0, "group": "root", "md5sum": "174daa74804c3fb22ad4cdf399256011", "mode": "0400", "owner": "root", "size": 725, "src": "/root/.ansible/tmp/ansible-tmp-1524911759.59-53977240065898/source", "state": "file", "uid": 0}
NOTIFIED HANDLER restart ypbind
changed: [compute001] => {"changed": true, "checksum": "ccafbafdf6830b9f4c1fc505dda7c510da9d48c6", "dest": "/etc/yp.conf", "gid": 0, "group": "root", "md5sum": "174daa74804c3fb22ad4cdf399256011", "mode": "0400", "owner": "root", "size": 725, "src": "/root/.ansible/tmp/ansible-tmp-1524911759.58-166714444699471/source", "state": "file", "uid": 0}

TASK [nis : Ensure NIS/YP is used as a name service] ***************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypbind.yml:12
skipping: [frontend001] => (item=passwd)  => {"changed": false, "item": "passwd", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [frontend001] => (item=group)  => {"changed": false, "item": "group", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [frontend001] => (item=shadow)  => {"changed": false, "item": "shadow", "skip_reason": "Conditional check failed", "skipped": true}
changed: [compute001] => (item=passwd) => {"backup": "", "changed": true, "item": "passwd", "msg": "line replaced"}
changed: [compute002] => (item=passwd) => {"backup": "", "changed": true, "item": "passwd", "msg": "line replaced"}
changed: [compute001] => (item=group) => {"backup": "", "changed": true, "item": "group", "msg": "line replaced"}
changed: [compute002] => (item=group) => {"backup": "", "changed": true, "item": "group", "msg": "line replaced"}
changed: [compute002] => (item=shadow) => {"backup": "", "changed": true, "item": "shadow", "msg": "line replaced"}
changed: [compute001] => (item=shadow) => {"backup": "", "changed": true, "item": "shadow", "msg": "line replaced"}

TASK [nis : Replace `compat` in `/etc/nsswitch.conf` with `files nis`] *********
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypbind.yml:26
skipping: [frontend001] => (item=passwd)  => {"changed": false, "item": "passwd", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [frontend001] => (item=group)  => {"changed": false, "item": "group", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [frontend001] => (item=shadow)  => {"changed": false, "item": "shadow", "skip_reason": "Conditional check failed", "skipped": true}
ok: [compute001] => (item=passwd) => {"changed": false, "item": "passwd", "msg": ""}
ok: [compute002] => (item=passwd) => {"changed": false, "item": "passwd", "msg": ""}
ok: [compute002] => (item=group) => {"changed": false, "item": "group", "msg": ""}
ok: [compute001] => (item=group) => {"changed": false, "item": "group", "msg": ""}
ok: [compute001] => (item=shadow) => {"changed": false, "item": "shadow", "msg": ""}
ok: [compute002] => (item=shadow) => {"changed": false, "item": "shadow", "msg": ""}

TASK [nis : Ensure `ypbind` starts at boot] ************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypbind.yml:37
skipping: [frontend001] => (item=rpcbind)  => {"changed": false, "item": "rpcbind", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [frontend001] => (item=ypbind)  => {"changed": false, "item": "ypbind", "skip_reason": "Conditional check failed", "skipped": true}
ok: [compute001] => (item=rpcbind) => {"changed": false, "enabled": true, "item": "rpcbind", "name": "rpcbind", "state": "started"}
ok: [compute002] => (item=rpcbind) => {"changed": false, "enabled": true, "item": "rpcbind", "name": "rpcbind", "state": "started"}
changed: [compute001] => (item=ypbind) => {"changed": true, "enabled": true, "item": "ypbind", "name": "ypbind", "state": "started"}
changed: [compute002] => (item=ypbind) => {"changed": true, "enabled": true, "item": "ypbind", "name": "ypbind", "state": "started"}

RUNNING HANDLER [nis : restart ypbind] *****************************************
changed: [compute002] => (item=rpcbind) => {"changed": true, "item": "rpcbind", "name": "rpcbind", "state": "started"}
changed: [compute001] => (item=rpcbind) => {"changed": true, "item": "rpcbind", "name": "rpcbind", "state": "started"}
changed: [compute002] => (item=ypbind) => {"changed": true, "item": "ypbind", "name": "ypbind", "state": "started"}
changed: [compute001] => (item=ypbind) => {"changed": true, "item": "ypbind", "name": "ypbind", "state": "started"}

TASK [nfs-client : install NFS client software (Debian/Ubuntu)] ****************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-client/tasks/main.yml:3
skipping: [frontend001] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}

TASK [nfs-client : install NFS client software (RHEL-compatible)] **************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-client/tasks/main.yml:15
ok: [frontend001] => (item=[u'nfs-utils', u'nfs4-acl-tools']) => {"changed": false, "item": ["nfs-utils", "nfs4-acl-tools"], "msg": "", "rc": 0, "results": ["nfs-utils-1:1.2.3-75.el6_9.x86_64 providing nfs-utils is already installed", "nfs4-acl-tools-0.3.3-8.el6.x86_64 providing nfs4-acl-tools is already installed"]}
ok: [compute001] => (item=[u'nfs-utils', u'nfs4-acl-tools']) => {"changed": false, "item": ["nfs-utils", "nfs4-acl-tools"], "msg": "", "rc": 0, "results": ["nfs-utils-1:1.2.3-75.el6_9.x86_64 providing nfs-utils is already installed", "nfs4-acl-tools-0.3.3-8.el6.x86_64 providing nfs4-acl-tools is already installed"]}
ok: [compute002] => (item=[u'nfs-utils', u'nfs4-acl-tools']) => {"changed": false, "item": ["nfs-utils", "nfs4-acl-tools"], "msg": "", "rc": 0, "results": ["nfs-utils-1:1.2.3-75.el6_9.x86_64 providing nfs-utils is already installed", "nfs4-acl-tools-0.3.3-8.el6.x86_64 providing nfs4-acl-tools is already installed"]}

TASK [nfs-client : Ensure `rpcbind` is running (Debian)] ***********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-client/tasks/main.yml:28
skipping: [frontend001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [nfs-client : Ensure `rpcbind` is running (RHEL-compatible)] **************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-client/tasks/main.yml:35
ok: [compute001] => {"changed": false, "enabled": true, "name": "rpcbind", "state": "started"}
ok: [frontend001] => {"changed": false, "enabled": true, "name": "rpcbind", "state": "started"}
ok: [compute002] => {"changed": false, "enabled": true, "name": "rpcbind", "state": "started"}

TASK [nfs-client : Ensure `portmap` is running (Ubuntu prior to 14.04)] ********
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-client/tasks/main.yml:42
skipping: [frontend001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [nfs-client : Ensure `rpcbind` is running (Ubuntu 14.04 or newer)] ********
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-client/tasks/main.yml:49
skipping: [frontend001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [nfs-client : Mount NFS filesystems] **************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-client/tasks/main.yml:57
included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-client/tasks/nfsmount.yml for frontend001, compute001, compute002

TASK [nfs-client : ensure /sfs directory exists] *******************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-client/tasks/nfsmount.yml:3
changed: [frontend001] => {"changed": true, "gid": 0, "group": "root", "mode": "0755", "owner": "root", "path": "/sfs", "size": 4096, "state": "directory", "uid": 0}
changed: [compute001] => {"changed": true, "gid": 0, "group": "root", "mode": "0755", "owner": "root", "path": "/sfs", "size": 4096, "state": "directory", "uid": 0}
changed: [compute002] => {"changed": true, "gid": 0, "group": "root", "mode": "0755", "owner": "root", "path": "/sfs", "size": 4096, "state": "directory", "uid": 0}

TASK [nfs-client : add to /etc/fstab] ******************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-client/tasks/nfsmount.yml:8
changed: [compute001] => {"changed": true, "dump": "0", "fstab": "/etc/fstab", "fstype": "nfs", "name": "/sfs", "opts": "rw,async", "passno": "0", "src": "sfs-nas1.cn-north-1.myhuaweicloud.com:/share-480fca1b"}
changed: [frontend001] => {"changed": true, "dump": "0", "fstab": "/etc/fstab", "fstype": "nfs", "name": "/sfs", "opts": "rw,async", "passno": "0", "src": "sfs-nas1.cn-north-1.myhuaweicloud.com:/share-480fca1b"}
changed: [compute002] => {"changed": true, "dump": "0", "fstab": "/etc/fstab", "fstype": "nfs", "name": "/sfs", "opts": "rw,async", "passno": "0", "src": "sfs-nas1.cn-north-1.myhuaweicloud.com:/share-480fca1b"}

TASK [autofs : Load distribution-specific parameters] **************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/autofs/tasks/main.yml:8
included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/autofs/tasks/init-RedHat.yml for frontend001, compute001, compute002

TASK [autofs : Provide RHEL-specific values] ***********************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/autofs/tasks/init-RedHat.yml:6
ok: [frontend001] => {"ansible_facts": {"autofs_packages": ["autofs"], "autofs_service": "autofs", "autofs_username": "root"}, "changed": false}
ok: [compute001] => {"ansible_facts": {"autofs_packages": ["autofs"], "autofs_service": "autofs", "autofs_username": "root"}, "changed": false}
ok: [compute002] => {"ansible_facts": {"autofs_packages": ["autofs"], "autofs_service": "autofs", "autofs_username": "root"}, "changed": false}

TASK [autofs : Deploy autofs configuration] ************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/autofs/tasks/main.yml:12
changed: [compute001] => (item=etc/auto.master) => {"changed": true, "checksum": "758d13b8a76f743e400c0badc6157e8327f1a8f1", "dest": "/etc/auto.master", "gid": 0, "group": "root", "item": "etc/auto.master", "md5sum": "e3d05b4f4a066db7039fed20a9574a94", "mode": "0444", "owner": "root", "size": 540, "src": "/root/.ansible/tmp/ansible-tmp-1524911771.08-52143626787350/source", "state": "file", "uid": 0}
changed: [frontend001] => (item=etc/auto.master) => {"changed": true, "checksum": "758d13b8a76f743e400c0badc6157e8327f1a8f1", "dest": "/etc/auto.master", "gid": 0, "group": "root", "item": "etc/auto.master", "md5sum": "e3d05b4f4a066db7039fed20a9574a94", "mode": "0444", "owner": "root", "size": 540, "src": "/root/.ansible/tmp/ansible-tmp-1524911771.07-235881929244370/source", "state": "file", "uid": 0}
changed: [compute002] => (item=etc/auto.master) => {"changed": true, "checksum": "758d13b8a76f743e400c0badc6157e8327f1a8f1", "dest": "/etc/auto.master", "gid": 0, "group": "root", "item": "etc/auto.master", "md5sum": "e3d05b4f4a066db7039fed20a9574a94", "mode": "0444", "owner": "root", "size": 540, "src": "/root/.ansible/tmp/ansible-tmp-1524911771.1-235588955049534/source", "state": "file", "uid": 0}
changed: [compute001] => (item=etc/auto.home) => {"changed": true, "checksum": "df9645aa3e6a41b1aece0b5867b79d0c9c16f430", "dest": "/etc/auto.home", "gid": 0, "group": "root", "item": "etc/auto.home", "md5sum": "271d38d13da93386ff2c00c836c5608b", "mode": "0444", "owner": "root", "size": 852, "src": "/root/.ansible/tmp/ansible-tmp-1524911771.66-247276551883616/source", "state": "file", "uid": 0}
changed: [frontend001] => (item=etc/auto.home) => {"changed": true, "checksum": "df9645aa3e6a41b1aece0b5867b79d0c9c16f430", "dest": "/etc/auto.home", "gid": 0, "group": "root", "item": "etc/auto.home", "md5sum": "271d38d13da93386ff2c00c836c5608b", "mode": "0444", "owner": "root", "size": 852, "src": "/root/.ansible/tmp/ansible-tmp-1524911771.66-23814841310173/source", "state": "file", "uid": 0}
changed: [compute002] => (item=etc/auto.home) => {"changed": true, "checksum": "df9645aa3e6a41b1aece0b5867b79d0c9c16f430", "dest": "/etc/auto.home", "gid": 0, "group": "root", "item": "etc/auto.home", "md5sum": "271d38d13da93386ff2c00c836c5608b", "mode": "0444", "owner": "root", "size": 852, "src": "/root/.ansible/tmp/ansible-tmp-1524911771.69-180040690707162/source", "state": "file", "uid": 0}

TASK [autofs : Deploy autofs mount script for NFSv4 (Debian/Ubuntu)] ***********
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/autofs/tasks/main.yml:24
skipping: [frontend001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [autofs : Install Autofs] *************************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/autofs/tasks/main.yml:34
ok: [frontend001] => (item=[u'autofs']) => {"changed": false, "item": ["autofs"], "msg": "", "rc": 0, "results": ["autofs-1:5.0.5-133.el6_9.x86_64 providing autofs is already installed"]}
ok: [compute001] => (item=[u'autofs']) => {"changed": false, "item": ["autofs"], "msg": "", "rc": 0, "results": ["autofs-1:5.0.5-133.el6_9.x86_64 providing autofs is already installed"]}
ok: [compute002] => (item=[u'autofs']) => {"changed": false, "item": ["autofs"], "msg": "", "rc": 0, "results": ["autofs-1:5.0.5-133.el6_9.x86_64 providing autofs is already installed"]}

TASK [autofs : Ensure autofs is running and starts at boot] ********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/autofs/tasks/main.yml:41
ok: [compute001] => {"changed": false, "enabled": true, "name": "autofs", "state": "started"}
ok: [frontend001] => {"changed": false, "enabled": true, "name": "autofs", "state": "started"}
ok: [compute002] => {"changed": false, "enabled": true, "name": "autofs", "state": "started"}

TASK [slurm-common : Load distribution-specific parameters] ********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:3
included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/init-RedHat.yml for frontend001, compute001, compute002

TASK [slurm-common : Set SLURM common playbook params (RHEL 7.x compatible)] ***
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/init-RedHat.yml:3
ok: [frontend001] => {"ansible_facts": {"slurm_pid_dir": "/var/run"}, "changed": false}
ok: [compute001] => {"ansible_facts": {"slurm_pid_dir": "/var/run"}, "changed": false}
ok: [compute002] => {"ansible_facts": {"slurm_pid_dir": "/var/run"}, "changed": false}

TASK [slurm-common : Create `slurm` system group] ******************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:16
ok: [compute001] => {"changed": false, "gid": 498, "name": "slurm", "state": "present", "system": true}
ok: [compute002] => {"changed": false, "gid": 498, "name": "slurm", "state": "present", "system": true}
ok: [frontend001] => {"changed": false, "gid": 498, "name": "slurm", "state": "present", "system": true}

TASK [slurm-common : Create `slurm` system user] *******************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:24
ok: [compute002] => {"append": false, "changed": false, "comment": "", "group": 498, "home": "/home/slurm", "move_home": false, "name": "slurm", "shell": "/bin/bash", "state": "present", "uid": 498}
ok: [compute001] => {"append": false, "changed": false, "comment": "", "group": 498, "home": "/home/slurm", "move_home": false, "name": "slurm", "shell": "/bin/bash", "state": "present", "uid": 498}
ok: [frontend001] => {"append": false, "changed": false, "comment": "", "group": 498, "home": "/home/slurm", "move_home": false, "name": "slurm", "shell": "/bin/bash", "state": "present", "uid": 498}

TASK [slurm-common : Create work directory {{item}}] ***************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:33
ok: [frontend001] => (item=/etc/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/etc/slurm", "mode": "0755", "owner": "slurm", "path": "/etc/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [compute001] => (item=/etc/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/etc/slurm", "mode": "0755", "owner": "slurm", "path": "/etc/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [compute002] => (item=/etc/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/etc/slurm", "mode": "0755", "owner": "slurm", "path": "/etc/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [frontend001] => (item=/var/lib/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/lib/slurm", "mode": "0755", "owner": "slurm", "path": "/var/lib/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [compute002] => (item=/var/lib/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/lib/slurm", "mode": "0755", "owner": "slurm", "path": "/var/lib/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [compute001] => (item=/var/lib/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/lib/slurm", "mode": "0755", "owner": "slurm", "path": "/var/lib/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [frontend001] => (item=/var/lib/slurm/checkpoint) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/lib/slurm/checkpoint", "mode": "0755", "owner": "slurm", "path": "/var/lib/slurm/checkpoint", "size": 4096, "state": "directory", "uid": 498}
ok: [compute001] => (item=/var/lib/slurm/checkpoint) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/lib/slurm/checkpoint", "mode": "0755", "owner": "slurm", "path": "/var/lib/slurm/checkpoint", "size": 4096, "state": "directory", "uid": 498}
ok: [compute002] => (item=/var/lib/slurm/checkpoint) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/lib/slurm/checkpoint", "mode": "0755", "owner": "slurm", "path": "/var/lib/slurm/checkpoint", "size": 4096, "state": "directory", "uid": 498}
ok: [frontend001] => (item=/var/lib/slurm/slurmctld) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/lib/slurm/slurmctld", "mode": "0755", "owner": "slurm", "path": "/var/lib/slurm/slurmctld", "size": 4096, "state": "directory", "uid": 498}
ok: [compute001] => (item=/var/lib/slurm/slurmctld) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/lib/slurm/slurmctld", "mode": "0755", "owner": "slurm", "path": "/var/lib/slurm/slurmctld", "size": 4096, "state": "directory", "uid": 498}
ok: [compute002] => (item=/var/lib/slurm/slurmctld) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/lib/slurm/slurmctld", "mode": "0755", "owner": "slurm", "path": "/var/lib/slurm/slurmctld", "size": 4096, "state": "directory", "uid": 498}
ok: [frontend001] => (item=/var/lib/slurm/slurmd) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/lib/slurm/slurmd", "mode": "0755", "owner": "slurm", "path": "/var/lib/slurm/slurmd", "size": 4096, "state": "directory", "uid": 498}
ok: [compute001] => (item=/var/lib/slurm/slurmd) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/lib/slurm/slurmd", "mode": "0755", "owner": "slurm", "path": "/var/lib/slurm/slurmd", "size": 4096, "state": "directory", "uid": 498}
ok: [compute002] => (item=/var/lib/slurm/slurmd) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/lib/slurm/slurmd", "mode": "0755", "owner": "slurm", "path": "/var/lib/slurm/slurmd", "size": 4096, "state": "directory", "uid": 498}
ok: [frontend001] => (item=/var/log/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/log/slurm", "mode": "0755", "owner": "slurm", "path": "/var/log/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [compute001] => (item=/var/log/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/log/slurm", "mode": "0755", "owner": "slurm", "path": "/var/log/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [compute002] => (item=/var/log/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/log/slurm", "mode": "0755", "owner": "slurm", "path": "/var/log/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [frontend001] => (item=/var/run/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/run/slurm", "mode": "0755", "owner": "slurm", "path": "/var/run/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [compute001] => (item=/var/run/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/run/slurm", "mode": "0755", "owner": "slurm", "path": "/var/run/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [compute002] => (item=/var/run/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/run/slurm", "mode": "0755", "owner": "slurm", "path": "/var/run/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [frontend001] => (item=/var/spool/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/spool/slurm", "mode": "0755", "owner": "slurm", "path": "/var/spool/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [compute001] => (item=/var/spool/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/spool/slurm", "mode": "0755", "owner": "slurm", "path": "/var/spool/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [compute002] => (item=/var/spool/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/spool/slurm", "mode": "0755", "owner": "slurm", "path": "/var/spool/slurm", "size": 4096, "state": "directory", "uid": 498}

TASK [slurm-common : Make compatibility symlinks] ******************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:56
skipping: [frontend001] => (item={u'to': u'/etc/slurm-llnl', u'from': u'/etc/slurm'})  => {"changed": false, "item": {"from": "/etc/slurm", "to": "/etc/slurm-llnl"}, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [frontend001] => (item={u'to': u'/var/lib/slurm-llnl', u'from': u'/var/lib/slurm'})  => {"changed": false, "item": {"from": "/var/lib/slurm", "to": "/var/lib/slurm-llnl"}, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => (item={u'to': u'/etc/slurm-llnl', u'from': u'/etc/slurm'})  => {"changed": false, "item": {"from": "/etc/slurm", "to": "/etc/slurm-llnl"}, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [frontend001] => (item={u'to': u'/var/log/slurm-llnl', u'from': u'/var/log/slurm'})  => {"changed": false, "item": {"from": "/var/log/slurm", "to": "/var/log/slurm-llnl"}, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => (item={u'to': u'/var/lib/slurm-llnl', u'from': u'/var/lib/slurm'})  => {"changed": false, "item": {"from": "/var/lib/slurm", "to": "/var/lib/slurm-llnl"}, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [frontend001] => (item={u'to': u'/var/run/slurm-llnl', u'from': u'/var/run/slurm'})  => {"changed": false, "item": {"from": "/var/run/slurm", "to": "/var/run/slurm-llnl"}, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => (item={u'to': u'/var/log/slurm-llnl', u'from': u'/var/log/slurm'})  => {"changed": false, "item": {"from": "/var/log/slurm", "to": "/var/log/slurm-llnl"}, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item={u'to': u'/etc/slurm-llnl', u'from': u'/etc/slurm'})  => {"changed": false, "item": {"from": "/etc/slurm", "to": "/etc/slurm-llnl"}, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item={u'to': u'/var/lib/slurm-llnl', u'from': u'/var/lib/slurm'})  => {"changed": false, "item": {"from": "/var/lib/slurm", "to": "/var/lib/slurm-llnl"}, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => (item={u'to': u'/var/run/slurm-llnl', u'from': u'/var/run/slurm'})  => {"changed": false, "item": {"from": "/var/run/slurm", "to": "/var/run/slurm-llnl"}, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item={u'to': u'/var/log/slurm-llnl', u'from': u'/var/log/slurm'})  => {"changed": false, "item": {"from": "/var/log/slurm", "to": "/var/log/slurm-llnl"}, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item={u'to': u'/var/run/slurm-llnl', u'from': u'/var/run/slurm'})  => {"changed": false, "item": {"from": "/var/run/slurm", "to": "/var/run/slurm-llnl"}, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-common : Enable Copr SLURM repo by verdurin] ***********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:73
changed: [frontend001] => {"changed": true, "checksum": "18960fef3cdfefe9d932e02cb1791c9c43780896", "dest": "/etc/yum.repos.d/copr-slurm.repo", "gid": 0, "group": "root", "md5sum": "9d472c764f0985eec79d4192598f2d06", "mode": "0444", "owner": "root", "size": 615, "src": "/root/.ansible/tmp/ansible-tmp-1524911775.76-167564440616246/source", "state": "file", "uid": 0}
changed: [compute001] => {"changed": true, "checksum": "18960fef3cdfefe9d932e02cb1791c9c43780896", "dest": "/etc/yum.repos.d/copr-slurm.repo", "gid": 0, "group": "root", "md5sum": "9d472c764f0985eec79d4192598f2d06", "mode": "0444", "owner": "root", "size": 615, "src": "/root/.ansible/tmp/ansible-tmp-1524911775.78-184387551390286/source", "state": "file", "uid": 0}
changed: [compute002] => {"changed": true, "checksum": "18960fef3cdfefe9d932e02cb1791c9c43780896", "dest": "/etc/yum.repos.d/copr-slurm.repo", "gid": 0, "group": "root", "md5sum": "9d472c764f0985eec79d4192598f2d06", "mode": "0444", "owner": "root", "size": 615, "src": "/root/.ansible/tmp/ansible-tmp-1524911775.8-128925466267822/source", "state": "file", "uid": 0}

TASK [slurm-common : Prevent MUNGE from starting] ******************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:20
skipping: [frontend001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-common : Install MUNGE (Debian/Ubuntu)] ****************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:31
skipping: [frontend001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-common : Install MUNGE (RHEL-compatible)] **************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:43
changed: [frontend001] => {"changed": true, "msg": "Warning: RPMDB altered outside of yum.\n", "rc": 0, "results": ["Loaded plugins: fastestmirror, security\nSetting up Install Process\nLoading mirror speeds from cached hostfile\nResolving Dependencies\n--> Running transaction check\n---> Package slurm-munge.x86_64 0:17.02.7-1.el6 will be installed\n--> Processing Dependency: slurm for package: slurm-munge-17.02.7-1.el6.x86_64\n--> Running transaction check\n---> Package slurm.x86_64 0:17.02.7-1.el6 will be installed\n--> Processing Dependency: slurm-plugins for package: slurm-17.02.7-1.el6.x86_64\n--> Running transaction check\n---> Package slurm-plugins.x86_64 0:17.02.7-1.el6 will be installed\n--> Finished Dependency Resolution\n\nDependencies Resolved\n\n================================================================================\n Package             Arch         Version               Repository         Size\n================================================================================\nInstalling:\n slurm-munge         x86_64       17.02.7-1.el6         local-slurm        16 k\nInstalling for dependencies:\n slurm               x86_64       17.02.7-1.el6         local-slurm        27 M\n slurm-plugins       x86_64       17.02.7-1.el6         local-slurm       1.1 M\n\nTransaction Summary\n================================================================================\nInstall       3 Package(s)\n\nTotal download size: 28 M\nInstalled size: 98 M\nDownloading Packages:\n--------------------------------------------------------------------------------\nTotal                                            64 MB/s |  28 MB     00:00     \nRunning rpm_check_debug\nRunning Transaction Test\nTransaction Test Succeeded\nRunning Transaction\n\r  Installing : slurm-plugins-17.02.7-1.el6.x86_64                           1/3 \n\r  Installing : slurm-17.02.7-1.el6.x86_64                                   2/3 \n\r  Installing : slurm-munge-17.02.7-1.el6.x86_64                             3/3 \n\r  Verifying  : slurm-plugins-17.02.7-1.el6.x86_64                           1/3 \n\r  Verifying  : slurm-17.02.7-1.el6.x86_64                                   2/3 \n\r  Verifying  : slurm-munge-17.02.7-1.el6.x86_64                             3/3 \n\nInstalled:\n  slurm-munge.x86_64 0:17.02.7-1.el6                                            \n\nDependency Installed:\n  slurm.x86_64 0:17.02.7-1.el6       slurm-plugins.x86_64 0:17.02.7-1.el6      \n\nComplete!\n"]}
changed: [compute002] => {"changed": true, "msg": "Warning: RPMDB altered outside of yum.\n", "rc": 0, "results": ["Loaded plugins: fastestmirror, security\nSetting up Install Process\nLoading mirror speeds from cached hostfile\nResolving Dependencies\n--> Running transaction check\n---> Package slurm-munge.x86_64 0:17.02.7-1.el6 will be installed\n--> Processing Dependency: slurm for package: slurm-munge-17.02.7-1.el6.x86_64\n--> Running transaction check\n---> Package slurm.x86_64 0:17.02.7-1.el6 will be installed\n--> Processing Dependency: slurm-plugins for package: slurm-17.02.7-1.el6.x86_64\n--> Running transaction check\n---> Package slurm-plugins.x86_64 0:17.02.7-1.el6 will be installed\n--> Finished Dependency Resolution\n\nDependencies Resolved\n\n================================================================================\n Package             Arch         Version               Repository         Size\n================================================================================\nInstalling:\n slurm-munge         x86_64       17.02.7-1.el6         local-slurm        16 k\nInstalling for dependencies:\n slurm               x86_64       17.02.7-1.el6         local-slurm        27 M\n slurm-plugins       x86_64       17.02.7-1.el6         local-slurm       1.1 M\n\nTransaction Summary\n================================================================================\nInstall       3 Package(s)\n\nTotal download size: 28 M\nInstalled size: 98 M\nDownloading Packages:\n--------------------------------------------------------------------------------\nTotal                                            65 MB/s |  28 MB     00:00     \nRunning rpm_check_debug\nRunning Transaction Test\nTransaction Test Succeeded\nRunning Transaction\n\r  Installing : slurm-plugins-17.02.7-1.el6.x86_64                           1/3 \n\r  Installing : slurm-17.02.7-1.el6.x86_64                                   2/3 \n\r  Installing : slurm-munge-17.02.7-1.el6.x86_64                             3/3 \n\r  Verifying  : slurm-plugins-17.02.7-1.el6.x86_64                           1/3 \n\r  Verifying  : slurm-17.02.7-1.el6.x86_64                                   2/3 \n\r  Verifying  : slurm-munge-17.02.7-1.el6.x86_64                             3/3 \n\nInstalled:\n  slurm-munge.x86_64 0:17.02.7-1.el6                                            \n\nDependency Installed:\n  slurm.x86_64 0:17.02.7-1.el6       slurm-plugins.x86_64 0:17.02.7-1.el6      \n\nComplete!\n"]}
changed: [compute001] => {"changed": true, "msg": "Warning: RPMDB altered outside of yum.\n", "rc": 0, "results": ["Loaded plugins: fastestmirror, security\nSetting up Install Process\nLoading mirror speeds from cached hostfile\nResolving Dependencies\n--> Running transaction check\n---> Package slurm-munge.x86_64 0:17.02.7-1.el6 will be installed\n--> Processing Dependency: slurm for package: slurm-munge-17.02.7-1.el6.x86_64\n--> Running transaction check\n---> Package slurm.x86_64 0:17.02.7-1.el6 will be installed\n--> Processing Dependency: slurm-plugins for package: slurm-17.02.7-1.el6.x86_64\n--> Running transaction check\n---> Package slurm-plugins.x86_64 0:17.02.7-1.el6 will be installed\n--> Finished Dependency Resolution\n\nDependencies Resolved\n\n================================================================================\n Package             Arch         Version               Repository         Size\n================================================================================\nInstalling:\n slurm-munge         x86_64       17.02.7-1.el6         local-slurm        16 k\nInstalling for dependencies:\n slurm               x86_64       17.02.7-1.el6         local-slurm        27 M\n slurm-plugins       x86_64       17.02.7-1.el6         local-slurm       1.1 M\n\nTransaction Summary\n================================================================================\nInstall       3 Package(s)\n\nTotal download size: 28 M\nInstalled size: 98 M\nDownloading Packages:\n--------------------------------------------------------------------------------\nTotal                                            65 MB/s |  28 MB     00:00     \nRunning rpm_check_debug\nRunning Transaction Test\nTransaction Test Succeeded\nRunning Transaction\n\r  Installing : slurm-plugins-17.02.7-1.el6.x86_64                           1/3 \n\r  Installing : slurm-17.02.7-1.el6.x86_64                                   2/3 \n\r  Installing : slurm-munge-17.02.7-1.el6.x86_64                             3/3 \n\r  Verifying  : slurm-plugins-17.02.7-1.el6.x86_64                           1/3 \n\r  Verifying  : slurm-17.02.7-1.el6.x86_64                                   2/3 \n\r  Verifying  : slurm-munge-17.02.7-1.el6.x86_64                             3/3 \n\nInstalled:\n  slurm-munge.x86_64 0:17.02.7-1.el6                                            \n\nDependency Installed:\n  slurm.x86_64 0:17.02.7-1.el6       slurm-plugins.x86_64 0:17.02.7-1.el6      \n\nComplete!\n"]}

TASK [slurm-common : Allow MUNGE to start] *************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:57
skipping: [frontend001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-common : Add `--syslog` option to `munged` startup] ****************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:66
skipping: [frontend001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-common : Reload systemd configuration] *****************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:77
skipping: [frontend001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-common : Add `--syslog` startup options for `munged` (Ubuntu 14.04)] ***
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:86
skipping: [frontend001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-common : Configure MUNGE] ******************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:101
ok: [frontend001] => {"changed": false, "checksum": "222afe887f907039b799b394715664ffa584fec5", "dest": "/etc/munge/munge.key", "gid": 497, "group": "munge", "mode": "0400", "owner": "munge", "path": "/etc/munge/munge.key", "size": 1024, "state": "file", "uid": 497}
ok: [compute001] => {"changed": false, "checksum": "222afe887f907039b799b394715664ffa584fec5", "dest": "/etc/munge/munge.key", "gid": 497, "group": "munge", "mode": "0400", "owner": "munge", "path": "/etc/munge/munge.key", "size": 1024, "state": "file", "uid": 497}
ok: [compute002] => {"changed": false, "checksum": "222afe887f907039b799b394715664ffa584fec5", "dest": "/etc/munge/munge.key", "gid": 497, "group": "munge", "mode": "0400", "owner": "munge", "path": "/etc/munge/munge.key", "size": 1024, "state": "file", "uid": 497}

TASK [slurm-common : Ensure the MUNGE service is running] **********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:114
ok: [frontend001] => {"changed": false, "enabled": true, "name": "munge", "state": "started"}
ok: [compute001] => {"changed": false, "enabled": true, "name": "munge", "state": "started"}
ok: [compute002] => {"changed": false, "enabled": true, "name": "munge", "state": "started"}

TASK [slurm-common : Deploy SLURM configuration file] **************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:86
changed: [compute001] => {"changed": true, "checksum": "e489b1ef82e020606e61f19533556154c8ef3d63", "dest": "/etc/slurm/slurm.conf", "gid": 0, "group": "root", "md5sum": "3183bae76af6897c16dce25c74d3a5bb", "mode": "0444", "owner": "root", "size": 4157, "src": "/root/.ansible/tmp/ansible-tmp-1524911792.15-59273618557679/source", "state": "file", "uid": 0}
changed: [frontend001] => {"changed": true, "checksum": "e489b1ef82e020606e61f19533556154c8ef3d63", "dest": "/etc/slurm/slurm.conf", "gid": 0, "group": "root", "md5sum": "3183bae76af6897c16dce25c74d3a5bb", "mode": "0444", "owner": "root", "size": 4157, "src": "/root/.ansible/tmp/ansible-tmp-1524911792.14-110033284336947/source", "state": "file", "uid": 0}
changed: [compute002] => {"changed": true, "checksum": "e489b1ef82e020606e61f19533556154c8ef3d63", "dest": "/etc/slurm/slurm.conf", "gid": 0, "group": "root", "md5sum": "3183bae76af6897c16dce25c74d3a5bb", "mode": "0444", "owner": "root", "size": 4157, "src": "/root/.ansible/tmp/ansible-tmp-1524911792.17-225151303507519/source", "state": "file", "uid": 0}

TASK [slurm-common : Deploy script to suspend cloud instance] ******************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:96
changed: [frontend001] => {"changed": true, "checksum": "8377db742e981ff95e4e9f833eca530fe3248f44", "dest": "/etc/slurm/slurm.suspend.sh", "gid": 0, "group": "root", "md5sum": "1f5ceccbb484e295c67bbcdb3710ac5f", "mode": "0555", "owner": "root", "size": 604, "src": "/root/.ansible/tmp/ansible-tmp-1524911792.82-99303374932563/source", "state": "file", "uid": 0}
changed: [compute001] => {"changed": true, "checksum": "8377db742e981ff95e4e9f833eca530fe3248f44", "dest": "/etc/slurm/slurm.suspend.sh", "gid": 0, "group": "root", "md5sum": "1f5ceccbb484e295c67bbcdb3710ac5f", "mode": "0555", "owner": "root", "size": 604, "src": "/root/.ansible/tmp/ansible-tmp-1524911792.83-250359829382837/source", "state": "file", "uid": 0}
changed: [compute002] => {"changed": true, "checksum": "8377db742e981ff95e4e9f833eca530fe3248f44", "dest": "/etc/slurm/slurm.suspend.sh", "gid": 0, "group": "root", "md5sum": "1f5ceccbb484e295c67bbcdb3710ac5f", "mode": "0555", "owner": "root", "size": 604, "src": "/root/.ansible/tmp/ansible-tmp-1524911792.86-31218169833857/source", "state": "file", "uid": 0}

TASK [slurm-common : Deploy script to resume cloud instance] *******************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:106
changed: [frontend001] => {"changed": true, "checksum": "9abb021ec034c1875481fbe2e5ed2975469997aa", "dest": "/etc/slurm/slurm.resume.sh", "gid": 0, "group": "root", "md5sum": "c9c92c1d3a601152bc7c6fdaa508ac5b", "mode": "0555", "owner": "root", "size": 631, "src": "/root/.ansible/tmp/ansible-tmp-1524911793.48-111122404349376/source", "state": "file", "uid": 0}
changed: [compute001] => {"changed": true, "checksum": "9abb021ec034c1875481fbe2e5ed2975469997aa", "dest": "/etc/slurm/slurm.resume.sh", "gid": 0, "group": "root", "md5sum": "c9c92c1d3a601152bc7c6fdaa508ac5b", "mode": "0555", "owner": "root", "size": 631, "src": "/root/.ansible/tmp/ansible-tmp-1524911793.5-14375283458688/source", "state": "file", "uid": 0}
changed: [compute002] => {"changed": true, "checksum": "9abb021ec034c1875481fbe2e5ed2975469997aa", "dest": "/etc/slurm/slurm.resume.sh", "gid": 0, "group": "root", "md5sum": "c9c92c1d3a601152bc7c6fdaa508ac5b", "mode": "0555", "owner": "root", "size": 631, "src": "/root/.ansible/tmp/ansible-tmp-1524911793.52-133883685709593/source", "state": "file", "uid": 0}

TASK [slurm-common : Install support packages (Debian/Ubuntu)] *****************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:116
skipping: [frontend001] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-common : Install SLURM packages (RHEL-compatible)] *****************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:127
skipping: [frontend001] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-client : Install required SLURM packages (Debian/Ubuntu)] **********
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-client/tasks/main.yml:4
skipping: [frontend001] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-client : Install required SLURM packages (older Debian/Ubuntu)] ****
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-client/tasks/main.yml:23
skipping: [frontend001] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-client : service] **************************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-client/tasks/main.yml:35
skipping: [frontend001] => (item=slurm-llnl)  => {"changed": false, "item": "slurm-llnl", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => (item=slurm-llnl)  => {"changed": false, "item": "slurm-llnl", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item=slurm-llnl)  => {"changed": false, "item": "slurm-llnl", "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-client : Install SLURM packages (RHEL-compatible)] *****************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-client/tasks/main.yml:47
changed: [compute001] => (item=[u'slurm', u'slurm-devel', u'slurm-perlapi', u'slurm-contribs', u'slurm-torque']) => {"changed": true, "item": ["slurm", "slurm-devel", "slurm-perlapi", "slurm-contribs", "slurm-torque"], "msg": "", "rc": 0, "results": ["slurm-17.02.7-1.el6.x86_64 providing slurm is already installed", "Loaded plugins: fastestmirror, security\nSetting up Install Process\nLoading mirror speeds from cached hostfile\nResolving Dependencies\n--> Running transaction check\n---> Package slurm-contribs.x86_64 0:17.02.7-1.el6 will be installed\n---> Package slurm-devel.x86_64 0:17.02.7-1.el6 will be installed\n---> Package slurm-perlapi.x86_64 0:17.02.7-1.el6 will be installed\n---> Package slurm-torque.x86_64 0:17.02.7-1.el6 will be installed\n--> Finished Dependency Resolution\n\nDependencies Resolved\n\n================================================================================\n Package              Arch         Version              Repository         Size\n================================================================================\nInstalling:\n slurm-contribs       x86_64       17.02.7-1.el6        local-slurm        17 k\n slurm-devel          x86_64       17.02.7-1.el6        local-slurm       143 k\n slurm-perlapi        x86_64       17.02.7-1.el6        local-slurm       456 k\n slurm-torque         x86_64       17.02.7-1.el6        local-slurm        37 k\n\nTransaction Summary\n================================================================================\nInstall       4 Package(s)\n\nTotal download size: 653 k\nInstalled size: 2.5 M\nDownloading Packages:\n--------------------------------------------------------------------------------\nTotal                                            47 MB/s | 653 kB     00:00     \nRunning rpm_check_debug\nRunning Transaction Test\nTransaction Test Succeeded\nRunning Transaction\n\r  Installing : slurm-perlapi-17.02.7-1.el6.x86_64                           1/4 \n\r  Installing : slurm-torque-17.02.7-1.el6.x86_64                            2/4 \n\r  Installing : slurm-contribs-17.02.7-1.el6.x86_64                          3/4 \n\r  Installing : slurm-devel-17.02.7-1.el6.x86_64                             4/4 \n\r  Verifying  : slurm-perlapi-17.02.7-1.el6.x86_64                           1/4 \n\r  Verifying  : slurm-devel-17.02.7-1.el6.x86_64                             2/4 \n\r  Verifying  : slurm-torque-17.02.7-1.el6.x86_64                            3/4 \n\r  Verifying  : slurm-contribs-17.02.7-1.el6.x86_64                          4/4 \n\nInstalled:\n  slurm-contribs.x86_64 0:17.02.7-1.el6   slurm-devel.x86_64 0:17.02.7-1.el6   \n  slurm-perlapi.x86_64 0:17.02.7-1.el6    slurm-torque.x86_64 0:17.02.7-1.el6  \n\nComplete!\n"]}
changed: [compute002] => (item=[u'slurm', u'slurm-devel', u'slurm-perlapi', u'slurm-contribs', u'slurm-torque']) => {"changed": true, "item": ["slurm", "slurm-devel", "slurm-perlapi", "slurm-contribs", "slurm-torque"], "msg": "", "rc": 0, "results": ["slurm-17.02.7-1.el6.x86_64 providing slurm is already installed", "Loaded plugins: fastestmirror, security\nSetting up Install Process\nLoading mirror speeds from cached hostfile\nResolving Dependencies\n--> Running transaction check\n---> Package slurm-contribs.x86_64 0:17.02.7-1.el6 will be installed\n---> Package slurm-devel.x86_64 0:17.02.7-1.el6 will be installed\n---> Package slurm-perlapi.x86_64 0:17.02.7-1.el6 will be installed\n---> Package slurm-torque.x86_64 0:17.02.7-1.el6 will be installed\n--> Finished Dependency Resolution\n\nDependencies Resolved\n\n================================================================================\n Package              Arch         Version              Repository         Size\n================================================================================\nInstalling:\n slurm-contribs       x86_64       17.02.7-1.el6        local-slurm        17 k\n slurm-devel          x86_64       17.02.7-1.el6        local-slurm       143 k\n slurm-perlapi        x86_64       17.02.7-1.el6        local-slurm       456 k\n slurm-torque         x86_64       17.02.7-1.el6        local-slurm        37 k\n\nTransaction Summary\n================================================================================\nInstall       4 Package(s)\n\nTotal download size: 653 k\nInstalled size: 2.5 M\nDownloading Packages:\n--------------------------------------------------------------------------------\nTotal                                            39 MB/s | 653 kB     00:00     \nRunning rpm_check_debug\nRunning Transaction Test\nTransaction Test Succeeded\nRunning Transaction\n\r  Installing : slurm-perlapi-17.02.7-1.el6.x86_64                           1/4 \n\r  Installing : slurm-torque-17.02.7-1.el6.x86_64                            2/4 \n\r  Installing : slurm-contribs-17.02.7-1.el6.x86_64                          3/4 \n\r  Installing : slurm-devel-17.02.7-1.el6.x86_64                             4/4 \n\r  Verifying  : slurm-perlapi-17.02.7-1.el6.x86_64                           1/4 \n\r  Verifying  : slurm-devel-17.02.7-1.el6.x86_64                             2/4 \n\r  Verifying  : slurm-torque-17.02.7-1.el6.x86_64                            3/4 \n\r  Verifying  : slurm-contribs-17.02.7-1.el6.x86_64                          4/4 \n\nInstalled:\n  slurm-contribs.x86_64 0:17.02.7-1.el6   slurm-devel.x86_64 0:17.02.7-1.el6   \n  slurm-perlapi.x86_64 0:17.02.7-1.el6    slurm-torque.x86_64 0:17.02.7-1.el6  \n\nComplete!\n"]}
changed: [frontend001] => (item=[u'slurm', u'slurm-devel', u'slurm-perlapi', u'slurm-contribs', u'slurm-torque']) => {"changed": true, "item": ["slurm", "slurm-devel", "slurm-perlapi", "slurm-contribs", "slurm-torque"], "msg": "", "rc": 0, "results": ["slurm-17.02.7-1.el6.x86_64 providing slurm is already installed", "Loaded plugins: fastestmirror, security\nSetting up Install Process\nLoading mirror speeds from cached hostfile\nResolving Dependencies\n--> Running transaction check\n---> Package slurm-contribs.x86_64 0:17.02.7-1.el6 will be installed\n---> Package slurm-devel.x86_64 0:17.02.7-1.el6 will be installed\n---> Package slurm-perlapi.x86_64 0:17.02.7-1.el6 will be installed\n---> Package slurm-torque.x86_64 0:17.02.7-1.el6 will be installed\n--> Finished Dependency Resolution\n\nDependencies Resolved\n\n================================================================================\n Package              Arch         Version              Repository         Size\n================================================================================\nInstalling:\n slurm-contribs       x86_64       17.02.7-1.el6        local-slurm        17 k\n slurm-devel          x86_64       17.02.7-1.el6        local-slurm       143 k\n slurm-perlapi        x86_64       17.02.7-1.el6        local-slurm       456 k\n slurm-torque         x86_64       17.02.7-1.el6        local-slurm        37 k\n\nTransaction Summary\n================================================================================\nInstall       4 Package(s)\n\nTotal download size: 653 k\nInstalled size: 2.5 M\nDownloading Packages:\n--------------------------------------------------------------------------------\nTotal                                            48 MB/s | 653 kB     00:00     \nRunning rpm_check_debug\nRunning Transaction Test\nTransaction Test Succeeded\nRunning Transaction\n\r  Installing : slurm-perlapi-17.02.7-1.el6.x86_64                           1/4 \n\r  Installing : slurm-torque-17.02.7-1.el6.x86_64                            2/4 \n\r  Installing : slurm-contribs-17.02.7-1.el6.x86_64                          3/4 \n\r  Installing : slurm-devel-17.02.7-1.el6.x86_64                             4/4 \n\r  Verifying  : slurm-perlapi-17.02.7-1.el6.x86_64                           1/4 \n\r  Verifying  : slurm-devel-17.02.7-1.el6.x86_64                             2/4 \n\r  Verifying  : slurm-torque-17.02.7-1.el6.x86_64                            3/4 \n\r  Verifying  : slurm-contribs-17.02.7-1.el6.x86_64                          4/4 \n\nInstalled:\n  slurm-contribs.x86_64 0:17.02.7-1.el6   slurm-devel.x86_64 0:17.02.7-1.el6   \n  slurm-perlapi.x86_64 0:17.02.7-1.el6    slurm-torque.x86_64 0:17.02.7-1.el6  \n\nComplete!\n"]}

TASK [slurm-client : Stop SLURM services (RHEL/CentOS 6.x)] ********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-client/tasks/main.yml:59
changed: [frontend001] => {"changed": true, "enabled": false, "name": "slurm", "state": "stopped"}
changed: [compute001] => {"changed": true, "enabled": false, "name": "slurm", "state": "stopped"}
changed: [compute002] => {"changed": true, "enabled": false, "name": "slurm", "state": "stopped"}

TASK [slurm-client : Stop SLURM services (RHEL/CentOS 7.x)] ********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-client/tasks/main.yml:66
skipping: [frontend001] => (item=slurmd)  => {"changed": false, "item": "slurmd", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [frontend001] => (item=slurmctld)  => {"changed": false, "item": "slurmctld", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => (item=slurmd)  => {"changed": false, "item": "slurmd", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => (item=slurmctld)  => {"changed": false, "item": "slurmctld", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item=slurmd)  => {"changed": false, "item": "slurmd", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item=slurmctld)  => {"changed": false, "item": "slurmctld", "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-worker : Load distribution-specific parameters] ********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/main.yml:4
included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/init-RedHat.yml for frontend001, compute001, compute002

TASK [slurm-worker : Set SLURM worker playbook params (RHEL compatible)] *******
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/init-RedHat.yml:3
ok: [frontend001] => {"ansible_facts": {"slurmd_packages": ["slurm", "slurm-plugins", "slurm-sql"]}, "changed": false}
ok: [compute001] => {"ansible_facts": {"slurmd_packages": ["slurm", "slurm-plugins", "slurm-sql"]}, "changed": false}
ok: [compute002] => {"ansible_facts": {"slurmd_packages": ["slurm", "slurm-plugins", "slurm-sql"]}, "changed": false}

TASK [slurm-worker : Set SLURM worker service name (RHEL 7.x compatible)] ******
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/init-RedHat.yml:11
skipping: [frontend001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-worker : Set SLURM worker service name (RHEL 6.x compatible)] ******
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/init-RedHat.yml:16
ok: [frontend001] => {"ansible_facts": {"slurmd_service": "slurm"}, "changed": false}
ok: [compute001] => {"ansible_facts": {"slurmd_service": "slurm"}, "changed": false}
ok: [compute002] => {"ansible_facts": {"slurmd_service": "slurm"}, "changed": false}

TASK [slurm-worker : Deploy SLURM configuration files] *************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/main.yml:7
changed: [frontend001] => (item=gres.conf) => {"changed": true, "checksum": "afc9ac4c3247ee557700305700c9743a79f0dfd0", "dest": "/etc/slurm/gres.conf", "gid": 0, "group": "root", "item": "gres.conf", "md5sum": "a178d15a40bf70261e38f1cd729a8d56", "mode": "0444", "owner": "root", "size": 297, "src": "/root/.ansible/tmp/ansible-tmp-1524911799.61-167184746094821/source", "state": "file", "uid": 0}
changed: [compute002] => (item=gres.conf) => {"changed": true, "checksum": "afc9ac4c3247ee557700305700c9743a79f0dfd0", "dest": "/etc/slurm/gres.conf", "gid": 0, "group": "root", "item": "gres.conf", "md5sum": "a178d15a40bf70261e38f1cd729a8d56", "mode": "0444", "owner": "root", "size": 297, "src": "/root/.ansible/tmp/ansible-tmp-1524911799.65-255667424258407/source", "state": "file", "uid": 0}
changed: [compute001] => (item=gres.conf) => {"changed": true, "checksum": "afc9ac4c3247ee557700305700c9743a79f0dfd0", "dest": "/etc/slurm/gres.conf", "gid": 0, "group": "root", "item": "gres.conf", "md5sum": "a178d15a40bf70261e38f1cd729a8d56", "mode": "0444", "owner": "root", "size": 297, "src": "/root/.ansible/tmp/ansible-tmp-1524911799.63-59158043482413/source", "state": "file", "uid": 0}

TASK [slurm-worker : Deploy kernel config check script] ************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/cgroup.yml:3
skipping: [frontend001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-worker : Check that kernel is configured for cgroups] **************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/cgroup.yml:12
skipping: [frontend001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-worker : Ensure cgroup filesystems are mounted] ********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/cgroup.yml:18
skipping: [frontend001] => (item=blkio)  => {"changed": false, "item": "blkio", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [frontend001] => (item=cpuacct)  => {"changed": false, "item": "cpuacct", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => (item=blkio)  => {"changed": false, "item": "blkio", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => (item=cpuacct)  => {"changed": false, "item": "cpuacct", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [frontend001] => (item=cpuset)  => {"changed": false, "item": "cpuset", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item=blkio)  => {"changed": false, "item": "blkio", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => (item=cpuset)  => {"changed": false, "item": "cpuset", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [frontend001] => (item=devices)  => {"changed": false, "item": "devices", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item=cpuacct)  => {"changed": false, "item": "cpuacct", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => (item=devices)  => {"changed": false, "item": "devices", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [frontend001] => (item=freezer)  => {"changed": false, "item": "freezer", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item=cpuset)  => {"changed": false, "item": "cpuset", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => (item=freezer)  => {"changed": false, "item": "freezer", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [frontend001] => (item=memory)  => {"changed": false, "item": "memory", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item=devices)  => {"changed": false, "item": "devices", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => (item=memory)  => {"changed": false, "item": "memory", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item=freezer)  => {"changed": false, "item": "freezer", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item=memory)  => {"changed": false, "item": "memory", "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-worker : Ensure kernel is booted with swap accounting enabled] *****
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/cgroup.yml:44
skipping: [frontend001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-worker : Check if swap accouting is enabled (may fail!)] ***********
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/cgroup.yml:53
skipping: [frontend001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-worker : Reboot to enable swap accounting] *************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/cgroup.yml:59
skipping: [frontend001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-worker : Wait for server to come up again] *************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/cgroup.yml:72
skipping: [frontend001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-worker : Ensure release agent directory exists] ********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/cgroup.yml:84
skipping: [frontend001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-worker : Deploy cgroup-specific release agent script] **************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/cgroup.yml:93
skipping: [frontend001] => (item=blkio)  => {"changed": false, "item": "blkio", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [frontend001] => (item=cpuacct)  => {"changed": false, "item": "cpuacct", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => (item=blkio)  => {"changed": false, "item": "blkio", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => (item=cpuacct)  => {"changed": false, "item": "cpuacct", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [frontend001] => (item=cpuset)  => {"changed": false, "item": "cpuset", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item=blkio)  => {"changed": false, "item": "blkio", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => (item=cpuset)  => {"changed": false, "item": "cpuset", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [frontend001] => (item=devices)  => {"changed": false, "item": "devices", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item=cpuacct)  => {"changed": false, "item": "cpuacct", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => (item=devices)  => {"changed": false, "item": "devices", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [frontend001] => (item=freezer)  => {"changed": false, "item": "freezer", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item=cpuset)  => {"changed": false, "item": "cpuset", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => (item=freezer)  => {"changed": false, "item": "freezer", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [frontend001] => (item=memory)  => {"changed": false, "item": "memory", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item=devices)  => {"changed": false, "item": "devices", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => (item=memory)  => {"changed": false, "item": "memory", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item=freezer)  => {"changed": false, "item": "freezer", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item=memory)  => {"changed": false, "item": "memory", "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-worker : Deploy SLURM cgroup configuration file (main)] ************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/cgroup.yml:103
skipping: [frontend001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-worker : Deploy SLURM cgroup configuration file (devices)] *********
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/cgroup.yml:112
skipping: [frontend001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-worker : Install SLURM worker packages] ****************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/main.yml:27
changed: [compute001] => (item=[u'slurm', u'slurm-plugins', u'slurm-sql']) => {"changed": true, "item": ["slurm", "slurm-plugins", "slurm-sql"], "msg": "", "rc": 0, "results": ["slurm-17.02.7-1.el6.x86_64 providing slurm is already installed", "slurm-plugins-17.02.7-1.el6.x86_64 providing slurm-plugins is already installed", "Loaded plugins: fastestmirror, security\nSetting up Install Process\nLoading mirror speeds from cached hostfile\nResolving Dependencies\n--> Running transaction check\n---> Package slurm-sql.x86_64 0:17.02.7-1.el6 will be installed\n--> Finished Dependency Resolution\n\nDependencies Resolved\n\n================================================================================\n Package          Arch          Version                Repository          Size\n================================================================================\nInstalling:\n slurm-sql        x86_64        17.02.7-1.el6          local-slurm        306 k\n\nTransaction Summary\n================================================================================\nInstall       1 Package(s)\n\nTotal download size: 306 k\nInstalled size: 1.1 M\nDownloading Packages:\nRunning rpm_check_debug\nRunning Transaction Test\nTransaction Test Succeeded\nRunning Transaction\n\r  Installing : slurm-sql-17.02.7-1.el6.x86_64                               1/1 \n\r  Verifying  : slurm-sql-17.02.7-1.el6.x86_64                               1/1 \n\nInstalled:\n  slurm-sql.x86_64 0:17.02.7-1.el6                                              \n\nComplete!\n"]}
changed: [frontend001] => (item=[u'slurm', u'slurm-plugins', u'slurm-sql']) => {"changed": true, "item": ["slurm", "slurm-plugins", "slurm-sql"], "msg": "", "rc": 0, "results": ["slurm-17.02.7-1.el6.x86_64 providing slurm is already installed", "slurm-plugins-17.02.7-1.el6.x86_64 providing slurm-plugins is already installed", "Loaded plugins: fastestmirror, security\nSetting up Install Process\nLoading mirror speeds from cached hostfile\nResolving Dependencies\n--> Running transaction check\n---> Package slurm-sql.x86_64 0:17.02.7-1.el6 will be installed\n--> Finished Dependency Resolution\n\nDependencies Resolved\n\n================================================================================\n Package          Arch          Version                Repository          Size\n================================================================================\nInstalling:\n slurm-sql        x86_64        17.02.7-1.el6          local-slurm        306 k\n\nTransaction Summary\n================================================================================\nInstall       1 Package(s)\n\nTotal download size: 306 k\nInstalled size: 1.1 M\nDownloading Packages:\nRunning rpm_check_debug\nRunning Transaction Test\nTransaction Test Succeeded\nRunning Transaction\n\r  Installing : slurm-sql-17.02.7-1.el6.x86_64                               1/1 \n\r  Verifying  : slurm-sql-17.02.7-1.el6.x86_64                               1/1 \n\nInstalled:\n  slurm-sql.x86_64 0:17.02.7-1.el6                                              \n\nComplete!\n"]}
changed: [compute002] => (item=[u'slurm', u'slurm-plugins', u'slurm-sql']) => {"changed": true, "item": ["slurm", "slurm-plugins", "slurm-sql"], "msg": "", "rc": 0, "results": ["slurm-17.02.7-1.el6.x86_64 providing slurm is already installed", "slurm-plugins-17.02.7-1.el6.x86_64 providing slurm-plugins is already installed", "Loaded plugins: fastestmirror, security\nSetting up Install Process\nLoading mirror speeds from cached hostfile\nResolving Dependencies\n--> Running transaction check\n---> Package slurm-sql.x86_64 0:17.02.7-1.el6 will be installed\n--> Finished Dependency Resolution\n\nDependencies Resolved\n\n================================================================================\n Package          Arch          Version                Repository          Size\n================================================================================\nInstalling:\n slurm-sql        x86_64        17.02.7-1.el6          local-slurm        306 k\n\nTransaction Summary\n================================================================================\nInstall       1 Package(s)\n\nTotal download size: 306 k\nInstalled size: 1.1 M\nDownloading Packages:\nRunning rpm_check_debug\nRunning Transaction Test\nTransaction Test Succeeded\nRunning Transaction\n\r  Installing : slurm-sql-17.02.7-1.el6.x86_64                               1/1 \n\r  Verifying  : slurm-sql-17.02.7-1.el6.x86_64                               1/1 \n\nInstalled:\n  slurm-sql.x86_64 0:17.02.7-1.el6                                              \n\nComplete!\n"]}

TASK [slurm-worker : Ensure SLURMd starts at boot] *****************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/main.yml:33
changed: [frontend001] => {"changed": true, "enabled": true, "name": "slurm"}
changed: [compute001] => {"changed": true, "enabled": true, "name": "slurm"}
changed: [compute002] => {"changed": true, "enabled": true, "name": "slurm"}

PLAY [Setup OBS-initial mount directory and password file] *********************

TASK [setup] *******************************************************************
ok: [frontend001]
ok: [compute001]
ok: [compute002]

TASK [Create the obs directory] ************************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/s3fs.yml:8
changed: [frontend001] => {"changed": true, "gid": 0, "group": "root", "mode": "0755", "owner": "root", "path": "/obs", "size": 4096, "state": "directory", "uid": 0}
changed: [compute001] => {"changed": true, "gid": 0, "group": "root", "mode": "0755", "owner": "root", "path": "/obs", "size": 4096, "state": "directory", "uid": 0}
changed: [compute002] => {"changed": true, "gid": 0, "group": "root", "mode": "0755", "owner": "root", "path": "/obs", "size": 4096, "state": "directory", "uid": 0}

TASK [Create the obs password file] ********************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/s3fs.yml:12
changed: [frontend001] => {"changed": true, "dest": "/etc/passwd-obs", "gid": 0, "group": "root", "mode": "0600", "owner": "root", "size": 0, "state": "file", "uid": 0}
changed: [compute001] => {"changed": true, "dest": "/etc/passwd-obs", "gid": 0, "group": "root", "mode": "0600", "owner": "root", "size": 0, "state": "file", "uid": 0}
changed: [compute002] => {"changed": true, "dest": "/etc/passwd-obs", "gid": 0, "group": "root", "mode": "0600", "owner": "root", "size": 0, "state": "file", "uid": 0}

TASK [Setup OBS-insert into password file] *************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/s3fs.yml:17
changed: [frontend001] => {"changed": true, "cmd": "echo CNYVGDKK2PPBQEEMUYQG:6imAmmCVVzMqR1dm5x0AKJcL1BILA7TlOsSMhu4G > /etc/passwd-obs", "delta": "0:00:00.003543", "end": "2018-04-28 18:36:44.474100", "rc": 0, "start": "2018-04-28 18:36:44.470557", "stderr": "", "stdout": "", "stdout_lines": [], "warnings": []}
changed: [compute001] => {"changed": true, "cmd": "echo CNYVGDKK2PPBQEEMUYQG:6imAmmCVVzMqR1dm5x0AKJcL1BILA7TlOsSMhu4G > /etc/passwd-obs", "delta": "0:00:00.003740", "end": "2018-04-28 18:36:44.936535", "rc": 0, "start": "2018-04-28 18:36:44.932795", "stderr": "", "stdout": "", "stdout_lines": [], "warnings": []}
changed: [compute002] => {"changed": true, "cmd": "echo CNYVGDKK2PPBQEEMUYQG:6imAmmCVVzMqR1dm5x0AKJcL1BILA7TlOsSMhu4G > /etc/passwd-obs", "delta": "0:00:00.003348", "end": "2018-04-28 18:36:44.826509", "rc": 0, "start": "2018-04-28 18:36:44.823161", "stderr": "", "stdout": "", "stdout_lines": [], "warnings": []}

TASK [Setup OBS-umount obs] ****************************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/s3fs.yml:19
ok: [frontend001] => {"changed": false, "dump": "0", "fstab": "/etc/fstab", "fstype": "none", "name": "/obs", "opts": "defaults", "passno": "0", "src": "none"}
ok: [compute002] => {"changed": false, "dump": "0", "fstab": "/etc/fstab", "fstype": "none", "name": "/obs", "opts": "defaults", "passno": "0", "src": "none"}
ok: [compute001] => {"changed": false, "dump": "0", "fstab": "/etc/fstab", "fstype": "none", "name": "/obs", "opts": "defaults", "passno": "0", "src": "none"}

TASK [Setup OBS-mount obs] *****************************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/s3fs.yml:25
changed: [frontend001] => {"changed": true, "cmd": "s3fs hwc-obs /obs -o url=http://obs.cn-north-1.myhwclouds.com -o endpoint=cn-north-1 -o passwd_file=/etc/passwd-obs", "delta": "0:00:00.037768", "end": "2018-04-28 18:36:45.009542", "rc": 0, "start": "2018-04-28 18:36:44.971774", "stderr": "", "stdout": "", "stdout_lines": [], "warnings": []}
changed: [compute001] => {"changed": true, "cmd": "s3fs hwc-obs /obs -o url=http://obs.cn-north-1.myhwclouds.com -o endpoint=cn-north-1 -o passwd_file=/etc/passwd-obs", "delta": "0:00:00.035815", "end": "2018-04-28 18:36:45.462618", "rc": 0, "start": "2018-04-28 18:36:45.426803", "stderr": "", "stdout": "", "stdout_lines": [], "warnings": []}
changed: [compute002] => {"changed": true, "cmd": "s3fs hwc-obs /obs -o url=http://obs.cn-north-1.myhwclouds.com -o endpoint=cn-north-1 -o passwd_file=/etc/passwd-obs", "delta": "0:00:00.035841", "end": "2018-04-28 18:36:45.355997", "rc": 0, "start": "2018-04-28 18:36:45.320156", "stderr": "", "stdout": "", "stdout_lines": [], "warnings": []}

TASK [Remove item from /etc/fstaba] ********************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/s3fs.yml:27
ok: [frontend001] => {"backup": "", "changed": false, "found": 0, "msg": ""}
ok: [compute001] => {"backup": "", "changed": false, "found": 0, "msg": ""}
ok: [compute002] => {"backup": "", "changed": false, "found": 0, "msg": ""}

TASK [Add to /etc/fstab] *******************************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/s3fs.yml:33
changed: [frontend001] => {"changed": true, "cmd": "echo s3fs#hwc-obs /obs fuse _netdev,allow_other,use_path_request_style,url=http://obs.cn-north-1.myhwclouds.com,passwd_file=/etc/passwd-obs 0 0 >> /etc/fstab", "delta": "0:00:00.003736", "end": "2018-04-28 18:36:45.500810", "rc": 0, "start": "2018-04-28 18:36:45.497074", "stderr": "", "stdout": "", "stdout_lines": [], "warnings": []}
changed: [compute001] => {"changed": true, "cmd": "echo s3fs#hwc-obs /obs fuse _netdev,allow_other,use_path_request_style,url=http://obs.cn-north-1.myhwclouds.com,passwd_file=/etc/passwd-obs 0 0 >> /etc/fstab", "delta": "0:00:00.003456", "end": "2018-04-28 18:36:45.956632", "rc": 0, "start": "2018-04-28 18:36:45.953176", "stderr": "", "stdout": "", "stdout_lines": [], "warnings": []}
changed: [compute002] => {"changed": true, "cmd": "echo s3fs#hwc-obs /obs fuse _netdev,allow_other,use_path_request_style,url=http://obs.cn-north-1.myhwclouds.com,passwd_file=/etc/passwd-obs 0 0 >> /etc/fstab", "delta": "0:00:00.003374", "end": "2018-04-28 18:36:45.845308", "rc": 0, "start": "2018-04-28 18:36:45.841934", "stderr": "", "stdout": "", "stdout_lines": [], "warnings": []}

PLAY [Slurm master Playbook] ***************************************************

TASK [setup] *******************************************************************
ok: [frontend001]

TASK [nis : Load distribution-specific parameters] *****************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/main.yml:3
included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/init-RedHat.yml for frontend001

TASK [nis : Set NIS common playbook params (CentOS/RHEL)] **********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/init-RedHat.yml:3
ok: [frontend001] => {"ansible_facts": {"nis_client_packages": ["ypbind"], "nis_client_services": ["rpcbind", "ypbind"], "nis_common_packages": ["yp-tools"], "nis_master_packages": ["ypserv"], "nis_master_services": ["rpcbind", "yppasswdd", "ypserv"], "nis_securenets_path": "/var/yp/securenets"}, "changed": false}

TASK [nis : Pre-load debconf answer to questions (Debian/Ubuntu)] **************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/main.yml:7
skipping: [frontend001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [nis : Deploy additional NIS configuration (Debian/Ubuntu)] ***************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/main.yml:18
skipping: [frontend001] => (item=etc/default/nis)  => {"changed": false, "item": "etc/default/nis", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [frontend001] => (item=etc/defaultdomain)  => {"changed": false, "item": "etc/defaultdomain", "skip_reason": "Conditional check failed", "skipped": true}

TASK [nis : Set NIS domain (CentOS/RHEL)] **************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/main.yml:29
changed: [frontend001] => {"backup": "", "changed": true, "msg": "line added"}

TASK [nis : Install NIS common packages] ***************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/main.yml:39
ok: [frontend001] => (item=[u'yp-tools']) => {"changed": false, "item": ["yp-tools"], "msg": "", "rc": 0, "results": ["yp-tools-2.9-12.el6.x86_64 providing yp-tools is already installed"]}

TASK [nis : Deploy `ypserv` configuration files] *******************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml:3
ok: [frontend001] => {"changed": false, "gid": 0, "group": "root", "mode": "0400", "owner": "root", "path": "/var/yp/securenets", "size": 708, "state": "file", "uid": 0}

TASK [nis : Deploy `yppasswdd` configuration file (CentOS/RHEL)] ***************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml:12
ok: [frontend001] => {"changed": false, "checksum": "18392b4e3ecc4f9e7832c274663b5f6cee00d3f1", "dest": "/etc/sysconfig/yppasswdd", "gid": 0, "group": "root", "mode": "0444", "owner": "root", "path": "/etc/sysconfig/yppasswdd", "size": 681, "state": "file", "uid": 0}

TASK [nis : Install NIS master server packages] ********************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml:22
ok: [frontend001] => (item=[u'ypserv']) => {"changed": false, "item": ["ypserv"], "msg": "", "rc": 0, "results": ["ypserv-2.19-31.el6.x86_64 providing ypserv is already installed"]}

TASK [nis : Ensure `ypserv` starts at boot] ************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml:33
ok: [frontend001] => (item=rpcbind) => {"changed": false, "enabled": true, "item": "rpcbind", "name": "rpcbind", "state": "started"}
ok: [frontend001] => (item=yppasswdd) => {"changed": false, "enabled": true, "item": "yppasswdd", "name": "yppasswdd", "state": "started"}
ok: [frontend001] => (item=ypserv) => {"changed": false, "enabled": true, "item": "ypserv", "name": "ypserv", "state": "started"}

TASK [nis : Update NIS/YP databases (NIS master server)] ***********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml:41
changed: [frontend001] => {"changed": true, "cmd": ["make"], "delta": "0:00:00.032361", "end": "2018-04-28 18:36:48.776218", "rc": 0, "start": "2018-04-28 18:36:48.743857", "stderr": "", "stdout": "gmake[1]: Entering directory `/var/yp/elasticluster'\nUpdating netid.byname...\ngmake[1]: Leaving directory `/var/yp/elasticluster'", "stdout_lines": ["gmake[1]: Entering directory `/var/yp/elasticluster'", "Updating netid.byname...", "gmake[1]: Leaving directory `/var/yp/elasticluster'"], "warnings": []}

TASK [nis : Deploy `ypbind` configuration files] *******************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypbind.yml:3
skipping: [frontend001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [nis : Ensure NIS/YP is used as a name service] ***************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypbind.yml:12
skipping: [frontend001] => (item=passwd)  => {"changed": false, "item": "passwd", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [frontend001] => (item=group)  => {"changed": false, "item": "group", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [frontend001] => (item=shadow)  => {"changed": false, "item": "shadow", "skip_reason": "Conditional check failed", "skipped": true}

TASK [nis : Replace `compat` in `/etc/nsswitch.conf` with `files nis`] *********
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypbind.yml:26
skipping: [frontend001] => (item=passwd)  => {"changed": false, "item": "passwd", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [frontend001] => (item=group)  => {"changed": false, "item": "group", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [frontend001] => (item=shadow)  => {"changed": false, "item": "shadow", "skip_reason": "Conditional check failed", "skipped": true}

TASK [nis : Ensure `ypbind` starts at boot] ************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypbind.yml:37
skipping: [frontend001] => (item=rpcbind)  => {"changed": false, "item": "rpcbind", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [frontend001] => (item=ypbind)  => {"changed": false, "item": "ypbind", "skip_reason": "Conditional check failed", "skipped": true}

TASK [nfs-server : Load distribution-specific parameters] **********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-server/tasks/main.yml:6
included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-server/tasks/init-RedHat.yml for frontend001

TASK [nfs-server : Set NFS server variables (RHEL/CentOS 7.x)] *****************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-server/tasks/init-RedHat.yml:6
skipping: [frontend001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [nfs-server : Set NFS server variables (RHEL/CentOS 6.x)] *****************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-server/tasks/init-RedHat.yml:20
ok: [frontend001] => {"ansible_facts": {"_nfs_server_started_state": "started", "nfs_server_packages": ["nfs-utils"], "nfs_server_services": ["rpcbind", "nfslock", "nfs"]}, "changed": false}

TASK [nfs-server : install NFS server software] ********************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-server/tasks/main.yml:12
ok: [frontend001] => (item=[u'nfs-utils']) => {"changed": false, "item": ["nfs-utils"], "msg": "", "rc": 0, "results": ["nfs-utils-1:1.2.3-75.el6_9.x86_64 providing nfs-utils is already installed"]}

TASK [nfs-server : Export directories] *****************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-server/tasks/main.yml:25
changed: [frontend001] => (item={u'path': u'/home', u'clients': [u'compute001', u'compute002']}) => {"changed": true, "item": {"clients": ["compute001", "compute002"], "path": "/home"}}

TASK [nfs-server : Ensure portmapper is running] *******************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-server/tasks/main.yml:38
ok: [frontend001] => {"changed": false, "enabled": true, "name": "rpcbind", "state": "started"}

TASK [nfs-server : Ensure NFS server is running (Debian 8 "jessie")] ***********
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-server/tasks/main.yml:48
skipping: [frontend001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [nfs-server : Ensure NFS server is running] *******************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-server/tasks/main.yml:59
ok: [frontend001] => (item=rpcbind) => {"changed": false, "enabled": true, "item": "rpcbind", "name": "rpcbind", "state": "started"}
ok: [frontend001] => (item=nfslock) => {"changed": false, "enabled": true, "item": "nfslock", "name": "nfslock", "state": "started"}
ok: [frontend001] => (item=nfs) => {"changed": false, "enabled": true, "item": "nfs", "name": "nfs", "state": "started"}

TASK [nfs-server : Reload NFS exports file] ************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-server/tasks/main.yml:71
changed: [frontend001] => {"changed": true, "cmd": ["exportfs", "-r"], "delta": "0:00:00.003569", "end": "2018-04-28 18:36:50.848304", "rc": 0, "start": "2018-04-28 18:36:50.844735", "stderr": "", "stdout": "", "stdout_lines": [], "warnings": []}

TASK [nfs-server : Restart NFS server services] ********************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-server/tasks/main.yml:79
changed: [frontend001] => {"changed": true, "name": "nfs", "state": "started"}

TASK [slurm-common : Load distribution-specific parameters] ********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:3
included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/init-RedHat.yml for frontend001

TASK [slurm-common : Set SLURM common playbook params (RHEL 7.x compatible)] ***
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/init-RedHat.yml:3
ok: [frontend001] => {"ansible_facts": {"slurm_pid_dir": "/var/run"}, "changed": false}

TASK [slurm-common : Create `slurm` system group] ******************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:16
ok: [frontend001] => {"changed": false, "gid": 498, "name": "slurm", "state": "present", "system": true}

TASK [slurm-common : Create `slurm` system user] *******************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:24
ok: [frontend001] => {"append": false, "changed": false, "comment": "", "group": 498, "home": "/home/slurm", "move_home": false, "name": "slurm", "shell": "/bin/bash", "state": "present", "uid": 498}

TASK [slurm-common : Create work directory {{item}}] ***************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:33
changed: [frontend001] => (item=/etc/slurm) => {"changed": true, "gid": 498, "group": "slurm", "item": "/etc/slurm", "mode": "0755", "owner": "slurm", "path": "/etc/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [frontend001] => (item=/var/lib/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/lib/slurm", "mode": "0755", "owner": "slurm", "path": "/var/lib/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [frontend001] => (item=/var/lib/slurm/checkpoint) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/lib/slurm/checkpoint", "mode": "0755", "owner": "slurm", "path": "/var/lib/slurm/checkpoint", "size": 4096, "state": "directory", "uid": 498}
ok: [frontend001] => (item=/var/lib/slurm/slurmctld) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/lib/slurm/slurmctld", "mode": "0755", "owner": "slurm", "path": "/var/lib/slurm/slurmctld", "size": 4096, "state": "directory", "uid": 498}
ok: [frontend001] => (item=/var/lib/slurm/slurmd) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/lib/slurm/slurmd", "mode": "0755", "owner": "slurm", "path": "/var/lib/slurm/slurmd", "size": 4096, "state": "directory", "uid": 498}
ok: [frontend001] => (item=/var/log/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/log/slurm", "mode": "0755", "owner": "slurm", "path": "/var/log/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [frontend001] => (item=/var/run/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/run/slurm", "mode": "0755", "owner": "slurm", "path": "/var/run/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [frontend001] => (item=/var/spool/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/spool/slurm", "mode": "0755", "owner": "slurm", "path": "/var/spool/slurm", "size": 4096, "state": "directory", "uid": 498}

TASK [slurm-common : Make compatibility symlinks] ******************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:56
skipping: [frontend001] => (item={u'to': u'/var/lib/slurm-llnl', u'from': u'/var/lib/slurm'})  => {"changed": false, "item": {"from": "/var/lib/slurm", "to": "/var/lib/slurm-llnl"}, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [frontend001] => (item={u'to': u'/etc/slurm-llnl', u'from': u'/etc/slurm'})  => {"changed": false, "item": {"from": "/etc/slurm", "to": "/etc/slurm-llnl"}, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [frontend001] => (item={u'to': u'/var/log/slurm-llnl', u'from': u'/var/log/slurm'})  => {"changed": false, "item": {"from": "/var/log/slurm", "to": "/var/log/slurm-llnl"}, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [frontend001] => (item={u'to': u'/var/run/slurm-llnl', u'from': u'/var/run/slurm'})  => {"changed": false, "item": {"from": "/var/run/slurm", "to": "/var/run/slurm-llnl"}, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-common : Enable Copr SLURM repo by verdurin] ***********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:73
ok: [frontend001] => {"changed": false, "gid": 0, "group": "root", "mode": "0444", "owner": "root", "path": "/etc/yum.repos.d/copr-slurm.repo", "size": 615, "state": "file", "uid": 0}

TASK [slurm-common : Prevent MUNGE from starting] ******************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:20
skipping: [frontend001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-common : Install MUNGE (Debian/Ubuntu)] ****************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:31
skipping: [frontend001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-common : Install MUNGE (RHEL-compatible)] **************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:43
ok: [frontend001] => {"changed": false, "msg": "", "rc": 0, "results": ["slurm-munge-17.02.7-1.el6.x86_64 providing slurm-munge is already installed"]}

TASK [slurm-common : Allow MUNGE to start] *************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:57
skipping: [frontend001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-common : Add `--syslog` option to `munged` startup] ****************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:66
skipping: [frontend001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-common : Reload systemd configuration] *****************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:77
skipping: [frontend001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-common : Add `--syslog` startup options for `munged` (Ubuntu 14.04)] ***
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:86
skipping: [frontend001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-common : Configure MUNGE] ******************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:101
ok: [frontend001] => {"changed": false, "checksum": "222afe887f907039b799b394715664ffa584fec5", "dest": "/etc/munge/munge.key", "gid": 497, "group": "munge", "mode": "0400", "owner": "munge", "path": "/etc/munge/munge.key", "size": 1024, "state": "file", "uid": 497}

TASK [slurm-common : Ensure the MUNGE service is running] **********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:114
ok: [frontend001] => {"changed": false, "enabled": true, "name": "munge", "state": "started"}

TASK [slurm-common : Deploy SLURM configuration file] **************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:86
ok: [frontend001] => {"changed": false, "gid": 0, "group": "root", "mode": "0444", "owner": "root", "path": "/etc/slurm/slurm.conf", "size": 4157, "state": "file", "uid": 0}

TASK [slurm-common : Deploy script to suspend cloud instance] ******************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:96
ok: [frontend001] => {"changed": false, "gid": 0, "group": "root", "mode": "0555", "owner": "root", "path": "/etc/slurm/slurm.suspend.sh", "size": 604, "state": "file", "uid": 0}

TASK [slurm-common : Deploy script to resume cloud instance] *******************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:106
ok: [frontend001] => {"changed": false, "gid": 0, "group": "root", "mode": "0555", "owner": "root", "path": "/etc/slurm/slurm.resume.sh", "size": 631, "state": "file", "uid": 0}

TASK [slurm-common : Install support packages (Debian/Ubuntu)] *****************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:116
skipping: [frontend001] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-common : Install SLURM packages (RHEL-compatible)] *****************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:127
skipping: [frontend001] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-client : Install required SLURM packages (Debian/Ubuntu)] **********
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-client/tasks/main.yml:4
skipping: [frontend001] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-client : Install required SLURM packages (older Debian/Ubuntu)] ****
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-client/tasks/main.yml:23
skipping: [frontend001] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-client : service] **************************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-client/tasks/main.yml:35
skipping: [frontend001] => (item=slurm-llnl)  => {"changed": false, "item": "slurm-llnl", "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-client : Install SLURM packages (RHEL-compatible)] *****************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-client/tasks/main.yml:47
ok: [frontend001] => (item=[u'slurm', u'slurm-devel', u'slurm-perlapi', u'slurm-contribs', u'slurm-torque']) => {"changed": false, "item": ["slurm", "slurm-devel", "slurm-perlapi", "slurm-contribs", "slurm-torque"], "msg": "", "rc": 0, "results": ["slurm-17.02.7-1.el6.x86_64 providing slurm is already installed", "slurm-devel-17.02.7-1.el6.x86_64 providing slurm-devel is already installed", "slurm-perlapi-17.02.7-1.el6.x86_64 providing slurm-perlapi is already installed", "slurm-contribs-17.02.7-1.el6.x86_64 providing slurm-contribs is already installed", "slurm-torque-17.02.7-1.el6.x86_64 providing slurm-torque is already installed"]}

TASK [slurm-client : Stop SLURM services (RHEL/CentOS 6.x)] ********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-client/tasks/main.yml:59
changed: [frontend001] => {"changed": true, "enabled": false, "name": "slurm", "state": "stopped"}

TASK [slurm-client : Stop SLURM services (RHEL/CentOS 7.x)] ********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-client/tasks/main.yml:66
skipping: [frontend001] => (item=slurmd)  => {"changed": false, "item": "slurmd", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [frontend001] => (item=slurmctld)  => {"changed": false, "item": "slurmctld", "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-master : Load distribution-specific parameters] ********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-master/tasks/main.yml:4
included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-master/tasks/init-RedHat.yml for frontend001

TASK [slurm-master : Set SLURM master playbook params (RHEL 7.x compatible)] ***
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-master/tasks/init-RedHat.yml:3
skipping: [frontend001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-master : Set SLURM playbook params (RHEL 6.x compatible)] **********
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-master/tasks/init-RedHat.yml:18
ok: [frontend001] => {"ansible_facts": {"slurmctld_packages": ["mailx", "slurm-plugins", "slurm"], "slurmctld_service_name": "slurm", "slurmdbd_packages": ["slurm-sql", "slurm-slurmdbd"], "slurmdbd_service_name": "slurmdbd"}, "changed": false}

TASK [slurm-master : Set variables (Debian/Ubuntu)] ****************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-master/tasks/db.yml:3
skipping: [frontend001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-master : Set variables (older Debian/Ubuntu)] **********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-master/tasks/db.yml:11
skipping: [frontend001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-master : Set variables (RHEL 7.x)] *********************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-master/tasks/db.yml:19
skipping: [frontend001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-master : Set variables (RHEL 6.x)] *********************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-master/tasks/db.yml:27
ok: [frontend001] => {"ansible_facts": {"slurm_db_python_pkg": "MySQL-python", "slurm_db_server_name": "MySQL", "slurm_db_server_pkg": "mysql-server", "slurm_db_service_name": "mysqld"}, "changed": false}

TASK [slurm-master : Install MySQL, used for SLURM accounting] *****************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-master/tasks/db.yml:36
ok: [frontend001] => (item=[u'mysql-server', u'MySQL-python']) => {"changed": false, "item": ["mysql-server", "MySQL-python"], "msg": "", "rc": 0, "results": ["mysql-server-5.1.73-8.el6_8.x86_64 providing mysql-server is already installed", "MySQL-python-1.2.3-0.3.c1.1.el6.x86_64 providing MySQL-python is already installed"]}

TASK [slurm-master : Ensure MySQL daemon is up] ********************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-master/tasks/db.yml:48
ok: [frontend001] => {"changed": false, "enabled": true, "name": "mysqld", "state": "started"}

TASK [slurm-master : Create DB for SLURMDBD] ***********************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-master/tasks/db.yml:58
ok: [frontend001] => {"changed": false, "db": "slurm"}

TASK [slurm-master : Create DB user for SLURMDBD] ******************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-master/tasks/db.yml:68
ok: [frontend001] => (item=frontend001) => {"changed": false, "item": "frontend001", "user": "slurm"}
ok: [frontend001] => (item=localhost) => {"changed": false, "item": "localhost", "user": "slurm"}

TASK [slurm-master : Deploy SLURMDBD configuration] ****************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-master/tasks/install-slurmdbd.yml:5
ok: [frontend001] => {"changed": false, "gid": 0, "group": "root", "mode": "0444", "owner": "root", "path": "/etc/slurm/slurmdbd.conf", "size": 2864, "state": "file", "uid": 0}

TASK [slurm-master : Install SLURM DBD packages] *******************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-master/tasks/install-slurmdbd.yml:19
changed: [frontend001] => (item=[u'slurm-sql', u'slurm-slurmdbd']) => {"changed": true, "item": ["slurm-sql", "slurm-slurmdbd"], "msg": "", "rc": 0, "results": ["slurm-sql-17.02.7-1.el6.x86_64 providing slurm-sql is already installed", "Loaded plugins: fastestmirror, security\nSetting up Install Process\nLoading mirror speeds from cached hostfile\nResolving Dependencies\n--> Running transaction check\n---> Package slurm-slurmdbd.x86_64 0:17.02.7-1.el6 will be installed\n--> Finished Dependency Resolution\n\nDependencies Resolved\n\n================================================================================\n Package              Arch         Version              Repository         Size\n================================================================================\nInstalling:\n slurm-slurmdbd       x86_64       17.02.7-1.el6        local-slurm       1.1 M\n\nTransaction Summary\n================================================================================\nInstall       1 Package(s)\n\nTotal download size: 1.1 M\nInstalled size: 3.9 M\nDownloading Packages:\nRunning rpm_check_debug\nRunning Transaction Test\nTransaction Test Succeeded\nRunning Transaction\n\r  Installing : slurm-slurmdbd-17.02.7-1.el6.x86_64                          1/1 \n\r  Verifying  : slurm-slurmdbd-17.02.7-1.el6.x86_64                          1/1 \n\nInstalled:\n  slurm-slurmdbd.x86_64 0:17.02.7-1.el6                                         \n\nComplete!\n"]}

TASK [slurm-master : Ensure `slurmdbd` is running] *****************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-master/tasks/install-slurmdbd.yml:26
changed: [frontend001] => (item=slurmdbd) => {"changed": true, "enabled": true, "item": "slurmdbd", "name": "slurmdbd", "state": "started"}

TASK [slurm-master : Install SLURM master packages] ****************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-master/tasks/install-slurmctld.yml:3
ok: [frontend001] => (item=[u'mailx', u'slurm-plugins', u'slurm']) => {"changed": false, "item": ["mailx", "slurm-plugins", "slurm"], "msg": "", "rc": 0, "results": ["mailx-12.4-8.el6_6.x86_64 providing mailx is already installed", "slurm-plugins-17.02.7-1.el6.x86_64 providing slurm-plugins is already installed", "slurm-17.02.7-1.el6.x86_64 providing slurm is already installed"]}

TASK [slurm-master : Create cluster in accounting database] ********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-master/tasks/main.yml:13
changed: [frontend001] => {"changed": true, "cmd": "sacctmgr --parsable --noheader list cluster | grep '^elasticluster|' || sacctmgr -i -Q add cluster elasticluster", "delta": "0:00:00.328535", "end": "2018-04-28 18:37:03.371737", "rc": 0, "start": "2018-04-28 18:37:03.043202", "stderr": "", "stdout": "elasticluster||0|7936|1||||||||normal||", "stdout_lines": ["elasticluster||0|7936|1||||||||normal||"], "warnings": []}

TASK [slurm-master : Create an account for default cluster] ********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-master/tasks/main.yml:20
changed: [frontend001] => {"changed": true, "cmd": "sacctmgr --immediate --parsable --noheader list account Cluster=elasticluster | grep '^root|' || sacctmgr -i --quiet add account root Cluster=elasticluster", "delta": "0:00:00.169237", "end": "2018-04-28 18:37:03.706450", "rc": 0, "start": "2018-04-28 18:37:03.537213", "stderr": "", "stdout": "root|default root account|root|", "stdout_lines": ["root|default root account|root|"], "warnings": []}

TASK [slurm-master : Add default user to cluster] ******************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-master/tasks/main.yml:27
changed: [frontend001] => (item=root) => {"changed": true, "cmd": "sacctmgr --immediate --parsable --noheader list user Account=root | grep '^root|' || sacctmgr --immediate --quiet add user 'root' DefaultAccount=root", "delta": "0:00:00.171198", "end": "2018-04-28 18:37:04.043710", "item": "root", "rc": 0, "start": "2018-04-28 18:37:03.872512", "stderr": "", "stdout": "root|root|Administrator|", "stdout_lines": ["root|root|Administrator|"], "warnings": []}

TASK [slurm-master : Ensure `slurmctld` is running] ****************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-master/tasks/main.yml:35
changed: [frontend001] => (item=slurm) => {"changed": true, "enabled": true, "item": "slurm", "name": "slurm", "state": "started"}

PLAY [Slurm worker nodes Playbook] *********************************************

TASK [setup] *******************************************************************
ok: [compute001]
ok: [compute002]

TASK [nis : Load distribution-specific parameters] *****************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/main.yml:3
included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/init-RedHat.yml for compute001, compute002

TASK [nis : Set NIS common playbook params (CentOS/RHEL)] **********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/init-RedHat.yml:3
ok: [compute001] => {"ansible_facts": {"nis_client_packages": ["ypbind"], "nis_client_services": ["rpcbind", "ypbind"], "nis_common_packages": ["yp-tools"], "nis_master_packages": ["ypserv"], "nis_master_services": ["rpcbind", "yppasswdd", "ypserv"], "nis_securenets_path": "/var/yp/securenets"}, "changed": false}
ok: [compute002] => {"ansible_facts": {"nis_client_packages": ["ypbind"], "nis_client_services": ["rpcbind", "ypbind"], "nis_common_packages": ["yp-tools"], "nis_master_packages": ["ypserv"], "nis_master_services": ["rpcbind", "yppasswdd", "ypserv"], "nis_securenets_path": "/var/yp/securenets"}, "changed": false}

TASK [nis : Pre-load debconf answer to questions (Debian/Ubuntu)] **************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/main.yml:7
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [nis : Deploy additional NIS configuration (Debian/Ubuntu)] ***************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/main.yml:18
skipping: [compute001] => (item=etc/default/nis)  => {"changed": false, "item": "etc/default/nis", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => (item=etc/defaultdomain)  => {"changed": false, "item": "etc/defaultdomain", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item=etc/default/nis)  => {"changed": false, "item": "etc/default/nis", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item=etc/defaultdomain)  => {"changed": false, "item": "etc/defaultdomain", "skip_reason": "Conditional check failed", "skipped": true}

TASK [nis : Set NIS domain (CentOS/RHEL)] **************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/main.yml:29
changed: [compute001] => {"backup": "", "changed": true, "msg": "line added"}
changed: [compute002] => {"backup": "", "changed": true, "msg": "line added"}

TASK [nis : Install NIS common packages] ***************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/main.yml:39
ok: [compute001] => (item=[u'yp-tools']) => {"changed": false, "item": ["yp-tools"], "msg": "", "rc": 0, "results": ["yp-tools-2.9-12.el6.x86_64 providing yp-tools is already installed"]}
ok: [compute002] => (item=[u'yp-tools']) => {"changed": false, "item": ["yp-tools"], "msg": "", "rc": 0, "results": ["yp-tools-2.9-12.el6.x86_64 providing yp-tools is already installed"]}

TASK [nis : Deploy `ypserv` configuration files] *******************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml:3
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [nis : Deploy `yppasswdd` configuration file (CentOS/RHEL)] ***************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml:12
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [nis : Install NIS master server packages] ********************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml:22
skipping: [compute001] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}

TASK [nis : Ensure `ypserv` starts at boot] ************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml:33
skipping: [compute001] => (item=rpcbind)  => {"changed": false, "item": "rpcbind", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item=rpcbind)  => {"changed": false, "item": "rpcbind", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => (item=yppasswdd)  => {"changed": false, "item": "yppasswdd", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item=yppasswdd)  => {"changed": false, "item": "yppasswdd", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => (item=ypserv)  => {"changed": false, "item": "ypserv", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item=ypserv)  => {"changed": false, "item": "ypserv", "skip_reason": "Conditional check failed", "skipped": true}

TASK [nis : Update NIS/YP databases (NIS master server)] ***********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml:41
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [nis : Deploy `ypbind` configuration files] *******************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypbind.yml:3
ok: [compute001] => {"changed": false, "gid": 0, "group": "root", "mode": "0400", "owner": "root", "path": "/etc/yp.conf", "size": 725, "state": "file", "uid": 0}
ok: [compute002] => {"changed": false, "gid": 0, "group": "root", "mode": "0400", "owner": "root", "path": "/etc/yp.conf", "size": 725, "state": "file", "uid": 0}

TASK [nis : Ensure NIS/YP is used as a name service] ***************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypbind.yml:12
ok: [compute001] => (item=passwd) => {"backup": "", "changed": false, "item": "passwd", "msg": ""}
ok: [compute002] => (item=passwd) => {"backup": "", "changed": false, "item": "passwd", "msg": ""}
ok: [compute001] => (item=group) => {"backup": "", "changed": false, "item": "group", "msg": ""}
ok: [compute002] => (item=group) => {"backup": "", "changed": false, "item": "group", "msg": ""}
ok: [compute002] => (item=shadow) => {"backup": "", "changed": false, "item": "shadow", "msg": ""}
ok: [compute001] => (item=shadow) => {"backup": "", "changed": false, "item": "shadow", "msg": ""}

TASK [nis : Replace `compat` in `/etc/nsswitch.conf` with `files nis`] *********
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypbind.yml:26
ok: [compute001] => (item=passwd) => {"changed": false, "item": "passwd", "msg": ""}
ok: [compute002] => (item=passwd) => {"changed": false, "item": "passwd", "msg": ""}
ok: [compute001] => (item=group) => {"changed": false, "item": "group", "msg": ""}
ok: [compute002] => (item=group) => {"changed": false, "item": "group", "msg": ""}
ok: [compute002] => (item=shadow) => {"changed": false, "item": "shadow", "msg": ""}
ok: [compute001] => (item=shadow) => {"changed": false, "item": "shadow", "msg": ""}

TASK [nis : Ensure `ypbind` starts at boot] ************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypbind.yml:37
ok: [compute001] => (item=rpcbind) => {"changed": false, "enabled": true, "item": "rpcbind", "name": "rpcbind", "state": "started"}
ok: [compute002] => (item=rpcbind) => {"changed": false, "enabled": true, "item": "rpcbind", "name": "rpcbind", "state": "started"}
ok: [compute001] => (item=ypbind) => {"changed": false, "enabled": true, "item": "ypbind", "name": "ypbind", "state": "started"}
ok: [compute002] => (item=ypbind) => {"changed": false, "enabled": true, "item": "ypbind", "name": "ypbind", "state": "started"}

TASK [nfs-client : install NFS client software (Debian/Ubuntu)] ****************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-client/tasks/main.yml:3
skipping: [compute001] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}

TASK [nfs-client : install NFS client software (RHEL-compatible)] **************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-client/tasks/main.yml:15
ok: [compute001] => (item=[u'nfs-utils', u'nfs4-acl-tools']) => {"changed": false, "item": ["nfs-utils", "nfs4-acl-tools"], "msg": "", "rc": 0, "results": ["nfs-utils-1:1.2.3-75.el6_9.x86_64 providing nfs-utils is already installed", "nfs4-acl-tools-0.3.3-8.el6.x86_64 providing nfs4-acl-tools is already installed"]}
ok: [compute002] => (item=[u'nfs-utils', u'nfs4-acl-tools']) => {"changed": false, "item": ["nfs-utils", "nfs4-acl-tools"], "msg": "", "rc": 0, "results": ["nfs-utils-1:1.2.3-75.el6_9.x86_64 providing nfs-utils is already installed", "nfs4-acl-tools-0.3.3-8.el6.x86_64 providing nfs4-acl-tools is already installed"]}

TASK [nfs-client : Ensure `rpcbind` is running (Debian)] ***********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-client/tasks/main.yml:28
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [nfs-client : Ensure `rpcbind` is running (RHEL-compatible)] **************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-client/tasks/main.yml:35
ok: [compute002] => {"changed": false, "enabled": true, "name": "rpcbind", "state": "started"}
ok: [compute001] => {"changed": false, "enabled": true, "name": "rpcbind", "state": "started"}

TASK [nfs-client : Ensure `portmap` is running (Ubuntu prior to 14.04)] ********
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-client/tasks/main.yml:42
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [nfs-client : Ensure `rpcbind` is running (Ubuntu 14.04 or newer)] ********
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-client/tasks/main.yml:49
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [nfs-client : Mount NFS filesystems] **************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-client/tasks/main.yml:57
included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-client/tasks/nfsmount.yml for compute001, compute002

TASK [nfs-client : ensure /home directory exists] ******************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-client/tasks/nfsmount.yml:3
ok: [compute001] => {"changed": false, "gid": 0, "group": "root", "mode": "0755", "owner": "root", "path": "/home", "size": 4096, "state": "directory", "uid": 0}
ok: [compute002] => {"changed": false, "gid": 0, "group": "root", "mode": "0755", "owner": "root", "path": "/home", "size": 4096, "state": "directory", "uid": 0}

TASK [nfs-client : add to /etc/fstab] ******************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-client/tasks/nfsmount.yml:8
changed: [compute001] => {"changed": true, "dump": "0", "fstab": "/etc/fstab", "fstype": "nfs", "name": "/home", "opts": "rw,async", "passno": "0", "src": "frontend001:/home"}
changed: [compute002] => {"changed": true, "dump": "0", "fstab": "/etc/fstab", "fstype": "nfs", "name": "/home", "opts": "rw,async", "passno": "0", "src": "frontend001:/home"}

TASK [autofs : Load distribution-specific parameters] **************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/autofs/tasks/main.yml:8
included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/autofs/tasks/init-RedHat.yml for compute001, compute002

TASK [autofs : Provide RHEL-specific values] ***********************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/autofs/tasks/init-RedHat.yml:6
ok: [compute001] => {"ansible_facts": {"autofs_packages": ["autofs"], "autofs_service": "autofs", "autofs_username": "root"}, "changed": false}
ok: [compute002] => {"ansible_facts": {"autofs_packages": ["autofs"], "autofs_service": "autofs", "autofs_username": "root"}, "changed": false}

TASK [autofs : Deploy autofs configuration] ************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/autofs/tasks/main.yml:12
ok: [compute001] => (item=etc/auto.master) => {"changed": false, "gid": 0, "group": "root", "item": "etc/auto.master", "mode": "0444", "owner": "root", "path": "/etc/auto.master", "size": 540, "state": "file", "uid": 0}
ok: [compute002] => (item=etc/auto.master) => {"changed": false, "gid": 0, "group": "root", "item": "etc/auto.master", "mode": "0444", "owner": "root", "path": "/etc/auto.master", "size": 540, "state": "file", "uid": 0}
ok: [compute001] => (item=etc/auto.home) => {"changed": false, "gid": 0, "group": "root", "item": "etc/auto.home", "mode": "0444", "owner": "root", "path": "/etc/auto.home", "size": 852, "state": "file", "uid": 0}
ok: [compute002] => (item=etc/auto.home) => {"changed": false, "gid": 0, "group": "root", "item": "etc/auto.home", "mode": "0444", "owner": "root", "path": "/etc/auto.home", "size": 852, "state": "file", "uid": 0}

TASK [autofs : Deploy autofs mount script for NFSv4 (Debian/Ubuntu)] ***********
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/autofs/tasks/main.yml:24
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [autofs : Install Autofs] *************************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/autofs/tasks/main.yml:34
ok: [compute001] => (item=[u'autofs']) => {"changed": false, "item": ["autofs"], "msg": "", "rc": 0, "results": ["autofs-1:5.0.5-133.el6_9.x86_64 providing autofs is already installed"]}
ok: [compute002] => (item=[u'autofs']) => {"changed": false, "item": ["autofs"], "msg": "", "rc": 0, "results": ["autofs-1:5.0.5-133.el6_9.x86_64 providing autofs is already installed"]}

TASK [autofs : Ensure autofs is running and starts at boot] ********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/autofs/tasks/main.yml:41
ok: [compute001] => {"changed": false, "enabled": true, "name": "autofs", "state": "started"}
ok: [compute002] => {"changed": false, "enabled": true, "name": "autofs", "state": "started"}

TASK [slurm-common : Load distribution-specific parameters] ********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:3
included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/init-RedHat.yml for compute001, compute002

TASK [slurm-common : Set SLURM common playbook params (RHEL 7.x compatible)] ***
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/init-RedHat.yml:3
ok: [compute001] => {"ansible_facts": {"slurm_pid_dir": "/var/run"}, "changed": false}
ok: [compute002] => {"ansible_facts": {"slurm_pid_dir": "/var/run"}, "changed": false}

TASK [slurm-common : Create `slurm` system group] ******************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:16
ok: [compute001] => {"changed": false, "gid": 498, "name": "slurm", "state": "present", "system": true}
ok: [compute002] => {"changed": false, "gid": 498, "name": "slurm", "state": "present", "system": true}

TASK [slurm-common : Create `slurm` system user] *******************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:24
ok: [compute002] => {"append": false, "changed": false, "comment": "", "group": 498, "home": "/home/slurm", "move_home": false, "name": "slurm", "shell": "/bin/bash", "state": "present", "uid": 498}
ok: [compute001] => {"append": false, "changed": false, "comment": "", "group": 498, "home": "/home/slurm", "move_home": false, "name": "slurm", "shell": "/bin/bash", "state": "present", "uid": 498}

TASK [slurm-common : Create work directory {{item}}] ***************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:33
changed: [compute001] => (item=/etc/slurm) => {"changed": true, "gid": 498, "group": "slurm", "item": "/etc/slurm", "mode": "0755", "owner": "slurm", "path": "/etc/slurm", "size": 4096, "state": "directory", "uid": 498}
changed: [compute002] => (item=/etc/slurm) => {"changed": true, "gid": 498, "group": "slurm", "item": "/etc/slurm", "mode": "0755", "owner": "slurm", "path": "/etc/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [compute001] => (item=/var/lib/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/lib/slurm", "mode": "0755", "owner": "slurm", "path": "/var/lib/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [compute002] => (item=/var/lib/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/lib/slurm", "mode": "0755", "owner": "slurm", "path": "/var/lib/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [compute002] => (item=/var/lib/slurm/checkpoint) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/lib/slurm/checkpoint", "mode": "0755", "owner": "slurm", "path": "/var/lib/slurm/checkpoint", "size": 4096, "state": "directory", "uid": 498}
ok: [compute001] => (item=/var/lib/slurm/checkpoint) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/lib/slurm/checkpoint", "mode": "0755", "owner": "slurm", "path": "/var/lib/slurm/checkpoint", "size": 4096, "state": "directory", "uid": 498}
ok: [compute001] => (item=/var/lib/slurm/slurmctld) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/lib/slurm/slurmctld", "mode": "0755", "owner": "slurm", "path": "/var/lib/slurm/slurmctld", "size": 4096, "state": "directory", "uid": 498}
ok: [compute002] => (item=/var/lib/slurm/slurmctld) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/lib/slurm/slurmctld", "mode": "0755", "owner": "slurm", "path": "/var/lib/slurm/slurmctld", "size": 4096, "state": "directory", "uid": 498}
ok: [compute002] => (item=/var/lib/slurm/slurmd) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/lib/slurm/slurmd", "mode": "0755", "owner": "slurm", "path": "/var/lib/slurm/slurmd", "size": 4096, "state": "directory", "uid": 498}
ok: [compute001] => (item=/var/lib/slurm/slurmd) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/lib/slurm/slurmd", "mode": "0755", "owner": "slurm", "path": "/var/lib/slurm/slurmd", "size": 4096, "state": "directory", "uid": 498}
ok: [compute001] => (item=/var/log/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/log/slurm", "mode": "0755", "owner": "slurm", "path": "/var/log/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [compute002] => (item=/var/log/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/log/slurm", "mode": "0755", "owner": "slurm", "path": "/var/log/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [compute001] => (item=/var/run/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/run/slurm", "mode": "0755", "owner": "slurm", "path": "/var/run/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [compute002] => (item=/var/run/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/run/slurm", "mode": "0755", "owner": "slurm", "path": "/var/run/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [compute002] => (item=/var/spool/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/spool/slurm", "mode": "0755", "owner": "slurm", "path": "/var/spool/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [compute001] => (item=/var/spool/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/spool/slurm", "mode": "0755", "owner": "slurm", "path": "/var/spool/slurm", "size": 4096, "state": "directory", "uid": 498}

TASK [slurm-common : Make compatibility symlinks] ******************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:56
skipping: [compute001] => (item={u'to': u'/etc/slurm-llnl', u'from': u'/etc/slurm'})  => {"changed": false, "item": {"from": "/etc/slurm", "to": "/etc/slurm-llnl"}, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => (item={u'to': u'/var/lib/slurm-llnl', u'from': u'/var/lib/slurm'})  => {"changed": false, "item": {"from": "/var/lib/slurm", "to": "/var/lib/slurm-llnl"}, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item={u'to': u'/etc/slurm-llnl', u'from': u'/etc/slurm'})  => {"changed": false, "item": {"from": "/etc/slurm", "to": "/etc/slurm-llnl"}, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => (item={u'to': u'/var/log/slurm-llnl', u'from': u'/var/log/slurm'})  => {"changed": false, "item": {"from": "/var/log/slurm", "to": "/var/log/slurm-llnl"}, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item={u'to': u'/var/lib/slurm-llnl', u'from': u'/var/lib/slurm'})  => {"changed": false, "item": {"from": "/var/lib/slurm", "to": "/var/lib/slurm-llnl"}, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => (item={u'to': u'/var/run/slurm-llnl', u'from': u'/var/run/slurm'})  => {"changed": false, "item": {"from": "/var/run/slurm", "to": "/var/run/slurm-llnl"}, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item={u'to': u'/var/log/slurm-llnl', u'from': u'/var/log/slurm'})  => {"changed": false, "item": {"from": "/var/log/slurm", "to": "/var/log/slurm-llnl"}, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item={u'to': u'/var/run/slurm-llnl', u'from': u'/var/run/slurm'})  => {"changed": false, "item": {"from": "/var/run/slurm", "to": "/var/run/slurm-llnl"}, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-common : Enable Copr SLURM repo by verdurin] ***********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:73
ok: [compute001] => {"changed": false, "gid": 0, "group": "root", "mode": "0444", "owner": "root", "path": "/etc/yum.repos.d/copr-slurm.repo", "size": 615, "state": "file", "uid": 0}
ok: [compute002] => {"changed": false, "gid": 0, "group": "root", "mode": "0444", "owner": "root", "path": "/etc/yum.repos.d/copr-slurm.repo", "size": 615, "state": "file", "uid": 0}

TASK [slurm-common : Prevent MUNGE from starting] ******************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:20
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-common : Install MUNGE (Debian/Ubuntu)] ****************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:31
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-common : Install MUNGE (RHEL-compatible)] **************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:43
ok: [compute001] => {"changed": false, "msg": "", "rc": 0, "results": ["slurm-munge-17.02.7-1.el6.x86_64 providing slurm-munge is already installed"]}
ok: [compute002] => {"changed": false, "msg": "", "rc": 0, "results": ["slurm-munge-17.02.7-1.el6.x86_64 providing slurm-munge is already installed"]}

TASK [slurm-common : Allow MUNGE to start] *************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:57
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-common : Add `--syslog` option to `munged` startup] ****************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:66
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-common : Reload systemd configuration] *****************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:77
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-common : Add `--syslog` startup options for `munged` (Ubuntu 14.04)] ***
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:86
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-common : Configure MUNGE] ******************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:101
ok: [compute001] => {"changed": false, "checksum": "222afe887f907039b799b394715664ffa584fec5", "dest": "/etc/munge/munge.key", "gid": 497, "group": "munge", "mode": "0400", "owner": "munge", "path": "/etc/munge/munge.key", "size": 1024, "state": "file", "uid": 497}
ok: [compute002] => {"changed": false, "checksum": "222afe887f907039b799b394715664ffa584fec5", "dest": "/etc/munge/munge.key", "gid": 497, "group": "munge", "mode": "0400", "owner": "munge", "path": "/etc/munge/munge.key", "size": 1024, "state": "file", "uid": 497}

TASK [slurm-common : Ensure the MUNGE service is running] **********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:114
ok: [compute001] => {"changed": false, "enabled": true, "name": "munge", "state": "started"}
ok: [compute002] => {"changed": false, "enabled": true, "name": "munge", "state": "started"}

TASK [slurm-common : Deploy SLURM configuration file] **************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:86
ok: [compute001] => {"changed": false, "gid": 0, "group": "root", "mode": "0444", "owner": "root", "path": "/etc/slurm/slurm.conf", "size": 4157, "state": "file", "uid": 0}
ok: [compute002] => {"changed": false, "gid": 0, "group": "root", "mode": "0444", "owner": "root", "path": "/etc/slurm/slurm.conf", "size": 4157, "state": "file", "uid": 0}

TASK [slurm-common : Deploy script to suspend cloud instance] ******************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:96
ok: [compute002] => {"changed": false, "gid": 0, "group": "root", "mode": "0555", "owner": "root", "path": "/etc/slurm/slurm.suspend.sh", "size": 604, "state": "file", "uid": 0}
ok: [compute001] => {"changed": false, "gid": 0, "group": "root", "mode": "0555", "owner": "root", "path": "/etc/slurm/slurm.suspend.sh", "size": 604, "state": "file", "uid": 0}

TASK [slurm-common : Deploy script to resume cloud instance] *******************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:106
ok: [compute002] => {"changed": false, "gid": 0, "group": "root", "mode": "0555", "owner": "root", "path": "/etc/slurm/slurm.resume.sh", "size": 631, "state": "file", "uid": 0}
ok: [compute001] => {"changed": false, "gid": 0, "group": "root", "mode": "0555", "owner": "root", "path": "/etc/slurm/slurm.resume.sh", "size": 631, "state": "file", "uid": 0}

TASK [slurm-common : Install support packages (Debian/Ubuntu)] *****************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:116
skipping: [compute001] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-common : Install SLURM packages (RHEL-compatible)] *****************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:127
skipping: [compute001] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-client : Install required SLURM packages (Debian/Ubuntu)] **********
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-client/tasks/main.yml:4
skipping: [compute001] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-client : Install required SLURM packages (older Debian/Ubuntu)] ****
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-client/tasks/main.yml:23
skipping: [compute001] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-client : service] **************************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-client/tasks/main.yml:35
skipping: [compute001] => (item=slurm-llnl)  => {"changed": false, "item": "slurm-llnl", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item=slurm-llnl)  => {"changed": false, "item": "slurm-llnl", "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-client : Install SLURM packages (RHEL-compatible)] *****************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-client/tasks/main.yml:47
ok: [compute001] => (item=[u'slurm', u'slurm-devel', u'slurm-perlapi', u'slurm-contribs', u'slurm-torque']) => {"changed": false, "item": ["slurm", "slurm-devel", "slurm-perlapi", "slurm-contribs", "slurm-torque"], "msg": "", "rc": 0, "results": ["slurm-17.02.7-1.el6.x86_64 providing slurm is already installed", "slurm-devel-17.02.7-1.el6.x86_64 providing slurm-devel is already installed", "slurm-perlapi-17.02.7-1.el6.x86_64 providing slurm-perlapi is already installed", "slurm-contribs-17.02.7-1.el6.x86_64 providing slurm-contribs is already installed", "slurm-torque-17.02.7-1.el6.x86_64 providing slurm-torque is already installed"]}
ok: [compute002] => (item=[u'slurm', u'slurm-devel', u'slurm-perlapi', u'slurm-contribs', u'slurm-torque']) => {"changed": false, "item": ["slurm", "slurm-devel", "slurm-perlapi", "slurm-contribs", "slurm-torque"], "msg": "", "rc": 0, "results": ["slurm-17.02.7-1.el6.x86_64 providing slurm is already installed", "slurm-devel-17.02.7-1.el6.x86_64 providing slurm-devel is already installed", "slurm-perlapi-17.02.7-1.el6.x86_64 providing slurm-perlapi is already installed", "slurm-contribs-17.02.7-1.el6.x86_64 providing slurm-contribs is already installed", "slurm-torque-17.02.7-1.el6.x86_64 providing slurm-torque is already installed"]}

TASK [slurm-client : Stop SLURM services (RHEL/CentOS 6.x)] ********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-client/tasks/main.yml:59
changed: [compute002] => {"changed": true, "enabled": false, "name": "slurm", "state": "stopped"}
changed: [compute001] => {"changed": true, "enabled": false, "name": "slurm", "state": "stopped"}

TASK [slurm-client : Stop SLURM services (RHEL/CentOS 7.x)] ********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-client/tasks/main.yml:66
skipping: [compute001] => (item=slurmd)  => {"changed": false, "item": "slurmd", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => (item=slurmctld)  => {"changed": false, "item": "slurmctld", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item=slurmd)  => {"changed": false, "item": "slurmd", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item=slurmctld)  => {"changed": false, "item": "slurmctld", "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-worker : Load distribution-specific parameters] ********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/main.yml:4
included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/init-RedHat.yml for compute001, compute002

TASK [slurm-worker : Set SLURM worker playbook params (RHEL compatible)] *******
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/init-RedHat.yml:3
ok: [compute001] => {"ansible_facts": {"slurmd_packages": ["slurm", "slurm-plugins", "slurm-sql"]}, "changed": false}
ok: [compute002] => {"ansible_facts": {"slurmd_packages": ["slurm", "slurm-plugins", "slurm-sql"]}, "changed": false}

TASK [slurm-worker : Set SLURM worker service name (RHEL 7.x compatible)] ******
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/init-RedHat.yml:11
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-worker : Set SLURM worker service name (RHEL 6.x compatible)] ******
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/init-RedHat.yml:16
ok: [compute001] => {"ansible_facts": {"slurmd_service": "slurm"}, "changed": false}
ok: [compute002] => {"ansible_facts": {"slurmd_service": "slurm"}, "changed": false}

TASK [slurm-worker : Deploy SLURM configuration files] *************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/main.yml:7
ok: [compute001] => (item=gres.conf) => {"changed": false, "gid": 0, "group": "root", "item": "gres.conf", "mode": "0444", "owner": "root", "path": "/etc/slurm/gres.conf", "size": 297, "state": "file", "uid": 0}
ok: [compute002] => (item=gres.conf) => {"changed": false, "gid": 0, "group": "root", "item": "gres.conf", "mode": "0444", "owner": "root", "path": "/etc/slurm/gres.conf", "size": 297, "state": "file", "uid": 0}

TASK [slurm-worker : Deploy kernel config check script] ************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/cgroup.yml:3
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-worker : Check that kernel is configured for cgroups] **************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/cgroup.yml:12
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-worker : Ensure cgroup filesystems are mounted] ********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/cgroup.yml:18
skipping: [compute001] => (item=blkio)  => {"changed": false, "item": "blkio", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => (item=cpuacct)  => {"changed": false, "item": "cpuacct", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item=blkio)  => {"changed": false, "item": "blkio", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => (item=cpuset)  => {"changed": false, "item": "cpuset", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item=cpuacct)  => {"changed": false, "item": "cpuacct", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => (item=devices)  => {"changed": false, "item": "devices", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item=cpuset)  => {"changed": false, "item": "cpuset", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => (item=freezer)  => {"changed": false, "item": "freezer", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item=devices)  => {"changed": false, "item": "devices", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => (item=memory)  => {"changed": false, "item": "memory", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item=freezer)  => {"changed": false, "item": "freezer", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item=memory)  => {"changed": false, "item": "memory", "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-worker : Ensure kernel is booted with swap accounting enabled] *****
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/cgroup.yml:44
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-worker : Check if swap accouting is enabled (may fail!)] ***********
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/cgroup.yml:53
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-worker : Reboot to enable swap accounting] *************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/cgroup.yml:59
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-worker : Wait for server to come up again] *************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/cgroup.yml:72
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-worker : Ensure release agent directory exists] ********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/cgroup.yml:84
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-worker : Deploy cgroup-specific release agent script] **************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/cgroup.yml:93
skipping: [compute001] => (item=blkio)  => {"changed": false, "item": "blkio", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => (item=cpuacct)  => {"changed": false, "item": "cpuacct", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item=blkio)  => {"changed": false, "item": "blkio", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item=cpuacct)  => {"changed": false, "item": "cpuacct", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => (item=cpuset)  => {"changed": false, "item": "cpuset", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => (item=devices)  => {"changed": false, "item": "devices", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item=cpuset)  => {"changed": false, "item": "cpuset", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item=devices)  => {"changed": false, "item": "devices", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => (item=freezer)  => {"changed": false, "item": "freezer", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item=freezer)  => {"changed": false, "item": "freezer", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute001] => (item=memory)  => {"changed": false, "item": "memory", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => (item=memory)  => {"changed": false, "item": "memory", "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-worker : Deploy SLURM cgroup configuration file (main)] ************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/cgroup.yml:103
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-worker : Deploy SLURM cgroup configuration file (devices)] *********
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/cgroup.yml:112
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-worker : Install SLURM worker packages] ****************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/main.yml:27
ok: [compute002] => (item=[u'slurm', u'slurm-plugins', u'slurm-sql']) => {"changed": false, "item": ["slurm", "slurm-plugins", "slurm-sql"], "msg": "", "rc": 0, "results": ["slurm-17.02.7-1.el6.x86_64 providing slurm is already installed", "slurm-plugins-17.02.7-1.el6.x86_64 providing slurm-plugins is already installed", "slurm-sql-17.02.7-1.el6.x86_64 providing slurm-sql is already installed"]}
ok: [compute001] => (item=[u'slurm', u'slurm-plugins', u'slurm-sql']) => {"changed": false, "item": ["slurm", "slurm-plugins", "slurm-sql"], "msg": "", "rc": 0, "results": ["slurm-17.02.7-1.el6.x86_64 providing slurm is already installed", "slurm-plugins-17.02.7-1.el6.x86_64 providing slurm-plugins is already installed", "slurm-sql-17.02.7-1.el6.x86_64 providing slurm-sql is already installed"]}

TASK [slurm-worker : Ensure SLURMd starts at boot] *****************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/main.yml:33
changed: [compute001] => {"changed": true, "enabled": true, "name": "slurm"}
changed: [compute002] => {"changed": true, "enabled": true, "name": "slurm"}

PLAY [Restart SLURMd after all config is done] *********************************

TASK [setup] *******************************************************************
ok: [compute002]
ok: [compute001]

TASK [service] *****************************************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm.yml:47
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [service] *****************************************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm.yml:51
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [service] *****************************************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm.yml:55
skipping: [compute001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [compute002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [service] *****************************************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm.yml:59
changed: [compute002] => {"changed": true, "name": "slurm", "state": "started"}
changed: [compute001] => {"changed": true, "name": "slurm", "state": "started"}

PLAY [Apply local customizations (after)] **************************************

TASK [setup] *******************************************************************
ok: [frontend001]
ok: [compute001]
ok: [compute002]

PLAY [Report success on cluster creation] **************************************

TASK [Mark host as successfully configured] ************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/site.yml:79
changed: [frontend001 -> localhost] => {"backup": "", "changed": true, "msg": "line added"}
changed: [compute001 -> localhost] => {"backup": "", "changed": true, "msg": "line added"}
changed: [compute002 -> localhost] => {"backup": "", "changed": true, "msg": "line added"}

PLAY RECAP *********************************************************************
compute001                 : ok=123  changed=41   unreachable=0    failed=0   
compute002                 : ok=123  changed=41   unreachable=0    failed=0   
frontend001                : ok=130  changed=46   unreachable=0    failed=0   

2018-04-28 18:37:24 hwc hwc[5245] INFO Cluster correctly configured.
Starting cluster `slurm` with:
* 1 frontend nodes.
* 2 compute nodes.
(This may take a while...)
Configuring the cluster ...
(this too may take a while)

Your cluster `slurm` is ready!

Cluster name:     slurm
Cluster template: slurm
Default ssh to node: frontend001
- frontend nodes: 1
- compute nodes: 2

To login on the frontend node, run the command:

    elasticluster ssh slurm

To upload or download files to the cluster, use the command:

    elasticluster sftp slurm


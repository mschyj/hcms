2018-05-02 12:11:20 hwc hwc[18431] DEBUG Checking section `cluster/slurm` ...
2018-05-02 12:11:20 hwc hwc[18431] DEBUG Checking section `login/centos` ...
2018-05-02 12:11:20 hwc hwc[18431] DEBUG Checking section `setup/ansible-slurm` ...
2018-05-02 12:11:20 hwc hwc[18431] DEBUG Checking section `cloud/catalyst` ...
2018-05-02 12:11:20 hwc hwc[18431] DEBUG Using class <class 'elasticluster.providers.ansible_provider.AnsibleSetupProvider'> from module <module 'elasticluster.providers.ansible_provider' from '/usr/local/lib/python2.7/site-packages/elasticluster-1.3.dev1-py2.7.egg/elasticluster/providers/ansible_provider.pyc'> to instanciate provider 'ansible'
2018-05-02 12:11:20 hwc hwc[18431] DEBUG setting variable user_ak=CNYVGDKK2PPBQEEMUYQG for node kind master
2018-05-02 12:11:20 hwc hwc[18431] DEBUG setting variable user_obs_url=http://obs.cn-north-1.myhwclouds.com for node kind master
2018-05-02 12:11:20 hwc hwc[18431] DEBUG setting variable user_client_home=/root/hwc for node kind master
2018-05-02 12:11:20 hwc hwc[18431] DEBUG setting variable upgrade_packages=no for node kind master
2018-05-02 12:11:20 hwc hwc[18431] DEBUG setting variable user_obs_endpoint=cn-north-1 for node kind master
2018-05-02 12:11:20 hwc hwc[18431] DEBUG setting variable user_obs_name=hwc-obs for node kind master
2018-05-02 12:11:20 hwc hwc[18431] DEBUG setting variable user_sfs_url=sfs-nas1.cn-north-1.myhuaweicloud.com:/share-480fca1b for node kind master
2018-05-02 12:11:20 hwc hwc[18431] DEBUG setting variable user_client_ip=192.168.0.230 for node kind master
2018-05-02 12:11:20 hwc hwc[18431] DEBUG setting variable user_sk=6imAmmCVVzMqR1dm5x0AKJcL1BILA7TlOsSMhu4G for node kind master
2018-05-02 12:11:20 hwc hwc[18431] DEBUG setting variable user_ak=CNYVGDKK2PPBQEEMUYQG for node kind worker
2018-05-02 12:11:20 hwc hwc[18431] DEBUG setting variable user_obs_url=http://obs.cn-north-1.myhwclouds.com for node kind worker
2018-05-02 12:11:20 hwc hwc[18431] DEBUG setting variable user_client_home=/root/hwc for node kind worker
2018-05-02 12:11:20 hwc hwc[18431] DEBUG setting variable upgrade_packages=no for node kind worker
2018-05-02 12:11:20 hwc hwc[18431] DEBUG setting variable user_obs_endpoint=cn-north-1 for node kind worker
2018-05-02 12:11:20 hwc hwc[18431] DEBUG setting variable user_obs_name=hwc-obs for node kind worker
2018-05-02 12:11:20 hwc hwc[18431] DEBUG setting variable user_sfs_url=sfs-nas1.cn-north-1.myhuaweicloud.com:/share-480fca1b for node kind worker
2018-05-02 12:11:20 hwc hwc[18431] DEBUG setting variable user_client_ip=192.168.0.230 for node kind worker
2018-05-02 12:11:20 hwc hwc[18431] DEBUG setting variable user_sk=6imAmmCVVzMqR1dm5x0AKJcL1BILA7TlOsSMhu4G for node kind worker
2018-05-02 12:11:20 hwc hwc[18431] DEBUG Using class <class 'elasticluster.providers.openstack.OpenStackCloudProvider'> from module <module 'elasticluster.providers.openstack' from '/usr/local/lib/python2.7/site-packages/elasticluster-1.3.dev1-py2.7.egg/elasticluster/providers/openstack.pyc'> to instanciate provider 'openstack'
2018-05-02 12:11:20 hwc hwc[18431] DEBUG OpenStack auth URL taken from env variable OS_AUTH_URL
2018-05-02 12:11:20 hwc hwc[18431] DEBUG OpenStack user name taken from env variable OS_USERNAME
2018-05-02 12:11:20 hwc hwc[18431] DEBUG OpenStack user domain name taken from env variable OS_USER_DOMAIN_NAME
2018-05-02 12:11:20 hwc hwc[18431] DEBUG OpenStack password taken from env variable OS_PASSWORD
2018-05-02 12:11:20 hwc hwc[18431] DEBUG OpenStack project name taken from env variable OS_PROJECT_NAME
2018-05-02 12:11:20 hwc hwc[18431] DEBUG OpenStack project domain name taken from env variable OS_PROJECT_DOMAIN_NAME
2018-05-02 12:11:20 hwc hwc[18431] DEBUG Initializing OpenStack API clients: OS_AUTH_URL='https://iam.cn-north-1.myhwclouds.com/v3' OS_USERNAME='mschyj' OS_USER_DOMAIN_NAME='mschyj' OS_PROJECT_NAME='cn-north-1' OS_PROJECT_DOMAIN_NAME='mschyj' OS_REGION_NAME='cn-north-1'
2018-05-02 12:11:20 hwc hwc[18431] INFO Using Keystone API v3 session to authenticate to OpenStack
2018-05-02 12:11:20 hwc hwc[18431] DEBUG Creating OpenStack Compute API (Nova) v2 client ...
2018-05-02 12:11:20 hwc hwc[18431] DEBUG Creating OpenStack Network API (Neutron) client ...
2018-05-02 12:11:20 hwc hwc[18431] DEBUG Creating OpenStack Image API (Glance) v2 client ...
2018-05-02 12:11:20 hwc hwc[18431] DEBUG Creating OpenStack Volume API (Cinder) v2 client ...
2018-05-02 12:11:23 hwc hwc[18431] INFO Starting cluster nodes (timeout: 600 seconds) ...
2018-05-02 12:11:23 hwc hwc[18431] DEBUG Note: starting 4 nodes concurrently.
2018-05-02 12:11:23 hwc hwc[18431] DEBUG _start_node: working on node `worker001`
2018-05-02 12:11:23 hwc hwc[18431] DEBUG Getting information for instance 4cd26d3b-5de0-4c6a-8656-820df5c7779d
2018-05-02 12:11:23 hwc hwc[18431] DEBUG _start_node: working on node `worker002`
2018-05-02 12:11:23 hwc hwc[18431] DEBUG Getting information for instance d2fb6da3-0787-4a55-9ba6-9ac8e83e152f
2018-05-02 12:11:23 hwc hwc[18431] DEBUG _start_node: working on node `worker003`
2018-05-02 12:11:23 hwc hwc[18431] INFO Starting node `worker003` from image `befbd7ca-dd7a-4737-bc60-33a1cdf6ea8b` with flavor c1.medium ...
2018-05-02 12:11:23 hwc hwc[18431] DEBUG Checking keypair `hwc-key` ...
2018-05-02 12:11:23 hwc hwc[18431] DEBUG _start_node: working on node `master001`
2018-05-02 12:11:23 hwc hwc[18431] DEBUG Getting information for instance e9cbe5c5-85ea-4b4c-a99f-659fdf152663
2018-05-02 12:11:23 hwc hwc[18431] DEBUG Checking existence of security group(s) ['security-mschyj'] ...
2018-05-02 12:11:23 hwc hwc[18431] DEBUG node `worker002` (instance id d2fb6da3-0787-4a55-9ba6-9ac8e83e152f) is up.
2018-05-02 12:11:23 hwc hwc[18431] DEBUG node `master001` (instance id e9cbe5c5-85ea-4b4c-a99f-659fdf152663) is up.
2018-05-02 12:11:24 hwc hwc[18431] INFO Not starting node `master001` which is already up.
2018-05-02 12:11:24 hwc hwc[18431] DEBUG node `worker001` (instance id 4cd26d3b-5de0-4c6a-8656-820df5c7779d) is up.
2018-05-02 12:11:24 hwc hwc[18431] INFO Not starting node `worker001` which is already up.
2018-05-02 12:11:24 hwc hwc[18431] INFO Not starting node `worker002` which is already up.
2018-05-02 12:11:37 hwc hwc[18431] DEBUG Specifying networks for node slurm-worker003: 6c7d662f-eb15-4a44-b2ef-2e71ff7fce15
2018-05-02 12:11:42 hwc hwc[18431] DEBUG Node `worker003` has instance ID `d3d19c9a-a839-4cd0-ade5-f72c966346f5`
2018-05-02 12:11:42 hwc hwc[18431] INFO Node `worker003` has been started.
2018-05-02 12:11:42 hwc hwc[18431] DEBUG Getting information for instance e9cbe5c5-85ea-4b4c-a99f-659fdf152663
2018-05-02 12:11:43 hwc hwc[18431] DEBUG node `master001` (instance id e9cbe5c5-85ea-4b4c-a99f-659fdf152663) is up.
2018-05-02 12:11:44 hwc hwc[18431] DEBUG Getting information for instance d3d19c9a-a839-4cd0-ade5-f72c966346f5
2018-05-02 12:11:45 hwc hwc[18431] DEBUG node `worker003` (instance id `d3d19c9a-a839-4cd0-ade5-f72c966346f5`) still building...
2018-05-02 12:11:45 hwc hwc[18431] DEBUG Getting information for instance 4cd26d3b-5de0-4c6a-8656-820df5c7779d
2018-05-02 12:11:45 hwc hwc[18431] DEBUG node `worker001` (instance id 4cd26d3b-5de0-4c6a-8656-820df5c7779d) is up.
2018-05-02 12:11:46 hwc hwc[18431] DEBUG Getting information for instance d2fb6da3-0787-4a55-9ba6-9ac8e83e152f
2018-05-02 12:11:46 hwc hwc[18431] DEBUG node `worker002` (instance id d2fb6da3-0787-4a55-9ba6-9ac8e83e152f) is up.
2018-05-02 12:11:46 hwc hwc[18431] DEBUG Waiting for 1 more nodes to come up ...
2018-05-02 12:11:56 hwc hwc[18431] DEBUG Getting information for instance d3d19c9a-a839-4cd0-ade5-f72c966346f5
2018-05-02 12:11:57 hwc hwc[18431] DEBUG node `worker003` (instance id `d3d19c9a-a839-4cd0-ade5-f72c966346f5`) still building...
2018-05-02 12:11:57 hwc hwc[18431] DEBUG Waiting for 1 more nodes to come up ...
2018-05-02 12:12:07 hwc hwc[18431] DEBUG Getting information for instance d3d19c9a-a839-4cd0-ade5-f72c966346f5
2018-05-02 12:12:08 hwc hwc[18431] DEBUG node `worker003` (instance id `d3d19c9a-a839-4cd0-ade5-f72c966346f5`) still building...
2018-05-02 12:12:08 hwc hwc[18431] DEBUG Waiting for 1 more nodes to come up ...
2018-05-02 12:12:18 hwc hwc[18431] DEBUG Getting information for instance d3d19c9a-a839-4cd0-ade5-f72c966346f5
2018-05-02 12:12:18 hwc hwc[18431] DEBUG node `worker003` (instance id `d3d19c9a-a839-4cd0-ade5-f72c966346f5`) still building...
2018-05-02 12:12:18 hwc hwc[18431] DEBUG Waiting for 1 more nodes to come up ...
2018-05-02 12:12:28 hwc hwc[18431] DEBUG Getting information for instance d3d19c9a-a839-4cd0-ade5-f72c966346f5
2018-05-02 12:12:28 hwc hwc[18431] DEBUG node `worker003` (instance id `d3d19c9a-a839-4cd0-ade5-f72c966346f5`) still building...
2018-05-02 12:12:28 hwc hwc[18431] DEBUG Waiting for 1 more nodes to come up ...
2018-05-02 12:12:38 hwc hwc[18431] DEBUG Getting information for instance d3d19c9a-a839-4cd0-ade5-f72c966346f5
2018-05-02 12:12:39 hwc hwc[18431] DEBUG node `worker003` (instance id `d3d19c9a-a839-4cd0-ade5-f72c966346f5`) still building...
2018-05-02 12:12:39 hwc hwc[18431] DEBUG Waiting for 1 more nodes to come up ...
2018-05-02 12:12:49 hwc hwc[18431] DEBUG Getting information for instance d3d19c9a-a839-4cd0-ade5-f72c966346f5
2018-05-02 12:12:49 hwc hwc[18431] DEBUG node `worker003` (instance id `d3d19c9a-a839-4cd0-ade5-f72c966346f5`) still building...
2018-05-02 12:12:49 hwc hwc[18431] DEBUG Waiting for 1 more nodes to come up ...
2018-05-02 12:12:59 hwc hwc[18431] DEBUG Getting information for instance d3d19c9a-a839-4cd0-ade5-f72c966346f5
2018-05-02 12:13:00 hwc hwc[18431] DEBUG node `worker003` (instance id `d3d19c9a-a839-4cd0-ade5-f72c966346f5`) still building...
2018-05-02 12:13:00 hwc hwc[18431] DEBUG Waiting for 1 more nodes to come up ...
2018-05-02 12:13:10 hwc hwc[18431] DEBUG Getting information for instance d3d19c9a-a839-4cd0-ade5-f72c966346f5
2018-05-02 12:13:10 hwc hwc[18431] DEBUG node `worker003` (instance id `d3d19c9a-a839-4cd0-ade5-f72c966346f5`) still building...
2018-05-02 12:13:10 hwc hwc[18431] DEBUG Waiting for 1 more nodes to come up ...
2018-05-02 12:13:20 hwc hwc[18431] DEBUG Getting information for instance d3d19c9a-a839-4cd0-ade5-f72c966346f5
2018-05-02 12:13:21 hwc hwc[18431] DEBUG node `worker003` (instance id `d3d19c9a-a839-4cd0-ade5-f72c966346f5`) still building...
2018-05-02 12:13:21 hwc hwc[18431] DEBUG Waiting for 1 more nodes to come up ...
2018-05-02 12:13:31 hwc hwc[18431] DEBUG Getting information for instance d3d19c9a-a839-4cd0-ade5-f72c966346f5
2018-05-02 12:13:32 hwc hwc[18431] DEBUG node `worker003` (instance id `d3d19c9a-a839-4cd0-ade5-f72c966346f5`) still building...
2018-05-02 12:13:32 hwc hwc[18431] DEBUG Waiting for 1 more nodes to come up ...
2018-05-02 12:13:42 hwc hwc[18431] DEBUG Getting information for instance d3d19c9a-a839-4cd0-ade5-f72c966346f5
2018-05-02 12:13:43 hwc hwc[18431] DEBUG node `worker003` (instance id `d3d19c9a-a839-4cd0-ade5-f72c966346f5`) still building...
2018-05-02 12:13:43 hwc hwc[18431] DEBUG Waiting for 1 more nodes to come up ...
2018-05-02 12:13:53 hwc hwc[18431] DEBUG Getting information for instance d3d19c9a-a839-4cd0-ade5-f72c966346f5
2018-05-02 12:13:53 hwc hwc[18431] DEBUG node `worker003` (instance id `d3d19c9a-a839-4cd0-ade5-f72c966346f5`) still building...
2018-05-02 12:13:53 hwc hwc[18431] DEBUG Waiting for 1 more nodes to come up ...
2018-05-02 12:14:03 hwc hwc[18431] DEBUG Getting information for instance d3d19c9a-a839-4cd0-ade5-f72c966346f5
2018-05-02 12:14:03 hwc hwc[18431] DEBUG node `worker003` (instance id `d3d19c9a-a839-4cd0-ade5-f72c966346f5`) still building...
2018-05-02 12:14:03 hwc hwc[18431] DEBUG Waiting for 1 more nodes to come up ...
2018-05-02 12:14:13 hwc hwc[18431] DEBUG Getting information for instance d3d19c9a-a839-4cd0-ade5-f72c966346f5
2018-05-02 12:14:14 hwc hwc[18431] DEBUG node `worker003` (instance id `d3d19c9a-a839-4cd0-ade5-f72c966346f5`) still building...
2018-05-02 12:14:14 hwc hwc[18431] DEBUG Waiting for 1 more nodes to come up ...
2018-05-02 12:14:24 hwc hwc[18431] DEBUG Getting information for instance d3d19c9a-a839-4cd0-ade5-f72c966346f5
2018-05-02 12:14:24 hwc hwc[18431] DEBUG node `worker003` (instance id `d3d19c9a-a839-4cd0-ade5-f72c966346f5`) still building...
2018-05-02 12:14:24 hwc hwc[18431] DEBUG Waiting for 1 more nodes to come up ...
2018-05-02 12:14:34 hwc hwc[18431] DEBUG Getting information for instance d3d19c9a-a839-4cd0-ade5-f72c966346f5
2018-05-02 12:14:35 hwc hwc[18431] DEBUG node `worker003` (instance id `d3d19c9a-a839-4cd0-ade5-f72c966346f5`) still building...
2018-05-02 12:14:35 hwc hwc[18431] DEBUG Waiting for 1 more nodes to come up ...
2018-05-02 12:14:45 hwc hwc[18431] DEBUG Getting information for instance d3d19c9a-a839-4cd0-ade5-f72c966346f5
2018-05-02 12:14:45 hwc hwc[18431] DEBUG node `worker003` (instance id `d3d19c9a-a839-4cd0-ade5-f72c966346f5`) still building...
2018-05-02 12:14:45 hwc hwc[18431] DEBUG Waiting for 1 more nodes to come up ...
2018-05-02 12:14:55 hwc hwc[18431] DEBUG Getting information for instance d3d19c9a-a839-4cd0-ade5-f72c966346f5
2018-05-02 12:14:56 hwc hwc[18431] DEBUG node `worker003` (instance id d3d19c9a-a839-4cd0-ade5-f72c966346f5) is up.
2018-05-02 12:14:57 hwc hwc[18431] INFO Checking SSH connection to nodes (timeout: 600 seconds) ...
2018-05-02 12:14:57 hwc hwc[18431] DEBUG Trying to connect to host worker001 (192.168.0.101) ...
2018-05-02 12:14:57 hwc hwc[18431] DEBUG Connection to 192.168.0.101 succeeded on port 22, will use this IP address for future connections.
2018-05-02 12:14:57 hwc hwc[18431] INFO Connection to node `worker001` successful, using IP address 192.168.0.101 to connect.
2018-05-02 12:14:57 hwc hwc[18431] DEBUG Trying to connect to host master001 (192.168.0.94) ...
2018-05-02 12:14:57 hwc hwc[18431] DEBUG Connection to 192.168.0.94 succeeded on port 22, will use this IP address for future connections.
2018-05-02 12:14:57 hwc hwc[18431] INFO Connection to node `master001` successful, using IP address 192.168.0.94 to connect.
2018-05-02 12:14:57 hwc hwc[18431] DEBUG Trying to connect to host worker002 (192.168.0.31) ...
2018-05-02 12:14:57 hwc hwc[18431] DEBUG Connection to 192.168.0.31 succeeded on port 22, will use this IP address for future connections.
2018-05-02 12:14:57 hwc hwc[18431] INFO Connection to node `worker002` successful, using IP address 192.168.0.31 to connect.
2018-05-02 12:14:57 hwc hwc[18431] DEBUG Trying to connect to host worker003 (192.168.0.231) ...
2018-05-02 12:15:02 hwc hwc[18431] DEBUG Host worker003 (192.168.0.231) not reachable within 5 seconds: timed out -- <class 'socket.timeout'>
2018-05-02 12:15:12 hwc hwc[18431] DEBUG Trying to connect to host worker003 (192.168.0.231) ...
2018-05-02 12:15:13 hwc hwc[18431] DEBUG Host worker003 (192.168.0.231) not reachable within 5 seconds: [Errno None] Unable to connect to port 22 on 192.168.0.231 -- <class 'paramiko.ssh_exception.NoValidConnectionsError'>
2018-05-02 12:15:23 hwc hwc[18431] DEBUG Trying to connect to host worker003 (192.168.0.231) ...
2018-05-02 12:15:23 hwc hwc[18431] DEBUG Host worker003 (192.168.0.231) not reachable within 5 seconds: [Errno None] Unable to connect to port 22 on 192.168.0.231 -- <class 'paramiko.ssh_exception.NoValidConnectionsError'>
2018-05-02 12:15:33 hwc hwc[18431] DEBUG Trying to connect to host worker003 (192.168.0.231) ...
2018-05-02 12:15:33 hwc hwc[18431] DEBUG Host worker003 (192.168.0.231) not reachable within 5 seconds: [Errno None] Unable to connect to port 22 on 192.168.0.231 -- <class 'paramiko.ssh_exception.NoValidConnectionsError'>
2018-05-02 12:15:43 hwc hwc[18431] DEBUG Trying to connect to host worker003 (192.168.0.231) ...
2018-05-02 12:15:44 hwc hwc[18431] DEBUG Connection to 192.168.0.231 succeeded on port 22, will use this IP address for future connections.
2018-05-02 12:15:44 hwc hwc[18431] INFO Connection to node `worker003` successful, using IP address 192.168.0.231 to connect.
2018-05-02 12:15:44 hwc hwc[18431] DEBUG Writing Ansible inventory to file `/root/hwc/cfg/storage/slurm.inventory` ...
2018-05-02 12:15:44 hwc hwc[18431] DEBUG Calling `ansible-playbook` with the following environment:
2018-05-02 12:15:44 hwc hwc[18431] DEBUG - ANSIBLE_ANY_ERRORS_FATAL='yes'
2018-05-02 12:15:44 hwc hwc[18431] DEBUG - ANSIBLE_FORKS='10'
2018-05-02 12:15:44 hwc hwc[18431] DEBUG - ANSIBLE_HOST_KEY_CHECKING='no'
2018-05-02 12:15:44 hwc hwc[18431] DEBUG - ANSIBLE_RETRY_FILES_ENABLED='no'
2018-05-02 12:15:44 hwc hwc[18431] DEBUG - ANSIBLE_ROLES_PATH='/root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles:/root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks:/etc/ansible/roles'
2018-05-02 12:15:44 hwc hwc[18431] DEBUG - ANSIBLE_SSH_PIPELINING='yes'
2018-05-02 12:15:44 hwc hwc[18431] DEBUG - ANSIBLE_TIMEOUT='120'
2018-05-02 12:15:44 hwc hwc[18431] DEBUG - CINDER_ENDPOINT_TYPE='publicURL'
2018-05-02 12:15:44 hwc hwc[18431] DEBUG - CLUSTER_HOME='/root/elasticluster'
2018-05-02 12:15:44 hwc hwc[18431] DEBUG - CVS_RSH='ssh'
2018-05-02 12:15:44 hwc hwc[18431] DEBUG - G_BROKEN_FILENAMES='1'
2018-05-02 12:15:44 hwc hwc[18431] DEBUG - HISTCONTROL='ignoredups'
2018-05-02 12:15:44 hwc hwc[18431] DEBUG - HISTSIZE='1000'
2018-05-02 12:15:44 hwc hwc[18431] DEBUG - HISTTIMEFORMAT='%F %T root '
2018-05-02 12:15:44 hwc hwc[18431] DEBUG - HOME='/root'
2018-05-02 12:15:44 hwc hwc[18431] DEBUG - HOSTNAME='hwc'
2018-05-02 12:15:44 hwc hwc[18431] DEBUG - LANG='en_US.UTF-8'
2018-05-02 12:15:44 hwc hwc[18431] DEBUG - LESSOPEN='||/usr/bin/lesspipe.sh %s'
2018-05-02 12:15:44 hwc hwc[18431] DEBUG - LOGNAME='root'
2018-05-02 12:15:44 hwc hwc[18431] DEBUG - LS_COLORS='rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=01;05;37;41:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arj=01;31:*.taz=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.dz=01;31:*.gz=01;31:*.lz=01;31:*.xz=01;31:*.bz2=01;31:*.tbz=01;31:*.tbz2=01;31:*.bz=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.rar=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.jpg=01;35:*.jpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.axv=01;35:*.anx=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=01;36:*.au=01;36:*.flac=01;36:*.mid=01;36:*.midi=01;36:*.mka=01;36:*.mp3=01;36:*.mpc=01;36:*.ogg=01;36:*.ra=01;36:*.wav=01;36:*.axa=01;36:*.oga=01;36:*.spx=01;36:*.xspf=01;36:'
2018-05-02 12:15:44 hwc hwc[18431] DEBUG - MAIL='/var/spool/mail/root'
2018-05-02 12:15:44 hwc hwc[18431] DEBUG - NOVA_ENDPOINT_TYPE='publicURL'
2018-05-02 12:15:44 hwc hwc[18431] DEBUG - OS_AUTH_URL='https://iam.cn-north-1.myhwclouds.com/v3'
2018-05-02 12:15:44 hwc hwc[18431] DEBUG - OS_AVAILABILITY='cn-north-1a'
2018-05-02 12:15:44 hwc hwc[18431] DEBUG - OS_ENDPOINT_TYPE='publicURL'
2018-05-02 12:15:44 hwc hwc[18431] DEBUG - OS_IDENTITY_API_VERSION='3'
2018-05-02 12:15:44 hwc hwc[18431] DEBUG - OS_IMAGE_API_VERSION='2'
2018-05-02 12:15:44 hwc hwc[18431] DEBUG - OS_PASSWORD='1980813c'
2018-05-02 12:15:44 hwc hwc[18431] DEBUG - OS_PROJECT_DOMAIN_NAME='mschyj'
2018-05-02 12:15:44 hwc hwc[18431] DEBUG - OS_PROJECT_NAME='cn-north-1'
2018-05-02 12:15:44 hwc hwc[18431] DEBUG - OS_TENANT_NAME='cn-north-1'
2018-05-02 12:15:44 hwc hwc[18431] DEBUG - OS_USERNAME='mschyj'
2018-05-02 12:15:44 hwc hwc[18431] DEBUG - OS_USER_DOMAIN_NAME='mschyj'
2018-05-02 12:15:44 hwc hwc[18431] DEBUG - OS_VOLUME_API_VERSION='2'
2018-05-02 12:15:44 hwc hwc[18431] DEBUG - PATH='/usr/lib64/qt-3.3/bin:/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin:/root/bin:/root/bin'
2018-05-02 12:15:44 hwc hwc[18431] DEBUG - PWD='/root/hwc'
2018-05-02 12:15:44 hwc hwc[18431] DEBUG - QTDIR='/usr/lib64/qt-3.3'
2018-05-02 12:15:44 hwc hwc[18431] DEBUG - QTINC='/usr/lib64/qt-3.3/include'
2018-05-02 12:15:44 hwc hwc[18431] DEBUG - QTLIB='/usr/lib64/qt-3.3/lib'
2018-05-02 12:15:44 hwc hwc[18431] DEBUG - SHELL='/bin/bash'
2018-05-02 12:15:44 hwc hwc[18431] DEBUG - SHLVL='3'
2018-05-02 12:15:44 hwc hwc[18431] DEBUG - SSH_CLIENT='58.213.108.56 47353 22'
2018-05-02 12:15:44 hwc hwc[18431] DEBUG - SSH_CONNECTION='58.213.108.56 47353 192.168.0.230 22'
2018-05-02 12:15:44 hwc hwc[18431] DEBUG - SSH_TTY='/dev/pts/1'
2018-05-02 12:15:44 hwc hwc[18431] DEBUG - TERM='screen'
2018-05-02 12:15:44 hwc hwc[18431] DEBUG - TMUX='/tmp/tmux-0/default,2894,0'
2018-05-02 12:15:44 hwc hwc[18431] DEBUG - TMUX_PANE='%5'
2018-05-02 12:15:44 hwc hwc[18431] DEBUG - USER='root'
2018-05-02 12:15:44 hwc hwc[18431] DEBUG - _='/usr/local/bin/elasticluster'
2018-05-02 12:15:44 hwc hwc[18431] DEBUG Using playbook file /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/site.yml.
2018-05-02 12:15:44 hwc hwc[18431] DEBUG Running Ansible command `ansible-playbook --private-key=/root/hwc/auth/id_rsa /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/site.yml --inventory=/root/hwc/cfg/storage/slurm.inventory --become --become-user=root -vv -e elasticluster_output_dir=/tmp/elasticluster.B6FQFP.d` ...
No config file found; using defaults
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/hosts.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/hostname.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/netgroup.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/ssh_auth.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/ssh_keysign_rhel7.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/cuda/tasks/_check_nvidia_dev.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/cuda/tasks/_reboot_and_wait.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/cuda/tasks/_check_nvidia_dev.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/ceph/tasks/mon.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/ceph/tasks/mgr.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/ceph/tasks/_create_key.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/ceph/tasks/osd.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/ceph/tasks/mds.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/ceph/tasks/fs.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/ceph/tasks/client.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/glusterfs-common/tasks/debian.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/glusterfs-common/tasks/rhel.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/glusterfs-common/tasks/debian.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/glusterfs-common/tasks/rhel.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/ssh_auth.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/ssh_keysign_rhel7.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/handlers/main.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/handlers/main.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypbind.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/cgroup.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypbind.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypbind.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/lmod/tasks/build.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/lmod/tasks/post-install.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypbind.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/lmod/tasks/build.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/lmod/tasks/post-install.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypbind.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/postgresql/tasks/install.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/postgresql/tasks/install_yum.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/postgresql/tasks/extensions.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/postgresql/tasks/extensions/contrib.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/postgresql/tasks/extensions/dev_headers.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/postgresql/tasks/extensions/postgis.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/postgresql/tasks/configure.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/postgresql/tasks/users.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/postgresql/tasks/databases.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/postgresql/tasks/users_privileges.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/postgresql/tasks/monit.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/lmod/tasks/build.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/lmod/tasks/post-install.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/r/tasks/rmpi.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypbind.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/lmod/tasks/build.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/lmod/tasks/post-install.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/r/tasks/rmpi.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/lmod/tasks/build.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/lmod/tasks/post-install.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypbind.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/handlers/main.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/handlers/main.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/ssh_auth.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/ssh_keysign_rhel7.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/jenkins/tasks/main.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/jenkins/handlers/main.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/kubernetes-common/tasks/debian.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/kubernetes-common/tasks/redhat.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/kubernetes-common/tasks/debian.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/kubernetes-common/tasks/redhat.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypbind.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/lmod/tasks/build.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/lmod/tasks/post-install.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/pbs+maui/tasks/master.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/handlers/main.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/pbs+maui/handlers/main.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/pbs+maui/tasks/maui.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/pbs+maui/handlers/main.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypbind.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/lmod/tasks/build.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/lmod/tasks/post-install.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/pbs+maui/tasks/clients.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/pbs+maui/handlers/main.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypbind.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-master/tasks/db.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-master/tasks/install-slurmdbd.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-master/tasks/install-slurmctld.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypbind.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/cgroup.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypbind.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/lmod/tasks/build.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/lmod/tasks/post-install.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/lmod/tasks/build.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/lmod/tasks/post-install.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/r/tasks/rmpi.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/smb-server/tasks/ctdb.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/lmod/tasks/build.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/lmod/tasks/post-install.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/r/tasks/rmpi.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/jupyter/tasks/bash.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/jupyter/tasks/python.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/jupyter/tasks/python.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/jupyter/tasks/pyspark.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/jupyter/tasks/pyspark.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/jupyter/tasks/irkernel.yml
statically included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/jupyter/tasks/matlab.yml

PLAYBOOK: site.yml *************************************************************
44 plays in /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/site.yml

PLAY [Prepare VM for running Ansible] ******************************************

TASK [Ensure Python is installed] **********************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/site.yml:6
skipping: [master001] => {"changed": false, "msg": "skipped, since /usr/bin/python exists", "skipped": true}
skipping: [worker001] => {"changed": false, "msg": "skipped, since /usr/bin/python exists", "skipped": true}
skipping: [worker002] => {"changed": false, "msg": "skipped, since /usr/bin/python exists", "skipped": true}
skipping: [worker003] => {"changed": false, "msg": "skipped, since /usr/bin/python exists", "skipped": true}

PLAY [Apply local customizations (before)] *************************************

TASK [setup] *******************************************************************
ok: [master001]
ok: [worker001]
ok: [worker002]
ok: [worker003]

PLAY [Common setup for all hosts] **********************************************

TASK [setup] *******************************************************************
ok: [master001]
ok: [worker001]
ok: [worker003]
ok: [worker002]

TASK [Ensure apt-daily is *not* running] ***************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/site.yml:20
skipping: [master001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [common : Provide workaround for YAML syntax error in lines containing colon+space] ***
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/main.yml:3
ok: [master001] => {"ansible_facts": {"__colon__": ":"}, "changed": false}
ok: [worker001] => {"ansible_facts": {"__colon__": ":"}, "changed": false}
ok: [worker002] => {"ansible_facts": {"__colon__": ":"}, "changed": false}
ok: [worker003] => {"ansible_facts": {"__colon__": ":"}, "changed": false}

TASK [common : Allow package updates] ******************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/main.yml:9
skipping: [master001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [common : Disallow package updates] ***************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/main.yml:14
ok: [master001] => {"ansible_facts": {"pkg_install_state": "present"}, "changed": false}
ok: [worker001] => {"ansible_facts": {"pkg_install_state": "present"}, "changed": false}
ok: [worker002] => {"ansible_facts": {"pkg_install_state": "present"}, "changed": false}
ok: [worker003] => {"ansible_facts": {"pkg_install_state": "present"}, "changed": false}

TASK [common : Set /etc/hosts from Ansible hostgroups] *************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/hosts.yml:2
changed: [master001] => {"changed": true, "checksum": "9ba1c1efec893c6921ce43a62cab8e78f20e36c9", "dest": "/etc/hosts", "gid": 0, "group": "root", "md5sum": "2aab7c3ca55fcc3beadbf77711dabfce", "mode": "0644", "owner": "root", "size": 475, "src": "/root/.ansible/tmp/ansible-tmp-1525234551.65-247448287018873/source", "state": "file", "uid": 0}
changed: [worker003] => {"changed": true, "checksum": "9ba1c1efec893c6921ce43a62cab8e78f20e36c9", "dest": "/etc/hosts", "gid": 0, "group": "root", "md5sum": "2aab7c3ca55fcc3beadbf77711dabfce", "mode": "0644", "owner": "root", "size": 475, "src": "/root/.ansible/tmp/ansible-tmp-1525234551.73-90126407155345/source", "state": "file", "uid": 0}
changed: [worker002] => {"changed": true, "checksum": "9ba1c1efec893c6921ce43a62cab8e78f20e36c9", "dest": "/etc/hosts", "gid": 0, "group": "root", "md5sum": "2aab7c3ca55fcc3beadbf77711dabfce", "mode": "0644", "owner": "root", "size": 475, "src": "/root/.ansible/tmp/ansible-tmp-1525234551.7-20260660803834/source", "state": "file", "uid": 0}
changed: [worker001] => {"changed": true, "checksum": "9ba1c1efec893c6921ce43a62cab8e78f20e36c9", "dest": "/etc/hosts", "gid": 0, "group": "root", "md5sum": "2aab7c3ca55fcc3beadbf77711dabfce", "mode": "0644", "owner": "root", "size": 475, "src": "/root/.ansible/tmp/ansible-tmp-1525234551.68-262430130296815/source", "state": "file", "uid": 0}

TASK [common : Patch `/etc/redhat-release`] ************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/hostname.yml:8
skipping: [master001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [common : Set host name to Ansible "inventory name"] **********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/hostname.yml:16
ok: [master001] => {"ansible_facts": {"ansible_domain": "", "ansible_fqdn": "master001", "ansible_hostname": "master001", "ansible_nodename": "master001"}, "changed": false, "name": "master001"}
changed: [worker003] => {"ansible_facts": {"ansible_domain": "", "ansible_fqdn": "worker003", "ansible_hostname": "worker003", "ansible_nodename": "worker003"}, "changed": true, "name": "worker003"}
ok: [worker001] => {"ansible_facts": {"ansible_domain": "", "ansible_fqdn": "worker001", "ansible_hostname": "worker001", "ansible_nodename": "worker001"}, "changed": false, "name": "worker001"}
ok: [worker002] => {"ansible_facts": {"ansible_domain": "", "ansible_fqdn": "worker002", "ansible_hostname": "worker002", "ansible_nodename": "worker002"}, "changed": false, "name": "worker002"}

TASK [common : Undo patch to `/etc/redhat-release`] ****************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/hostname.yml:22
skipping: [master001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [common : Check for cloud-init conf file] *********************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/hostname.yml:30
ok: [master001] => {"changed": false, "stat": {"atime": 1525233857.513, "checksum": "ce75755e6aabf7995730f54951ecb5da27974667", "ctime": 1523322248.64, "dev": 64514, "executable": false, "exists": true, "gid": 0, "gr_name": "root", "inode": 1572909, "isblk": false, "ischr": false, "isdir": false, "isfifo": false, "isgid": false, "islnk": false, "isreg": true, "issock": false, "isuid": false, "md5": "bba22929886688d6488b6b8b85e33d74", "mode": "0664", "mtime": 1523322248.64, "nlink": 1, "path": "/etc/cloud/cloud.cfg", "pw_name": "root", "readable": true, "rgrp": true, "roth": true, "rusr": true, "size": 1627, "uid": 0, "wgrp": true, "woth": false, "writeable": true, "wusr": true, "xgrp": false, "xoth": false, "xusr": false}}
ok: [worker001] => {"changed": false, "stat": {"atime": 1525233890.4129999, "checksum": "ce75755e6aabf7995730f54951ecb5da27974667", "ctime": 1523322248.64, "dev": 51714, "executable": false, "exists": true, "gid": 0, "gr_name": "root", "inode": 1572909, "isblk": false, "ischr": false, "isdir": false, "isfifo": false, "isgid": false, "islnk": false, "isreg": true, "issock": false, "isuid": false, "md5": "bba22929886688d6488b6b8b85e33d74", "mode": "0664", "mtime": 1523322248.64, "nlink": 1, "path": "/etc/cloud/cloud.cfg", "pw_name": "root", "readable": true, "rgrp": true, "roth": true, "rusr": true, "size": 1627, "uid": 0, "wgrp": true, "woth": false, "writeable": true, "wusr": true, "xgrp": false, "xoth": false, "xusr": false}}
ok: [worker002] => {"changed": false, "stat": {"atime": 1525233876.5740001, "checksum": "ce75755e6aabf7995730f54951ecb5da27974667", "ctime": 1523322248.64, "dev": 51714, "executable": false, "exists": true, "gid": 0, "gr_name": "root", "inode": 1572909, "isblk": false, "ischr": false, "isdir": false, "isfifo": false, "isgid": false, "islnk": false, "isreg": true, "issock": false, "isuid": false, "md5": "bba22929886688d6488b6b8b85e33d74", "mode": "0664", "mtime": 1523322248.64, "nlink": 1, "path": "/etc/cloud/cloud.cfg", "pw_name": "root", "readable": true, "rgrp": true, "roth": true, "rusr": true, "size": 1627, "uid": 0, "wgrp": true, "woth": false, "writeable": true, "wusr": true, "xgrp": false, "xoth": false, "xusr": false}}
ok: [worker003] => {"changed": false, "stat": {"atime": 1525234540.52, "checksum": "ce75755e6aabf7995730f54951ecb5da27974667", "ctime": 1523322248.64, "dev": 51714, "executable": false, "exists": true, "gid": 0, "gr_name": "root", "inode": 1572909, "isblk": false, "ischr": false, "isdir": false, "isfifo": false, "isgid": false, "islnk": false, "isreg": true, "issock": false, "isuid": false, "md5": "bba22929886688d6488b6b8b85e33d74", "mode": "0664", "mtime": 1523322248.64, "nlink": 1, "path": "/etc/cloud/cloud.cfg", "pw_name": "root", "readable": true, "rgrp": true, "roth": true, "rusr": true, "size": 1627, "uid": 0, "wgrp": true, "woth": false, "writeable": true, "wusr": true, "xgrp": false, "xoth": false, "xusr": false}}

TASK [common : Ensure changes to hostname are not overwritten by cloud-init] ***
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/hostname.yml:41
ok: [master001] => {"backup": "", "changed": false, "msg": ""}
ok: [worker002] => {"backup": "", "changed": false, "msg": ""}
ok: [worker003] => {"backup": "", "changed": false, "msg": ""}
ok: [worker001] => {"backup": "", "changed": false, "msg": ""}

TASK [common : Deploy `/etc/netgroup` file.] ***********************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/netgroup.yml:3
changed: [master001] => {"changed": true, "checksum": "723964604ff19242f7bad3315b866fc6c00bfdc9", "dest": "/etc/netgroup", "gid": 0, "group": "root", "md5sum": "30e6e3157b159f027125df42a65e960c", "mode": "0444", "owner": "root", "size": 294, "src": "/root/.ansible/tmp/ansible-tmp-1525234554.62-20847063936687/source", "state": "file", "uid": 0}
changed: [worker002] => {"changed": true, "checksum": "723964604ff19242f7bad3315b866fc6c00bfdc9", "dest": "/etc/netgroup", "gid": 0, "group": "root", "md5sum": "30e6e3157b159f027125df42a65e960c", "mode": "0444", "owner": "root", "size": 294, "src": "/root/.ansible/tmp/ansible-tmp-1525234554.67-214140702033828/source", "state": "file", "uid": 0}
changed: [worker003] => {"changed": true, "checksum": "723964604ff19242f7bad3315b866fc6c00bfdc9", "dest": "/etc/netgroup", "gid": 0, "group": "root", "md5sum": "30e6e3157b159f027125df42a65e960c", "mode": "0444", "owner": "root", "size": 294, "src": "/root/.ansible/tmp/ansible-tmp-1525234554.69-12598156621729/source", "state": "file", "uid": 0}
changed: [worker001] => {"changed": true, "checksum": "723964604ff19242f7bad3315b866fc6c00bfdc9", "dest": "/etc/netgroup", "gid": 0, "group": "root", "md5sum": "30e6e3157b159f027125df42a65e960c", "mode": "0444", "owner": "root", "size": 294, "src": "/root/.ansible/tmp/ansible-tmp-1525234554.63-192850387101451/source", "state": "file", "uid": 0}

TASK [common : Add `files` databases to `netgroup` service (I)] ****************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/netgroup.yml:21
ok: [master001] => {"changed": false, "msg": ""}
ok: [worker003] => {"changed": false, "msg": ""}
ok: [worker001] => {"changed": false, "msg": ""}
ok: [worker002] => {"changed": false, "msg": ""}

TASK [common : Add `files` databases to `netgroup` service (II)] ***************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/netgroup.yml:32
skipping: [master001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [common : Load distribution-dependent values (Debian/Ubuntu)] *************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/ssh_auth.yml:6
skipping: [master001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [common : Load distribution-dependent values (CentOS/RHEL)] ***************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/ssh_auth.yml:12
ok: [master001] => {"ansible_facts": {"ssh_keysign_path": "/usr/libexec/openssh/ssh-keysign"}, "changed": false}
ok: [worker001] => {"ansible_facts": {"ssh_keysign_path": "/usr/libexec/openssh/ssh-keysign"}, "changed": false}
ok: [worker002] => {"ansible_facts": {"ssh_keysign_path": "/usr/libexec/openssh/ssh-keysign"}, "changed": false}
ok: [worker003] => {"ansible_facts": {"ssh_keysign_path": "/usr/libexec/openssh/ssh-keysign"}, "changed": false}

TASK [common : Setup SSH known hosts file] *************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/ssh_auth.yml:18
changed: [master001] => {"changed": true, "checksum": "bbd808ac270500f801d0eab66646c48ce93a565e", "dest": "/etc/ssh/ssh_known_hosts", "gid": 0, "group": "root", "md5sum": "07995804538e1b88777d92d85108b10a", "mode": "0644", "owner": "root", "size": 4068, "src": "/root/.ansible/tmp/ansible-tmp-1525234557.77-174798533918906/source", "state": "file", "uid": 0}
changed: [worker001] => {"changed": true, "checksum": "bbd808ac270500f801d0eab66646c48ce93a565e", "dest": "/etc/ssh/ssh_known_hosts", "gid": 0, "group": "root", "md5sum": "07995804538e1b88777d92d85108b10a", "mode": "0644", "owner": "root", "size": 4068, "src": "/root/.ansible/tmp/ansible-tmp-1525234557.79-228997602971319/source", "state": "file", "uid": 0}
changed: [worker003] => {"changed": true, "checksum": "bbd808ac270500f801d0eab66646c48ce93a565e", "dest": "/etc/ssh/ssh_known_hosts", "gid": 0, "group": "root", "md5sum": "07995804538e1b88777d92d85108b10a", "mode": "0644", "owner": "root", "size": 4068, "src": "/root/.ansible/tmp/ansible-tmp-1525234557.85-176898370037385/source", "state": "file", "uid": 0}
changed: [worker002] => {"changed": true, "checksum": "bbd808ac270500f801d0eab66646c48ce93a565e", "dest": "/etc/ssh/ssh_known_hosts", "gid": 0, "group": "root", "md5sum": "07995804538e1b88777d92d85108b10a", "mode": "0644", "owner": "root", "size": 4068, "src": "/root/.ansible/tmp/ansible-tmp-1525234557.83-210935256871135/source", "state": "file", "uid": 0}

TASK [common : Setup /etc/ssh/shosts.equiv file] *******************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/ssh_auth.yml:27
changed: [master001] => {"changed": true, "checksum": "f73a74e016618d9098f8c0e0d8692bde29beed4a", "dest": "/etc/ssh/shosts.equiv", "gid": 0, "group": "root", "md5sum": "c2135ac55460d59c30b552af0812fbc3", "mode": "0644", "owner": "root", "size": 94, "src": "/root/.ansible/tmp/ansible-tmp-1525234559.43-251790076492670/source", "state": "file", "uid": 0}
changed: [worker001] => {"changed": true, "checksum": "f73a74e016618d9098f8c0e0d8692bde29beed4a", "dest": "/etc/ssh/shosts.equiv", "gid": 0, "group": "root", "md5sum": "c2135ac55460d59c30b552af0812fbc3", "mode": "0644", "owner": "root", "size": 94, "src": "/root/.ansible/tmp/ansible-tmp-1525234559.44-146158566900380/source", "state": "file", "uid": 0}
changed: [worker002] => {"changed": true, "checksum": "f73a74e016618d9098f8c0e0d8692bde29beed4a", "dest": "/etc/ssh/shosts.equiv", "gid": 0, "group": "root", "md5sum": "c2135ac55460d59c30b552af0812fbc3", "mode": "0644", "owner": "root", "size": 94, "src": "/root/.ansible/tmp/ansible-tmp-1525234559.47-205326026710376/source", "state": "file", "uid": 0}
changed: [worker003] => {"changed": true, "checksum": "f73a74e016618d9098f8c0e0d8692bde29beed4a", "dest": "/etc/ssh/shosts.equiv", "gid": 0, "group": "root", "md5sum": "c2135ac55460d59c30b552af0812fbc3", "mode": "0644", "owner": "root", "size": 94, "src": "/root/.ansible/tmp/ansible-tmp-1525234559.49-34809342893010/source", "state": "file", "uid": 0}

TASK [common : Setup /root/.shosts file] ***************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/ssh_auth.yml:35
changed: [master001] => {"changed": true, "checksum": "f73a74e016618d9098f8c0e0d8692bde29beed4a", "dest": "/root/.shosts", "gid": 0, "group": "root", "md5sum": "c2135ac55460d59c30b552af0812fbc3", "mode": "0644", "owner": "root", "size": 94, "src": "/root/.ansible/tmp/ansible-tmp-1525234561.23-202570391782622/source", "state": "file", "uid": 0}
changed: [worker001] => {"changed": true, "checksum": "f73a74e016618d9098f8c0e0d8692bde29beed4a", "dest": "/root/.shosts", "gid": 0, "group": "root", "md5sum": "c2135ac55460d59c30b552af0812fbc3", "mode": "0644", "owner": "root", "size": 94, "src": "/root/.ansible/tmp/ansible-tmp-1525234561.25-136412392510822/source", "state": "file", "uid": 0}
changed: [worker002] => {"changed": true, "checksum": "f73a74e016618d9098f8c0e0d8692bde29beed4a", "dest": "/root/.shosts", "gid": 0, "group": "root", "md5sum": "c2135ac55460d59c30b552af0812fbc3", "mode": "0644", "owner": "root", "size": 94, "src": "/root/.ansible/tmp/ansible-tmp-1525234561.29-277868081197805/source", "state": "file", "uid": 0}
changed: [worker003] => {"changed": true, "checksum": "f73a74e016618d9098f8c0e0d8692bde29beed4a", "dest": "/root/.shosts", "gid": 0, "group": "root", "md5sum": "c2135ac55460d59c30b552af0812fbc3", "mode": "0644", "owner": "root", "size": 94, "src": "/root/.ansible/tmp/ansible-tmp-1525234561.32-152913743626940/source", "state": "file", "uid": 0}

TASK [common : Setup SSH authentication (server configuration file)] ***********
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/ssh_auth.yml:43
ok: [master001] => (item={u'value': u'yes', u'key': u'HostbasedAuthentication'}) => {"backup": "", "changed": false, "item": {"key": "HostbasedAuthentication", "value": "yes"}, "msg": ""}
ok: [worker001] => (item={u'value': u'yes', u'key': u'HostbasedAuthentication'}) => {"backup": "", "changed": false, "item": {"key": "HostbasedAuthentication", "value": "yes"}, "msg": ""}
ok: [worker002] => (item={u'value': u'yes', u'key': u'HostbasedAuthentication'}) => {"backup": "", "changed": false, "item": {"key": "HostbasedAuthentication", "value": "yes"}, "msg": ""}
ok: [worker003] => (item={u'value': u'yes', u'key': u'HostbasedAuthentication'}) => {"backup": "", "changed": false, "item": {"key": "HostbasedAuthentication", "value": "yes"}, "msg": ""}
ok: [master001] => (item={u'value': u'no', u'key': u'IgnoreRhosts'}) => {"backup": "", "changed": false, "item": {"key": "IgnoreRhosts", "value": "no"}, "msg": ""}
ok: [worker001] => (item={u'value': u'no', u'key': u'IgnoreRhosts'}) => {"backup": "", "changed": false, "item": {"key": "IgnoreRhosts", "value": "no"}, "msg": ""}
ok: [worker003] => (item={u'value': u'no', u'key': u'IgnoreRhosts'}) => {"backup": "", "changed": false, "item": {"key": "IgnoreRhosts", "value": "no"}, "msg": ""}
ok: [worker002] => (item={u'value': u'no', u'key': u'IgnoreRhosts'}) => {"backup": "", "changed": false, "item": {"key": "IgnoreRhosts", "value": "no"}, "msg": ""}
ok: [master001] => (item={u'value': u'yes', u'key': u'PasswordAuthentication'}) => {"backup": "", "changed": false, "item": {"key": "PasswordAuthentication", "value": "yes"}, "msg": ""}
ok: [worker002] => (item={u'value': u'yes', u'key': u'PasswordAuthentication'}) => {"backup": "", "changed": false, "item": {"key": "PasswordAuthentication", "value": "yes"}, "msg": ""}
ok: [worker003] => (item={u'value': u'yes', u'key': u'PasswordAuthentication'}) => {"backup": "", "changed": false, "item": {"key": "PasswordAuthentication", "value": "yes"}, "msg": ""}
ok: [worker001] => (item={u'value': u'yes', u'key': u'PasswordAuthentication'}) => {"backup": "", "changed": false, "item": {"key": "PasswordAuthentication", "value": "yes"}, "msg": ""}

TASK [common : Setup SSH authentication (client configuration file)] ***********
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/ssh_auth.yml:55
ok: [master001] => (item={u'value': u'yes', u'key': u'HostbasedAuthentication'}) => {"backup": "", "changed": false, "item": {"key": "HostbasedAuthentication", "value": "yes"}, "msg": ""}
ok: [worker001] => (item={u'value': u'yes', u'key': u'HostbasedAuthentication'}) => {"backup": "", "changed": false, "item": {"key": "HostbasedAuthentication", "value": "yes"}, "msg": ""}
ok: [worker002] => (item={u'value': u'yes', u'key': u'HostbasedAuthentication'}) => {"backup": "", "changed": false, "item": {"key": "HostbasedAuthentication", "value": "yes"}, "msg": ""}
ok: [worker003] => (item={u'value': u'yes', u'key': u'HostbasedAuthentication'}) => {"backup": "", "changed": false, "item": {"key": "HostbasedAuthentication", "value": "yes"}, "msg": ""}
ok: [master001] => (item={u'value': u'yes', u'key': u'EnableSSHKeysign'}) => {"backup": "", "changed": false, "item": {"key": "EnableSSHKeysign", "value": "yes"}, "msg": ""}
ok: [worker001] => (item={u'value': u'yes', u'key': u'EnableSSHKeysign'}) => {"backup": "", "changed": false, "item": {"key": "EnableSSHKeysign", "value": "yes"}, "msg": ""}
ok: [worker002] => (item={u'value': u'yes', u'key': u'EnableSSHKeysign'}) => {"backup": "", "changed": false, "item": {"key": "EnableSSHKeysign", "value": "yes"}, "msg": ""}
ok: [worker003] => (item={u'value': u'yes', u'key': u'EnableSSHKeysign'}) => {"backup": "", "changed": false, "item": {"key": "EnableSSHKeysign", "value": "yes"}, "msg": ""}

TASK [common : Load info about the `ssh-keysign` executable] *******************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/ssh_keysign_rhel7.yml:9
skipping: [master001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [common : List all SSH host keys] *****************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/ssh_keysign_rhel7.yml:15
skipping: [master001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [common : Ensure SSH host keys can be read by `ssh-keysign`] **************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/common/tasks/ssh_keysign_rhel7.yml:23
skipping: [master001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [iptables : Load distribution-specific data] ******************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/iptables/tasks/main.yml:18
included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/iptables/tasks/init-RedHat.yml for master001, worker001, worker002, worker003

TASK [iptables : Load configuration and service names (RHEL-compatible)] *******
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/iptables/tasks/init-RedHat.yml:3
ok: [master001] => {"ansible_facts": {"configfile": {"etc/iptables/rules.v4": "/etc/sysconfig/iptables", "etc/iptables/rules.v6": "/etc/sysconfig/ip6tables"}, "reload": "reload iptables", "service": {"ip6tables": "ip6tables", "iptables": "iptables"}}, "changed": false}
ok: [worker001] => {"ansible_facts": {"configfile": {"etc/iptables/rules.v4": "/etc/sysconfig/iptables", "etc/iptables/rules.v6": "/etc/sysconfig/ip6tables"}, "reload": "reload iptables", "service": {"ip6tables": "ip6tables", "iptables": "iptables"}}, "changed": false}
ok: [worker002] => {"ansible_facts": {"configfile": {"etc/iptables/rules.v4": "/etc/sysconfig/iptables", "etc/iptables/rules.v6": "/etc/sysconfig/ip6tables"}, "reload": "reload iptables", "service": {"ip6tables": "ip6tables", "iptables": "iptables"}}, "changed": false}
ok: [worker003] => {"ansible_facts": {"configfile": {"etc/iptables/rules.v4": "/etc/sysconfig/iptables", "etc/iptables/rules.v6": "/etc/sysconfig/ip6tables"}, "reload": "reload iptables", "service": {"ip6tables": "ip6tables", "iptables": "iptables"}}, "changed": false}

TASK [iptables : Load configuration and service names (RHEL6-compatible)] ******
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/iptables/tasks/init-RedHat.yml:17
ok: [master001] => {"ansible_facts": {"packages": ["iptables", "iptables-ipv6"]}, "changed": false}
ok: [worker001] => {"ansible_facts": {"packages": ["iptables", "iptables-ipv6"]}, "changed": false}
ok: [worker002] => {"ansible_facts": {"packages": ["iptables", "iptables-ipv6"]}, "changed": false}
ok: [worker003] => {"ansible_facts": {"packages": ["iptables", "iptables-ipv6"]}, "changed": false}

TASK [iptables : Load configuration and service names (RHEL7-compatible)] ******
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/iptables/tasks/init-RedHat.yml:25
skipping: [master001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [iptables : Install iptables packages] ************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/iptables/tasks/main.yml:25
ok: [master001] => (item=[u'iptables', u'iptables-ipv6']) => {"changed": false, "item": ["iptables", "iptables-ipv6"], "msg": "", "rc": 0, "results": ["iptables-1.4.7-16.el6.x86_64 providing iptables is already installed", "iptables-ipv6-1.4.7-16.el6.x86_64 providing iptables-ipv6 is already installed"]}
ok: [worker002] => (item=[u'iptables', u'iptables-ipv6']) => {"changed": false, "item": ["iptables", "iptables-ipv6"], "msg": "", "rc": 0, "results": ["iptables-1.4.7-16.el6.x86_64 providing iptables is already installed", "iptables-ipv6-1.4.7-16.el6.x86_64 providing iptables-ipv6 is already installed"]}
ok: [worker001] => (item=[u'iptables', u'iptables-ipv6']) => {"changed": false, "item": ["iptables", "iptables-ipv6"], "msg": "", "rc": 0, "results": ["iptables-1.4.7-16.el6.x86_64 providing iptables is already installed", "iptables-ipv6-1.4.7-16.el6.x86_64 providing iptables-ipv6 is already installed"]}
ok: [worker003] => (item=[u'iptables', u'iptables-ipv6']) => {"changed": false, "item": ["iptables", "iptables-ipv6"], "msg": "", "rc": 0, "results": ["iptables-1.4.7-16.el6.x86_64 providing iptables is already installed", "iptables-ipv6-1.4.7-16.el6.x86_64 providing iptables-ipv6 is already installed"]}

TASK [iptables : Deploy netfilter rules] ***************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/iptables/tasks/main.yml:33
changed: [worker001] => (item=etc/iptables/rules.v4) => {"changed": true, "checksum": "9743c2254ff8f6d95cb648e02a0fe1215052b88b", "dest": "/etc/sysconfig/iptables", "gid": 0, "group": "root", "item": "etc/iptables/rules.v4", "md5sum": "f91f864fe22196d321484e2e203771de", "mode": "0444", "owner": "root", "size": 1150, "src": "/root/.ansible/tmp/ansible-tmp-1525234567.12-121628169849320/source", "state": "file", "uid": 0}
changed: [master001] => (item=etc/iptables/rules.v4) => {"changed": true, "checksum": "9743c2254ff8f6d95cb648e02a0fe1215052b88b", "dest": "/etc/sysconfig/iptables", "gid": 0, "group": "root", "item": "etc/iptables/rules.v4", "md5sum": "f91f864fe22196d321484e2e203771de", "mode": "0444", "owner": "root", "size": 1150, "src": "/root/.ansible/tmp/ansible-tmp-1525234567.1-101901777864454/source", "state": "file", "uid": 0}
changed: [worker003] => (item=etc/iptables/rules.v4) => {"changed": true, "checksum": "9743c2254ff8f6d95cb648e02a0fe1215052b88b", "dest": "/etc/sysconfig/iptables", "gid": 0, "group": "root", "item": "etc/iptables/rules.v4", "md5sum": "f91f864fe22196d321484e2e203771de", "mode": "0444", "owner": "root", "size": 1150, "src": "/root/.ansible/tmp/ansible-tmp-1525234567.18-43644302696792/source", "state": "file", "uid": 0}
changed: [worker002] => (item=etc/iptables/rules.v4) => {"changed": true, "checksum": "9743c2254ff8f6d95cb648e02a0fe1215052b88b", "dest": "/etc/sysconfig/iptables", "gid": 0, "group": "root", "item": "etc/iptables/rules.v4", "md5sum": "f91f864fe22196d321484e2e203771de", "mode": "0444", "owner": "root", "size": 1150, "src": "/root/.ansible/tmp/ansible-tmp-1525234567.16-253063759900264/source", "state": "file", "uid": 0}
NOTIFIED HANDLER reload iptables
ok: [master001] => (item=etc/iptables/rules.v6) => {"changed": false, "gid": 0, "group": "root", "item": "etc/iptables/rules.v6", "mode": "0444", "owner": "root", "path": "/etc/sysconfig/ip6tables", "size": 1180, "state": "file", "uid": 0}
NOTIFIED HANDLER reload iptables
NOTIFIED HANDLER reload iptables
ok: [worker002] => (item=etc/iptables/rules.v6) => {"changed": false, "gid": 0, "group": "root", "item": "etc/iptables/rules.v6", "mode": "0444", "owner": "root", "path": "/etc/sysconfig/ip6tables", "size": 1180, "state": "file", "uid": 0}
NOTIFIED HANDLER reload iptables
ok: [worker001] => (item=etc/iptables/rules.v6) => {"changed": false, "gid": 0, "group": "root", "item": "etc/iptables/rules.v6", "mode": "0444", "owner": "root", "path": "/etc/sysconfig/ip6tables", "size": 1180, "state": "file", "uid": 0}
ok: [worker003] => (item=etc/iptables/rules.v6) => {"changed": false, "gid": 0, "group": "root", "item": "etc/iptables/rules.v6", "mode": "0444", "owner": "root", "path": "/etc/sysconfig/ip6tables", "size": 1180, "state": "file", "uid": 0}

TASK [iptables : Ensure netfilter rules are loaded at boot] ********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/iptables/tasks/main.yml:50
ok: [master001] => (item=iptables) => {"changed": false, "enabled": true, "item": "iptables", "name": "iptables", "state": "started"}
ok: [worker003] => (item=iptables) => {"changed": false, "enabled": true, "item": "iptables", "name": "iptables", "state": "started"}
ok: [worker001] => (item=iptables) => {"changed": false, "enabled": true, "item": "iptables", "name": "iptables", "state": "started"}
ok: [worker002] => (item=iptables) => {"changed": false, "enabled": true, "item": "iptables", "name": "iptables", "state": "started"}
ok: [master001] => (item=ip6tables) => {"changed": false, "enabled": true, "item": "ip6tables", "name": "ip6tables", "state": "started"}
ok: [worker001] => (item=ip6tables) => {"changed": false, "enabled": true, "item": "ip6tables", "name": "ip6tables", "state": "started"}
ok: [worker002] => (item=ip6tables) => {"changed": false, "enabled": true, "item": "ip6tables", "name": "ip6tables", "state": "started"}
ok: [worker003] => (item=ip6tables) => {"changed": false, "enabled": true, "item": "ip6tables", "name": "ip6tables", "state": "started"}

RUNNING HANDLER [iptables : reload iptables] ***********************************
changed: [master001] => (item=iptables) => {"changed": true, "item": "iptables", "name": "iptables", "state": "started"}
changed: [worker003] => (item=iptables) => {"changed": true, "item": "iptables", "name": "iptables", "state": "started"}
changed: [worker001] => (item=iptables) => {"changed": true, "item": "iptables", "name": "iptables", "state": "started"}
changed: [worker002] => (item=iptables) => {"changed": true, "item": "iptables", "name": "iptables", "state": "started"}
changed: [master001] => (item=ip6tables) => {"changed": true, "item": "ip6tables", "name": "ip6tables", "state": "started"}
changed: [worker003] => (item=ip6tables) => {"changed": true, "item": "ip6tables", "name": "ip6tables", "state": "started"}
changed: [worker001] => (item=ip6tables) => {"changed": true, "item": "ip6tables", "name": "ip6tables", "state": "started"}
changed: [worker002] => (item=ip6tables) => {"changed": true, "item": "ip6tables", "name": "ip6tables", "state": "started"}

TASK [ntpd : Load distribution-specific parameters] ****************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/ntpd/tasks/main.yml:3
included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/ntpd/tasks/init-RedHat.yml for master001, worker001, worker002, worker003

TASK [ntpd : Set NTPd common playbook params (RHEL/CentOS)] ********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/ntpd/tasks/init-RedHat.yml:3
ok: [master001] => {"ansible_facts": {"ntpd_package": "ntp", "ntpd_service": "ntpd"}, "changed": false}
ok: [worker001] => {"ansible_facts": {"ntpd_package": "ntp", "ntpd_service": "ntpd"}, "changed": false}
ok: [worker002] => {"ansible_facts": {"ntpd_package": "ntp", "ntpd_service": "ntpd"}, "changed": false}
ok: [worker003] => {"ansible_facts": {"ntpd_package": "ntp", "ntpd_service": "ntpd"}, "changed": false}

TASK [ntpd : Deploy NTP configuration file] ************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/ntpd/tasks/main.yml:7
NOTIFIED HANDLER restart ntpd
changed: [master001] => {"changed": true, "checksum": "7688a1e2600900903c22729ea1a156c001d588b5", "dest": "/etc/ntp.conf", "gid": 0, "group": "root", "md5sum": "fa6de04fe0be5a5d624a3b4dd79ccd9f", "mode": "0444", "owner": "root", "size": 2053, "src": "/root/.ansible/tmp/ansible-tmp-1525234570.84-130557936240495/source", "state": "file", "uid": 0}
NOTIFIED HANDLER restart ntpd
changed: [worker001] => {"changed": true, "checksum": "060bf7fb1c255102cd20f9afaae34c8ff1a5a761", "dest": "/etc/ntp.conf", "gid": 0, "group": "root", "md5sum": "6b468fd939b5ebfdba572219533f100c", "mode": "0444", "owner": "root", "size": 2053, "src": "/root/.ansible/tmp/ansible-tmp-1525234570.85-58470379098448/source", "state": "file", "uid": 0}
NOTIFIED HANDLER restart ntpd
changed: [worker003] => {"changed": true, "checksum": "c4ac024c490a63a986a0ec224529141a6e8938ed", "dest": "/etc/ntp.conf", "gid": 0, "group": "root", "md5sum": "11047010f0a3138cba3064a679e6ebaa", "mode": "0444", "owner": "root", "size": 2053, "src": "/root/.ansible/tmp/ansible-tmp-1525234570.9-59305873136696/source", "state": "file", "uid": 0}
NOTIFIED HANDLER restart ntpd
changed: [worker002] => {"changed": true, "checksum": "a3c4a85cc02a1c767e6698afa0a0593f393aa9ca", "dest": "/etc/ntp.conf", "gid": 0, "group": "root", "md5sum": "e1225fd18c6048f177b47d609124288b", "mode": "0444", "owner": "root", "size": 2053, "src": "/root/.ansible/tmp/ansible-tmp-1525234570.88-206524826172721/source", "state": "file", "uid": 0}

TASK [ntpd : Install NTP packages] *********************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/ntpd/tasks/main.yml:19
ok: [master001] => {"changed": false, "msg": "", "rc": 0, "results": ["ntp-4.2.6p5-12.el6.centos.2.x86_64 providing ntp is already installed"]}
ok: [worker001] => {"changed": false, "msg": "", "rc": 0, "results": ["ntp-4.2.6p5-12.el6.centos.2.x86_64 providing ntp is already installed"]}
ok: [worker002] => {"changed": false, "msg": "", "rc": 0, "results": ["ntp-4.2.6p5-12.el6.centos.2.x86_64 providing ntp is already installed"]}
ok: [worker003] => {"changed": false, "msg": "", "rc": 0, "results": ["ntp-4.2.6p5-12.el6.centos.2.x86_64 providing ntp is already installed"]}

TASK [ntpd : Enable NTP service at boot] ***************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/ntpd/tasks/main.yml:42
ok: [master001] => {"changed": false, "enabled": true, "name": "ntpd", "state": "started"}
ok: [worker001] => {"changed": false, "enabled": true, "name": "ntpd", "state": "started"}
ok: [worker003] => {"changed": false, "enabled": true, "name": "ntpd", "state": "started"}
ok: [worker002] => {"changed": false, "enabled": true, "name": "ntpd", "state": "started"}

TASK [pdsh : Install pdsh packages (Debian-family)] ****************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/pdsh/tasks/main.yml:3
skipping: [master001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [pdsh : Install pdsh packages (RHEL-family)] ******************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/pdsh/tasks/main.yml:11
ok: [master001] => (item=[u'pdsh', u'pdsh-rcmd-ssh', u'pdsh-mod-genders']) => {"changed": false, "item": ["pdsh", "pdsh-rcmd-ssh", "pdsh-mod-genders"], "msg": "", "rc": 0, "results": ["pdsh-2.26-4.el6.x86_64 providing pdsh is already installed", "pdsh-rcmd-ssh-2.26-4.el6.x86_64 providing pdsh-rcmd-ssh is already installed", "pdsh-mod-genders-2.26-4.el6.x86_64 providing pdsh-mod-genders is already installed"]}
ok: [worker002] => (item=[u'pdsh', u'pdsh-rcmd-ssh', u'pdsh-mod-genders']) => {"changed": false, "item": ["pdsh", "pdsh-rcmd-ssh", "pdsh-mod-genders"], "msg": "", "rc": 0, "results": ["pdsh-2.26-4.el6.x86_64 providing pdsh is already installed", "pdsh-rcmd-ssh-2.26-4.el6.x86_64 providing pdsh-rcmd-ssh is already installed", "pdsh-mod-genders-2.26-4.el6.x86_64 providing pdsh-mod-genders is already installed"]}
ok: [worker003] => (item=[u'pdsh', u'pdsh-rcmd-ssh', u'pdsh-mod-genders']) => {"changed": false, "item": ["pdsh", "pdsh-rcmd-ssh", "pdsh-mod-genders"], "msg": "", "rc": 0, "results": ["pdsh-2.26-4.el6.x86_64 providing pdsh is already installed", "pdsh-rcmd-ssh-2.26-4.el6.x86_64 providing pdsh-rcmd-ssh is already installed", "pdsh-mod-genders-2.26-4.el6.x86_64 providing pdsh-mod-genders is already installed"]}
ok: [worker001] => (item=[u'pdsh', u'pdsh-rcmd-ssh', u'pdsh-mod-genders']) => {"changed": false, "item": ["pdsh", "pdsh-rcmd-ssh", "pdsh-mod-genders"], "msg": "", "rc": 0, "results": ["pdsh-2.26-4.el6.x86_64 providing pdsh is already installed", "pdsh-rcmd-ssh-2.26-4.el6.x86_64 providing pdsh-rcmd-ssh is already installed", "pdsh-mod-genders-2.26-4.el6.x86_64 providing pdsh-mod-genders is already installed"]}

TASK [pdsh : Create genders file for PDSH] *************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/pdsh/tasks/main.yml:21
changed: [master001] => {"changed": true, "checksum": "372737711392c904bf6d15db75a5c7c8b5589629", "dest": "/etc/genders", "gid": 0, "group": "root", "md5sum": "211916aca41f67cc7974f33db74a494a", "mode": "0444", "owner": "root", "size": 225, "src": "/root/.ansible/tmp/ansible-tmp-1525234574.69-253741826426215/source", "state": "file", "uid": 0}
changed: [worker001] => {"changed": true, "checksum": "372737711392c904bf6d15db75a5c7c8b5589629", "dest": "/etc/genders", "gid": 0, "group": "root", "md5sum": "211916aca41f67cc7974f33db74a494a", "mode": "0444", "owner": "root", "size": 225, "src": "/root/.ansible/tmp/ansible-tmp-1525234574.7-211571698555121/source", "state": "file", "uid": 0}
changed: [worker003] => {"changed": true, "checksum": "372737711392c904bf6d15db75a5c7c8b5589629", "dest": "/etc/genders", "gid": 0, "group": "root", "md5sum": "211916aca41f67cc7974f33db74a494a", "mode": "0444", "owner": "root", "size": 225, "src": "/root/.ansible/tmp/ansible-tmp-1525234574.76-47611742838175/source", "state": "file", "uid": 0}
changed: [worker002] => {"changed": true, "checksum": "372737711392c904bf6d15db75a5c7c8b5589629", "dest": "/etc/genders", "gid": 0, "group": "root", "md5sum": "211916aca41f67cc7974f33db74a494a", "mode": "0444", "owner": "root", "size": 225, "src": "/root/.ansible/tmp/ansible-tmp-1525234574.73-50505153527663/source", "state": "file", "uid": 0}

TASK [pdsh : Make SSH the default exec method for PDSH] ************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/pdsh/tasks/main.yml:29
ok: [master001] => {"changed": false, "checksum": "5929e07c9e9a681ab81ae9da0e1a4c17630509ab", "dest": "/etc/profile.d/pdsh.sh", "gid": 0, "group": "root", "mode": "0444", "owner": "root", "path": "/etc/profile.d/pdsh.sh", "size": 278, "state": "file", "uid": 0}
ok: [worker001] => {"changed": false, "checksum": "5929e07c9e9a681ab81ae9da0e1a4c17630509ab", "dest": "/etc/profile.d/pdsh.sh", "gid": 0, "group": "root", "mode": "0444", "owner": "root", "path": "/etc/profile.d/pdsh.sh", "size": 278, "state": "file", "uid": 0}
ok: [worker003] => {"changed": false, "checksum": "5929e07c9e9a681ab81ae9da0e1a4c17630509ab", "dest": "/etc/profile.d/pdsh.sh", "gid": 0, "group": "root", "mode": "0444", "owner": "root", "path": "/etc/profile.d/pdsh.sh", "size": 278, "state": "file", "uid": 0}
ok: [worker002] => {"changed": false, "checksum": "5929e07c9e9a681ab81ae9da0e1a4c17630509ab", "dest": "/etc/profile.d/pdsh.sh", "gid": 0, "group": "root", "mode": "0444", "owner": "root", "path": "/etc/profile.d/pdsh.sh", "size": 278, "state": "file", "uid": 0}

RUNNING HANDLER [ntpd : restart ntpd] ******************************************
changed: [master001] => {"changed": true, "name": "ntpd", "state": "started"}
changed: [worker001] => {"changed": true, "name": "ntpd", "state": "started"}
changed: [worker002] => {"changed": true, "name": "ntpd", "state": "started"}
changed: [worker003] => {"changed": true, "name": "ntpd", "state": "started"}

PLAY [Slurm worker nodes Playbook] *********************************************

TASK [setup] *******************************************************************
ok: [worker001]
ok: [master001]
ok: [worker003]
ok: [worker002]

TASK [nis : Load distribution-specific parameters] *****************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/main.yml:3
included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/init-RedHat.yml for master001, worker001, worker002, worker003

TASK [nis : Set NIS common playbook params (CentOS/RHEL)] **********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/init-RedHat.yml:3
ok: [master001] => {"ansible_facts": {"nis_client_packages": ["ypbind"], "nis_client_services": ["rpcbind", "ypbind"], "nis_common_packages": ["yp-tools"], "nis_master_packages": ["ypserv"], "nis_master_services": ["rpcbind", "yppasswdd", "ypserv"], "nis_securenets_path": "/var/yp/securenets"}, "changed": false}
ok: [worker001] => {"ansible_facts": {"nis_client_packages": ["ypbind"], "nis_client_services": ["rpcbind", "ypbind"], "nis_common_packages": ["yp-tools"], "nis_master_packages": ["ypserv"], "nis_master_services": ["rpcbind", "yppasswdd", "ypserv"], "nis_securenets_path": "/var/yp/securenets"}, "changed": false}
ok: [worker002] => {"ansible_facts": {"nis_client_packages": ["ypbind"], "nis_client_services": ["rpcbind", "ypbind"], "nis_common_packages": ["yp-tools"], "nis_master_packages": ["ypserv"], "nis_master_services": ["rpcbind", "yppasswdd", "ypserv"], "nis_securenets_path": "/var/yp/securenets"}, "changed": false}
ok: [worker003] => {"ansible_facts": {"nis_client_packages": ["ypbind"], "nis_client_services": ["rpcbind", "ypbind"], "nis_common_packages": ["yp-tools"], "nis_master_packages": ["ypserv"], "nis_master_services": ["rpcbind", "yppasswdd", "ypserv"], "nis_securenets_path": "/var/yp/securenets"}, "changed": false}

TASK [nis : Pre-load debconf answer to questions (Debian/Ubuntu)] **************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/main.yml:7
skipping: [master001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [nis : Deploy additional NIS configuration (Debian/Ubuntu)] ***************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/main.yml:18
skipping: [master001] => (item=etc/default/nis)  => {"changed": false, "item": "etc/default/nis", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [master001] => (item=etc/defaultdomain)  => {"changed": false, "item": "etc/defaultdomain", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => (item=etc/default/nis)  => {"changed": false, "item": "etc/default/nis", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => (item=etc/defaultdomain)  => {"changed": false, "item": "etc/defaultdomain", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item=etc/default/nis)  => {"changed": false, "item": "etc/default/nis", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item=etc/defaultdomain)  => {"changed": false, "item": "etc/defaultdomain", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => (item=etc/default/nis)  => {"changed": false, "item": "etc/default/nis", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => (item=etc/defaultdomain)  => {"changed": false, "item": "etc/defaultdomain", "skip_reason": "Conditional check failed", "skipped": true}

TASK [nis : Set NIS domain (CentOS/RHEL)] **************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/main.yml:29
changed: [master001] => {"backup": "", "changed": true, "msg": "line added"}
changed: [worker001] => {"backup": "", "changed": true, "msg": "line added"}
changed: [worker002] => {"backup": "", "changed": true, "msg": "line added"}
changed: [worker003] => {"backup": "", "changed": true, "msg": "line added"}

TASK [nis : Install NIS common packages] ***************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/main.yml:39
ok: [master001] => (item=[u'yp-tools']) => {"changed": false, "item": ["yp-tools"], "msg": "", "rc": 0, "results": ["yp-tools-2.9-12.el6.x86_64 providing yp-tools is already installed"]}
ok: [worker001] => (item=[u'yp-tools']) => {"changed": false, "item": ["yp-tools"], "msg": "", "rc": 0, "results": ["yp-tools-2.9-12.el6.x86_64 providing yp-tools is already installed"]}
ok: [worker002] => (item=[u'yp-tools']) => {"changed": false, "item": ["yp-tools"], "msg": "", "rc": 0, "results": ["yp-tools-2.9-12.el6.x86_64 providing yp-tools is already installed"]}
ok: [worker003] => (item=[u'yp-tools']) => {"changed": false, "item": ["yp-tools"], "msg": "", "rc": 0, "results": ["yp-tools-2.9-12.el6.x86_64 providing yp-tools is already installed"]}

TASK [nis : Deploy `ypserv` configuration files] *******************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml:3
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
NOTIFIED HANDLER restart NIS master services
changed: [master001] => {"changed": true, "checksum": "69e25288b7ba3c6f1f9cbf1b1040b46fa9025a2c", "dest": "/var/yp/securenets", "gid": 0, "group": "root", "md5sum": "b4d75249fdc52250f367680131b34639", "mode": "0400", "owner": "root", "size": 733, "src": "/root/.ansible/tmp/ansible-tmp-1525234579.69-228901897485096/source", "state": "file", "uid": 0}

TASK [nis : Deploy `yppasswdd` configuration file (CentOS/RHEL)] ***************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml:12
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
ok: [master001] => {"changed": false, "checksum": "18392b4e3ecc4f9e7832c274663b5f6cee00d3f1", "dest": "/etc/sysconfig/yppasswdd", "gid": 0, "group": "root", "mode": "0444", "owner": "root", "path": "/etc/sysconfig/yppasswdd", "size": 681, "state": "file", "uid": 0}

TASK [nis : Install NIS master server packages] ********************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml:22
skipping: [worker001] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}
ok: [master001] => (item=[u'ypserv']) => {"changed": false, "item": ["ypserv"], "msg": "", "rc": 0, "results": ["ypserv-2.19-31.el6.x86_64 providing ypserv is already installed"]}

RUNNING HANDLER [nis : restart NIS master services] ****************************
changed: [master001] => (item=rpcbind) => {"changed": true, "item": "rpcbind", "name": "rpcbind", "state": "started"}
changed: [master001] => (item=yppasswdd) => {"changed": true, "item": "yppasswdd", "name": "yppasswdd", "state": "started"}
changed: [master001] => (item=ypserv) => {"changed": true, "item": "ypserv", "name": "ypserv", "state": "started"}

TASK [nis : Ensure `ypserv` starts at boot] ************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml:33
skipping: [worker001] => (item=rpcbind)  => {"changed": false, "item": "rpcbind", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item=rpcbind)  => {"changed": false, "item": "rpcbind", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => (item=yppasswdd)  => {"changed": false, "item": "yppasswdd", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => (item=rpcbind)  => {"changed": false, "item": "rpcbind", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item=yppasswdd)  => {"changed": false, "item": "yppasswdd", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => (item=ypserv)  => {"changed": false, "item": "ypserv", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => (item=yppasswdd)  => {"changed": false, "item": "yppasswdd", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item=ypserv)  => {"changed": false, "item": "ypserv", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => (item=ypserv)  => {"changed": false, "item": "ypserv", "skip_reason": "Conditional check failed", "skipped": true}
ok: [master001] => (item=rpcbind) => {"changed": false, "enabled": true, "item": "rpcbind", "name": "rpcbind", "state": "started"}
ok: [master001] => (item=yppasswdd) => {"changed": false, "enabled": true, "item": "yppasswdd", "name": "yppasswdd", "state": "started"}
ok: [master001] => (item=ypserv) => {"changed": false, "enabled": true, "item": "ypserv", "name": "ypserv", "state": "started"}

TASK [nis : Update NIS/YP databases (NIS master server)] ***********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml:41
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
changed: [master001] => {"changed": true, "cmd": ["make"], "delta": "0:00:00.082612", "end": "2018-05-02 12:16:25.613454", "rc": 0, "start": "2018-05-02 12:16:25.530842", "stderr": "", "stdout": "gmake[1]: Entering directory `/var/yp/elasticluster'\nUpdating hosts.byname...\nUpdating hosts.byaddr...\nUpdating netid.byname...\ngmake[1]: Leaving directory `/var/yp/elasticluster'", "stdout_lines": ["gmake[1]: Entering directory `/var/yp/elasticluster'", "Updating hosts.byname...", "Updating hosts.byaddr...", "Updating netid.byname...", "gmake[1]: Leaving directory `/var/yp/elasticluster'"], "warnings": []}

TASK [nis : Deploy `ypbind` configuration files] *******************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypbind.yml:3
skipping: [master001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
ok: [worker001] => {"changed": false, "gid": 0, "group": "root", "mode": "0400", "owner": "root", "path": "/etc/yp.conf", "size": 723, "state": "file", "uid": 0}
ok: [worker002] => {"changed": false, "gid": 0, "group": "root", "mode": "0400", "owner": "root", "path": "/etc/yp.conf", "size": 723, "state": "file", "uid": 0}
NOTIFIED HANDLER restart ypbind
changed: [worker003] => {"changed": true, "checksum": "1d96dd5b0ffb492e179507891ef4701a7afe84b6", "dest": "/etc/yp.conf", "gid": 0, "group": "root", "md5sum": "daa2803ee93589645ef31b0eef25ee39", "mode": "0400", "owner": "root", "size": 723, "src": "/root/.ansible/tmp/ansible-tmp-1525234584.04-172699619691646/source", "state": "file", "uid": 0}

TASK [nis : Ensure NIS/YP is used as a name service] ***************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypbind.yml:12
skipping: [master001] => (item=passwd)  => {"changed": false, "item": "passwd", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [master001] => (item=group)  => {"changed": false, "item": "group", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [master001] => (item=shadow)  => {"changed": false, "item": "shadow", "skip_reason": "Conditional check failed", "skipped": true}
ok: [worker001] => (item=passwd) => {"backup": "", "changed": false, "item": "passwd", "msg": ""}
ok: [worker002] => (item=passwd) => {"backup": "", "changed": false, "item": "passwd", "msg": ""}
changed: [worker003] => (item=passwd) => {"backup": "", "changed": true, "item": "passwd", "msg": "line replaced"}
ok: [worker001] => (item=group) => {"backup": "", "changed": false, "item": "group", "msg": ""}
ok: [worker002] => (item=group) => {"backup": "", "changed": false, "item": "group", "msg": ""}
changed: [worker003] => (item=group) => {"backup": "", "changed": true, "item": "group", "msg": "line replaced"}
ok: [worker001] => (item=shadow) => {"backup": "", "changed": false, "item": "shadow", "msg": ""}
ok: [worker002] => (item=shadow) => {"backup": "", "changed": false, "item": "shadow", "msg": ""}
changed: [worker003] => (item=shadow) => {"backup": "", "changed": true, "item": "shadow", "msg": "line replaced"}

TASK [nis : Replace `compat` in `/etc/nsswitch.conf` with `files nis`] *********
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypbind.yml:26
skipping: [master001] => (item=passwd)  => {"changed": false, "item": "passwd", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [master001] => (item=group)  => {"changed": false, "item": "group", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [master001] => (item=shadow)  => {"changed": false, "item": "shadow", "skip_reason": "Conditional check failed", "skipped": true}
ok: [worker001] => (item=passwd) => {"changed": false, "item": "passwd", "msg": ""}
ok: [worker002] => (item=passwd) => {"changed": false, "item": "passwd", "msg": ""}
ok: [worker003] => (item=passwd) => {"changed": false, "item": "passwd", "msg": ""}
ok: [worker001] => (item=group) => {"changed": false, "item": "group", "msg": ""}
ok: [worker003] => (item=group) => {"changed": false, "item": "group", "msg": ""}
ok: [worker002] => (item=group) => {"changed": false, "item": "group", "msg": ""}
ok: [worker001] => (item=shadow) => {"changed": false, "item": "shadow", "msg": ""}
ok: [worker003] => (item=shadow) => {"changed": false, "item": "shadow", "msg": ""}
ok: [worker002] => (item=shadow) => {"changed": false, "item": "shadow", "msg": ""}

TASK [nis : Ensure `ypbind` starts at boot] ************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypbind.yml:37
skipping: [master001] => (item=rpcbind)  => {"changed": false, "item": "rpcbind", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [master001] => (item=ypbind)  => {"changed": false, "item": "ypbind", "skip_reason": "Conditional check failed", "skipped": true}
ok: [worker001] => (item=rpcbind) => {"changed": false, "enabled": true, "item": "rpcbind", "name": "rpcbind", "state": "started"}
ok: [worker002] => (item=rpcbind) => {"changed": false, "enabled": true, "item": "rpcbind", "name": "rpcbind", "state": "started"}
ok: [worker003] => (item=rpcbind) => {"changed": false, "enabled": true, "item": "rpcbind", "name": "rpcbind", "state": "started"}
ok: [worker001] => (item=ypbind) => {"changed": false, "enabled": true, "item": "ypbind", "name": "ypbind", "state": "started"}
ok: [worker002] => (item=ypbind) => {"changed": false, "enabled": true, "item": "ypbind", "name": "ypbind", "state": "started"}
changed: [worker003] => (item=ypbind) => {"changed": true, "enabled": true, "item": "ypbind", "name": "ypbind", "state": "started"}

RUNNING HANDLER [nis : restart ypbind] *****************************************
changed: [worker003] => (item=rpcbind) => {"changed": true, "item": "rpcbind", "name": "rpcbind", "state": "started"}
changed: [worker003] => (item=ypbind) => {"changed": true, "item": "ypbind", "name": "ypbind", "state": "started"}

TASK [nfs-client : install NFS client software (Debian/Ubuntu)] ****************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-client/tasks/main.yml:3
skipping: [master001] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}

TASK [nfs-client : install NFS client software (RHEL-compatible)] **************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-client/tasks/main.yml:15
ok: [master001] => (item=[u'nfs-utils', u'nfs4-acl-tools']) => {"changed": false, "item": ["nfs-utils", "nfs4-acl-tools"], "msg": "", "rc": 0, "results": ["nfs-utils-1:1.2.3-75.el6_9.x86_64 providing nfs-utils is already installed", "nfs4-acl-tools-0.3.3-8.el6.x86_64 providing nfs4-acl-tools is already installed"]}
ok: [worker002] => (item=[u'nfs-utils', u'nfs4-acl-tools']) => {"changed": false, "item": ["nfs-utils", "nfs4-acl-tools"], "msg": "", "rc": 0, "results": ["nfs-utils-1:1.2.3-75.el6_9.x86_64 providing nfs-utils is already installed", "nfs4-acl-tools-0.3.3-8.el6.x86_64 providing nfs4-acl-tools is already installed"]}
ok: [worker003] => (item=[u'nfs-utils', u'nfs4-acl-tools']) => {"changed": false, "item": ["nfs-utils", "nfs4-acl-tools"], "msg": "", "rc": 0, "results": ["nfs-utils-1:1.2.3-75.el6_9.x86_64 providing nfs-utils is already installed", "nfs4-acl-tools-0.3.3-8.el6.x86_64 providing nfs4-acl-tools is already installed"]}
ok: [worker001] => (item=[u'nfs-utils', u'nfs4-acl-tools']) => {"changed": false, "item": ["nfs-utils", "nfs4-acl-tools"], "msg": "", "rc": 0, "results": ["nfs-utils-1:1.2.3-75.el6_9.x86_64 providing nfs-utils is already installed", "nfs4-acl-tools-0.3.3-8.el6.x86_64 providing nfs4-acl-tools is already installed"]}

TASK [nfs-client : Ensure `rpcbind` is running (Debian)] ***********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-client/tasks/main.yml:28
skipping: [master001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [nfs-client : Ensure `rpcbind` is running (RHEL-compatible)] **************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-client/tasks/main.yml:35
ok: [master001] => {"changed": false, "enabled": true, "name": "rpcbind", "state": "started"}
ok: [worker001] => {"changed": false, "enabled": true, "name": "rpcbind", "state": "started"}
ok: [worker002] => {"changed": false, "enabled": true, "name": "rpcbind", "state": "started"}
ok: [worker003] => {"changed": false, "enabled": true, "name": "rpcbind", "state": "started"}

TASK [nfs-client : Ensure `portmap` is running (Ubuntu prior to 14.04)] ********
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-client/tasks/main.yml:42
skipping: [master001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [nfs-client : Ensure `rpcbind` is running (Ubuntu 14.04 or newer)] ********
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-client/tasks/main.yml:49
skipping: [master001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [nfs-client : Mount NFS filesystems] **************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-client/tasks/main.yml:57
included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-client/tasks/nfsmount.yml for master001, worker001, worker002, worker003

TASK [nfs-client : ensure /sfs directory exists] *******************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-client/tasks/nfsmount.yml:3
ok: [master001] => {"changed": false, "gid": 99999, "group": "99999", "mode": "0777", "owner": "root", "path": "/sfs", "size": 4096, "state": "directory", "uid": 0}
ok: [worker001] => {"changed": false, "gid": 99999, "group": "99999", "mode": "0777", "owner": "root", "path": "/sfs", "size": 4096, "state": "directory", "uid": 0}
ok: [worker002] => {"changed": false, "gid": 99999, "group": "99999", "mode": "0777", "owner": "root", "path": "/sfs", "size": 4096, "state": "directory", "uid": 0}
changed: [worker003] => {"changed": true, "gid": 0, "group": "root", "mode": "0755", "owner": "root", "path": "/sfs", "size": 4096, "state": "directory", "uid": 0}

TASK [nfs-client : add to /etc/fstab] ******************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-client/tasks/nfsmount.yml:8
ok: [master001] => {"changed": false, "dump": "0", "fstab": "/etc/fstab", "fstype": "nfs", "name": "/sfs", "opts": "rw,async", "passno": "0", "src": "sfs-nas1.cn-north-1.myhuaweicloud.com:/share-480fca1b"}
ok: [worker002] => {"changed": false, "dump": "0", "fstab": "/etc/fstab", "fstype": "nfs", "name": "/sfs", "opts": "rw,async", "passno": "0", "src": "sfs-nas1.cn-north-1.myhuaweicloud.com:/share-480fca1b"}
ok: [worker001] => {"changed": false, "dump": "0", "fstab": "/etc/fstab", "fstype": "nfs", "name": "/sfs", "opts": "rw,async", "passno": "0", "src": "sfs-nas1.cn-north-1.myhuaweicloud.com:/share-480fca1b"}
changed: [worker003] => {"changed": true, "dump": "0", "fstab": "/etc/fstab", "fstype": "nfs", "name": "/sfs", "opts": "rw,async", "passno": "0", "src": "sfs-nas1.cn-north-1.myhuaweicloud.com:/share-480fca1b"}

TASK [autofs : Load distribution-specific parameters] **************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/autofs/tasks/main.yml:8
included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/autofs/tasks/init-RedHat.yml for master001, worker001, worker002, worker003

TASK [autofs : Provide RHEL-specific values] ***********************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/autofs/tasks/init-RedHat.yml:6
ok: [master001] => {"ansible_facts": {"autofs_packages": ["autofs"], "autofs_service": "autofs", "autofs_username": "root"}, "changed": false}
ok: [worker001] => {"ansible_facts": {"autofs_packages": ["autofs"], "autofs_service": "autofs", "autofs_username": "root"}, "changed": false}
ok: [worker002] => {"ansible_facts": {"autofs_packages": ["autofs"], "autofs_service": "autofs", "autofs_username": "root"}, "changed": false}
ok: [worker003] => {"ansible_facts": {"autofs_packages": ["autofs"], "autofs_service": "autofs", "autofs_username": "root"}, "changed": false}

TASK [autofs : Deploy autofs configuration] ************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/autofs/tasks/main.yml:12
ok: [master001] => (item=etc/auto.master) => {"changed": false, "gid": 0, "group": "root", "item": "etc/auto.master", "mode": "0444", "owner": "root", "path": "/etc/auto.master", "size": 540, "state": "file", "uid": 0}
ok: [worker001] => (item=etc/auto.master) => {"changed": false, "gid": 0, "group": "root", "item": "etc/auto.master", "mode": "0444", "owner": "root", "path": "/etc/auto.master", "size": 540, "state": "file", "uid": 0}
ok: [worker002] => (item=etc/auto.master) => {"changed": false, "gid": 0, "group": "root", "item": "etc/auto.master", "mode": "0444", "owner": "root", "path": "/etc/auto.master", "size": 540, "state": "file", "uid": 0}
changed: [worker003] => (item=etc/auto.master) => {"changed": true, "checksum": "758d13b8a76f743e400c0badc6157e8327f1a8f1", "dest": "/etc/auto.master", "gid": 0, "group": "root", "item": "etc/auto.master", "md5sum": "e3d05b4f4a066db7039fed20a9574a94", "mode": "0444", "owner": "root", "size": 540, "src": "/root/.ansible/tmp/ansible-tmp-1525234596.26-255667904863737/source", "state": "file", "uid": 0}
ok: [master001] => (item=etc/auto.home) => {"changed": false, "gid": 0, "group": "root", "item": "etc/auto.home", "mode": "0444", "owner": "root", "path": "/etc/auto.home", "size": 852, "state": "file", "uid": 0}
ok: [worker002] => (item=etc/auto.home) => {"changed": false, "gid": 0, "group": "root", "item": "etc/auto.home", "mode": "0444", "owner": "root", "path": "/etc/auto.home", "size": 852, "state": "file", "uid": 0}
ok: [worker001] => (item=etc/auto.home) => {"changed": false, "gid": 0, "group": "root", "item": "etc/auto.home", "mode": "0444", "owner": "root", "path": "/etc/auto.home", "size": 852, "state": "file", "uid": 0}
changed: [worker003] => (item=etc/auto.home) => {"changed": true, "checksum": "df9645aa3e6a41b1aece0b5867b79d0c9c16f430", "dest": "/etc/auto.home", "gid": 0, "group": "root", "item": "etc/auto.home", "md5sum": "271d38d13da93386ff2c00c836c5608b", "mode": "0444", "owner": "root", "size": 852, "src": "/root/.ansible/tmp/ansible-tmp-1525234596.89-3640088616479/source", "state": "file", "uid": 0}

TASK [autofs : Deploy autofs mount script for NFSv4 (Debian/Ubuntu)] ***********
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/autofs/tasks/main.yml:24
skipping: [master001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [autofs : Install Autofs] *************************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/autofs/tasks/main.yml:34
ok: [master001] => (item=[u'autofs']) => {"changed": false, "item": ["autofs"], "msg": "", "rc": 0, "results": ["autofs-1:5.0.5-133.el6_9.x86_64 providing autofs is already installed"]}
ok: [worker001] => (item=[u'autofs']) => {"changed": false, "item": ["autofs"], "msg": "", "rc": 0, "results": ["autofs-1:5.0.5-133.el6_9.x86_64 providing autofs is already installed"]}
ok: [worker002] => (item=[u'autofs']) => {"changed": false, "item": ["autofs"], "msg": "", "rc": 0, "results": ["autofs-1:5.0.5-133.el6_9.x86_64 providing autofs is already installed"]}
ok: [worker003] => (item=[u'autofs']) => {"changed": false, "item": ["autofs"], "msg": "", "rc": 0, "results": ["autofs-1:5.0.5-133.el6_9.x86_64 providing autofs is already installed"]}

TASK [autofs : Ensure autofs is running and starts at boot] ********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/autofs/tasks/main.yml:41
ok: [master001] => {"changed": false, "enabled": true, "name": "autofs", "state": "started"}
ok: [worker001] => {"changed": false, "enabled": true, "name": "autofs", "state": "started"}
ok: [worker002] => {"changed": false, "enabled": true, "name": "autofs", "state": "started"}
ok: [worker003] => {"changed": false, "enabled": true, "name": "autofs", "state": "started"}

TASK [slurm-common : Load distribution-specific parameters] ********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:3
included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/init-RedHat.yml for master001, worker001, worker002, worker003

TASK [slurm-common : Set SLURM common playbook params (RHEL 7.x compatible)] ***
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/init-RedHat.yml:3
ok: [master001] => {"ansible_facts": {"slurm_pid_dir": "/var/run"}, "changed": false}
ok: [worker001] => {"ansible_facts": {"slurm_pid_dir": "/var/run"}, "changed": false}
ok: [worker002] => {"ansible_facts": {"slurm_pid_dir": "/var/run"}, "changed": false}
ok: [worker003] => {"ansible_facts": {"slurm_pid_dir": "/var/run"}, "changed": false}

TASK [slurm-common : Create `slurm` system group] ******************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:16
ok: [worker003] => {"changed": false, "gid": 498, "name": "slurm", "state": "present", "system": true}
ok: [master001] => {"changed": false, "gid": 498, "name": "slurm", "state": "present", "system": true}
ok: [worker002] => {"changed": false, "gid": 498, "name": "slurm", "state": "present", "system": true}
ok: [worker001] => {"changed": false, "gid": 498, "name": "slurm", "state": "present", "system": true}

TASK [slurm-common : Create `slurm` system user] *******************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:24
ok: [master001] => {"append": false, "changed": false, "comment": "", "group": 498, "home": "/home/slurm", "move_home": false, "name": "slurm", "shell": "/bin/bash", "state": "present", "uid": 498}
ok: [worker002] => {"append": false, "changed": false, "comment": "", "group": 498, "home": "/home/slurm", "move_home": false, "name": "slurm", "shell": "/bin/bash", "state": "present", "uid": 498}
ok: [worker001] => {"append": false, "changed": false, "comment": "", "group": 498, "home": "/home/slurm", "move_home": false, "name": "slurm", "shell": "/bin/bash", "state": "present", "uid": 498}
ok: [worker003] => {"append": false, "changed": false, "comment": "", "group": 498, "home": "/home/slurm", "move_home": false, "name": "slurm", "shell": "/bin/bash", "state": "present", "uid": 498}

TASK [slurm-common : Create work directory {{item}}] ***************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:33
ok: [master001] => (item=/etc/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/etc/slurm", "mode": "0755", "owner": "slurm", "path": "/etc/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [worker001] => (item=/etc/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/etc/slurm", "mode": "0755", "owner": "slurm", "path": "/etc/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [worker002] => (item=/etc/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/etc/slurm", "mode": "0755", "owner": "slurm", "path": "/etc/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [worker003] => (item=/etc/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/etc/slurm", "mode": "0755", "owner": "slurm", "path": "/etc/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [master001] => (item=/var/lib/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/lib/slurm", "mode": "0755", "owner": "slurm", "path": "/var/lib/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [worker001] => (item=/var/lib/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/lib/slurm", "mode": "0755", "owner": "slurm", "path": "/var/lib/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [worker002] => (item=/var/lib/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/lib/slurm", "mode": "0755", "owner": "slurm", "path": "/var/lib/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [worker003] => (item=/var/lib/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/lib/slurm", "mode": "0755", "owner": "slurm", "path": "/var/lib/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [master001] => (item=/var/lib/slurm/checkpoint) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/lib/slurm/checkpoint", "mode": "0755", "owner": "slurm", "path": "/var/lib/slurm/checkpoint", "size": 4096, "state": "directory", "uid": 498}
ok: [worker002] => (item=/var/lib/slurm/checkpoint) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/lib/slurm/checkpoint", "mode": "0755", "owner": "slurm", "path": "/var/lib/slurm/checkpoint", "size": 4096, "state": "directory", "uid": 498}
ok: [worker003] => (item=/var/lib/slurm/checkpoint) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/lib/slurm/checkpoint", "mode": "0755", "owner": "slurm", "path": "/var/lib/slurm/checkpoint", "size": 4096, "state": "directory", "uid": 498}
ok: [master001] => (item=/var/lib/slurm/slurmctld) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/lib/slurm/slurmctld", "mode": "0755", "owner": "slurm", "path": "/var/lib/slurm/slurmctld", "size": 4096, "state": "directory", "uid": 498}
ok: [worker001] => (item=/var/lib/slurm/checkpoint) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/lib/slurm/checkpoint", "mode": "0755", "owner": "slurm", "path": "/var/lib/slurm/checkpoint", "size": 4096, "state": "directory", "uid": 498}
ok: [worker002] => (item=/var/lib/slurm/slurmctld) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/lib/slurm/slurmctld", "mode": "0755", "owner": "slurm", "path": "/var/lib/slurm/slurmctld", "size": 4096, "state": "directory", "uid": 498}
ok: [master001] => (item=/var/lib/slurm/slurmd) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/lib/slurm/slurmd", "mode": "0755", "owner": "slurm", "path": "/var/lib/slurm/slurmd", "size": 4096, "state": "directory", "uid": 498}
ok: [worker003] => (item=/var/lib/slurm/slurmctld) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/lib/slurm/slurmctld", "mode": "0755", "owner": "slurm", "path": "/var/lib/slurm/slurmctld", "size": 4096, "state": "directory", "uid": 498}
ok: [worker001] => (item=/var/lib/slurm/slurmctld) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/lib/slurm/slurmctld", "mode": "0755", "owner": "slurm", "path": "/var/lib/slurm/slurmctld", "size": 4096, "state": "directory", "uid": 498}
ok: [worker002] => (item=/var/lib/slurm/slurmd) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/lib/slurm/slurmd", "mode": "0755", "owner": "slurm", "path": "/var/lib/slurm/slurmd", "size": 4096, "state": "directory", "uid": 498}
ok: [master001] => (item=/var/log/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/log/slurm", "mode": "0755", "owner": "slurm", "path": "/var/log/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [worker003] => (item=/var/lib/slurm/slurmd) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/lib/slurm/slurmd", "mode": "0755", "owner": "slurm", "path": "/var/lib/slurm/slurmd", "size": 4096, "state": "directory", "uid": 498}
ok: [worker001] => (item=/var/lib/slurm/slurmd) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/lib/slurm/slurmd", "mode": "0755", "owner": "slurm", "path": "/var/lib/slurm/slurmd", "size": 4096, "state": "directory", "uid": 498}
ok: [worker002] => (item=/var/log/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/log/slurm", "mode": "0755", "owner": "slurm", "path": "/var/log/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [master001] => (item=/var/run/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/run/slurm", "mode": "0755", "owner": "slurm", "path": "/var/run/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [worker003] => (item=/var/log/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/log/slurm", "mode": "0755", "owner": "slurm", "path": "/var/log/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [worker001] => (item=/var/log/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/log/slurm", "mode": "0755", "owner": "slurm", "path": "/var/log/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [master001] => (item=/var/spool/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/spool/slurm", "mode": "0755", "owner": "slurm", "path": "/var/spool/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [worker002] => (item=/var/run/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/run/slurm", "mode": "0755", "owner": "slurm", "path": "/var/run/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [worker003] => (item=/var/run/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/run/slurm", "mode": "0755", "owner": "slurm", "path": "/var/run/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [worker001] => (item=/var/run/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/run/slurm", "mode": "0755", "owner": "slurm", "path": "/var/run/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [worker002] => (item=/var/spool/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/spool/slurm", "mode": "0755", "owner": "slurm", "path": "/var/spool/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [worker003] => (item=/var/spool/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/spool/slurm", "mode": "0755", "owner": "slurm", "path": "/var/spool/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [worker001] => (item=/var/spool/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/spool/slurm", "mode": "0755", "owner": "slurm", "path": "/var/spool/slurm", "size": 4096, "state": "directory", "uid": 498}

TASK [slurm-common : Make compatibility symlinks] ******************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:56
skipping: [master001] => (item={u'to': u'/var/lib/slurm-llnl', u'from': u'/var/lib/slurm'})  => {"changed": false, "item": {"from": "/var/lib/slurm", "to": "/var/lib/slurm-llnl"}, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [master001] => (item={u'to': u'/etc/slurm-llnl', u'from': u'/etc/slurm'})  => {"changed": false, "item": {"from": "/etc/slurm", "to": "/etc/slurm-llnl"}, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => (item={u'to': u'/etc/slurm-llnl', u'from': u'/etc/slurm'})  => {"changed": false, "item": {"from": "/etc/slurm", "to": "/etc/slurm-llnl"}, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [master001] => (item={u'to': u'/var/log/slurm-llnl', u'from': u'/var/log/slurm'})  => {"changed": false, "item": {"from": "/var/log/slurm", "to": "/var/log/slurm-llnl"}, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => (item={u'to': u'/var/lib/slurm-llnl', u'from': u'/var/lib/slurm'})  => {"changed": false, "item": {"from": "/var/lib/slurm", "to": "/var/lib/slurm-llnl"}, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [master001] => (item={u'to': u'/var/run/slurm-llnl', u'from': u'/var/run/slurm'})  => {"changed": false, "item": {"from": "/var/run/slurm", "to": "/var/run/slurm-llnl"}, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item={u'to': u'/etc/slurm-llnl', u'from': u'/etc/slurm'})  => {"changed": false, "item": {"from": "/etc/slurm", "to": "/etc/slurm-llnl"}, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => (item={u'to': u'/var/log/slurm-llnl', u'from': u'/var/log/slurm'})  => {"changed": false, "item": {"from": "/var/log/slurm", "to": "/var/log/slurm-llnl"}, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item={u'to': u'/var/lib/slurm-llnl', u'from': u'/var/lib/slurm'})  => {"changed": false, "item": {"from": "/var/lib/slurm", "to": "/var/lib/slurm-llnl"}, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => (item={u'to': u'/var/run/slurm-llnl', u'from': u'/var/run/slurm'})  => {"changed": false, "item": {"from": "/var/run/slurm", "to": "/var/run/slurm-llnl"}, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item={u'to': u'/var/log/slurm-llnl', u'from': u'/var/log/slurm'})  => {"changed": false, "item": {"from": "/var/log/slurm", "to": "/var/log/slurm-llnl"}, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => (item={u'to': u'/etc/slurm-llnl', u'from': u'/etc/slurm'})  => {"changed": false, "item": {"from": "/etc/slurm", "to": "/etc/slurm-llnl"}, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => (item={u'to': u'/var/lib/slurm-llnl', u'from': u'/var/lib/slurm'})  => {"changed": false, "item": {"from": "/var/lib/slurm", "to": "/var/lib/slurm-llnl"}, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item={u'to': u'/var/run/slurm-llnl', u'from': u'/var/run/slurm'})  => {"changed": false, "item": {"from": "/var/run/slurm", "to": "/var/run/slurm-llnl"}, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => (item={u'to': u'/var/log/slurm-llnl', u'from': u'/var/log/slurm'})  => {"changed": false, "item": {"from": "/var/log/slurm", "to": "/var/log/slurm-llnl"}, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => (item={u'to': u'/var/run/slurm-llnl', u'from': u'/var/run/slurm'})  => {"changed": false, "item": {"from": "/var/run/slurm", "to": "/var/run/slurm-llnl"}, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-common : Enable Copr SLURM repo by verdurin] ***********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:73
ok: [master001] => {"changed": false, "gid": 0, "group": "root", "mode": "0444", "owner": "root", "path": "/etc/yum.repos.d/copr-slurm.repo", "size": 615, "state": "file", "uid": 0}
ok: [worker001] => {"changed": false, "gid": 0, "group": "root", "mode": "0444", "owner": "root", "path": "/etc/yum.repos.d/copr-slurm.repo", "size": 615, "state": "file", "uid": 0}
ok: [worker002] => {"changed": false, "gid": 0, "group": "root", "mode": "0444", "owner": "root", "path": "/etc/yum.repos.d/copr-slurm.repo", "size": 615, "state": "file", "uid": 0}
changed: [worker003] => {"changed": true, "checksum": "18960fef3cdfefe9d932e02cb1791c9c43780896", "dest": "/etc/yum.repos.d/copr-slurm.repo", "gid": 0, "group": "root", "md5sum": "9d472c764f0985eec79d4192598f2d06", "mode": "0444", "owner": "root", "size": 615, "src": "/root/.ansible/tmp/ansible-tmp-1525234601.86-240369846381865/source", "state": "file", "uid": 0}

TASK [slurm-common : Prevent MUNGE from starting] ******************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:20
skipping: [master001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-common : Install MUNGE (Debian/Ubuntu)] ****************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:31
skipping: [master001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-common : Install MUNGE (RHEL-compatible)] **************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:43
ok: [master001] => {"changed": false, "msg": "", "rc": 0, "results": ["slurm-munge-17.02.7-1.el6.x86_64 providing slurm-munge is already installed"]}
ok: [worker001] => {"changed": false, "msg": "", "rc": 0, "results": ["slurm-munge-17.02.7-1.el6.x86_64 providing slurm-munge is already installed"]}
ok: [worker002] => {"changed": false, "msg": "", "rc": 0, "results": ["slurm-munge-17.02.7-1.el6.x86_64 providing slurm-munge is already installed"]}
changed: [worker003] => {"changed": true, "msg": "Warning: RPMDB altered outside of yum.\n", "rc": 0, "results": ["Loaded plugins: fastestmirror, security\nSetting up Install Process\nLoading mirror speeds from cached hostfile\nResolving Dependencies\n--> Running transaction check\n---> Package slurm-munge.x86_64 0:17.02.7-1.el6 will be installed\n--> Processing Dependency: slurm for package: slurm-munge-17.02.7-1.el6.x86_64\n--> Running transaction check\n---> Package slurm.x86_64 0:17.02.7-1.el6 will be installed\n--> Processing Dependency: slurm-plugins for package: slurm-17.02.7-1.el6.x86_64\n--> Running transaction check\n---> Package slurm-plugins.x86_64 0:17.02.7-1.el6 will be installed\n--> Finished Dependency Resolution\n\nDependencies Resolved\n\n================================================================================\n Package             Arch         Version               Repository         Size\n================================================================================\nInstalling:\n slurm-munge         x86_64       17.02.7-1.el6         local-slurm        16 k\nInstalling for dependencies:\n slurm               x86_64       17.02.7-1.el6         local-slurm        27 M\n slurm-plugins       x86_64       17.02.7-1.el6         local-slurm       1.1 M\n\nTransaction Summary\n================================================================================\nInstall       3 Package(s)\n\nTotal download size: 28 M\nInstalled size: 98 M\nDownloading Packages:\n--------------------------------------------------------------------------------\nTotal                                            83 MB/s |  28 MB     00:00     \nRunning rpm_check_debug\nRunning Transaction Test\nTransaction Test Succeeded\nRunning Transaction\n\r  Installing : slurm-plugins-17.02.7-1.el6.x86_64                           1/3 \n\r  Installing : slurm-17.02.7-1.el6.x86_64                                   2/3 \n\r  Installing : slurm-munge-17.02.7-1.el6.x86_64                             3/3 \n\r  Verifying  : slurm-plugins-17.02.7-1.el6.x86_64                           1/3 \n\r  Verifying  : slurm-17.02.7-1.el6.x86_64                                   2/3 \n\r  Verifying  : slurm-munge-17.02.7-1.el6.x86_64                             3/3 \n\nInstalled:\n  slurm-munge.x86_64 0:17.02.7-1.el6                                            \n\nDependency Installed:\n  slurm.x86_64 0:17.02.7-1.el6       slurm-plugins.x86_64 0:17.02.7-1.el6      \n\nComplete!\n"]}

TASK [slurm-common : Allow MUNGE to start] *************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:57
skipping: [master001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-common : Add `--syslog` option to `munged` startup] ****************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:66
skipping: [master001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-common : Reload systemd configuration] *****************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:77
skipping: [master001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-common : Add `--syslog` startup options for `munged` (Ubuntu 14.04)] ***
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:86
skipping: [master001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-common : Configure MUNGE] ******************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:101
ok: [master001] => {"changed": false, "checksum": "222afe887f907039b799b394715664ffa584fec5", "dest": "/etc/munge/munge.key", "gid": 497, "group": "munge", "mode": "0400", "owner": "munge", "path": "/etc/munge/munge.key", "size": 1024, "state": "file", "uid": 497}
ok: [worker001] => {"changed": false, "checksum": "222afe887f907039b799b394715664ffa584fec5", "dest": "/etc/munge/munge.key", "gid": 497, "group": "munge", "mode": "0400", "owner": "munge", "path": "/etc/munge/munge.key", "size": 1024, "state": "file", "uid": 497}
ok: [worker002] => {"changed": false, "checksum": "222afe887f907039b799b394715664ffa584fec5", "dest": "/etc/munge/munge.key", "gid": 497, "group": "munge", "mode": "0400", "owner": "munge", "path": "/etc/munge/munge.key", "size": 1024, "state": "file", "uid": 497}
ok: [worker003] => {"changed": false, "checksum": "222afe887f907039b799b394715664ffa584fec5", "dest": "/etc/munge/munge.key", "gid": 497, "group": "munge", "mode": "0400", "owner": "munge", "path": "/etc/munge/munge.key", "size": 1024, "state": "file", "uid": 497}

TASK [slurm-common : Ensure the MUNGE service is running] **********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:114
ok: [master001] => {"changed": false, "enabled": true, "name": "munge", "state": "started"}
ok: [worker001] => {"changed": false, "enabled": true, "name": "munge", "state": "started"}
ok: [worker002] => {"changed": false, "enabled": true, "name": "munge", "state": "started"}
ok: [worker003] => {"changed": false, "enabled": true, "name": "munge", "state": "started"}

TASK [slurm-common : Deploy SLURM configuration file] **************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:86
changed: [master001] => {"changed": true, "checksum": "5ed21d6ae242d04b9d14e0e35de42aaa631da19c", "dest": "/etc/slurm/slurm.conf", "gid": 0, "group": "root", "md5sum": "d6bc97d54d11554ead2e45cedd13bff7", "mode": "0444", "owner": "root", "size": 4244, "src": "/root/.ansible/tmp/ansible-tmp-1525234619.32-259356813179426/source", "state": "file", "uid": 0}
changed: [worker001] => {"changed": true, "checksum": "5ed21d6ae242d04b9d14e0e35de42aaa631da19c", "dest": "/etc/slurm/slurm.conf", "gid": 0, "group": "root", "md5sum": "d6bc97d54d11554ead2e45cedd13bff7", "mode": "0444", "owner": "root", "size": 4244, "src": "/root/.ansible/tmp/ansible-tmp-1525234619.34-139253006418192/source", "state": "file", "uid": 0}
changed: [worker003] => {"changed": true, "checksum": "5ed21d6ae242d04b9d14e0e35de42aaa631da19c", "dest": "/etc/slurm/slurm.conf", "gid": 0, "group": "root", "md5sum": "d6bc97d54d11554ead2e45cedd13bff7", "mode": "0444", "owner": "root", "size": 4244, "src": "/root/.ansible/tmp/ansible-tmp-1525234619.4-186318801826973/source", "state": "file", "uid": 0}
changed: [worker002] => {"changed": true, "checksum": "5ed21d6ae242d04b9d14e0e35de42aaa631da19c", "dest": "/etc/slurm/slurm.conf", "gid": 0, "group": "root", "md5sum": "d6bc97d54d11554ead2e45cedd13bff7", "mode": "0444", "owner": "root", "size": 4244, "src": "/root/.ansible/tmp/ansible-tmp-1525234619.38-136400930935506/source", "state": "file", "uid": 0}

TASK [slurm-common : Deploy script to suspend cloud instance] ******************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:96
ok: [master001] => {"changed": false, "gid": 0, "group": "root", "mode": "0555", "owner": "root", "path": "/etc/slurm/slurm.suspend.sh", "size": 604, "state": "file", "uid": 0}
ok: [worker001] => {"changed": false, "gid": 0, "group": "root", "mode": "0555", "owner": "root", "path": "/etc/slurm/slurm.suspend.sh", "size": 604, "state": "file", "uid": 0}
ok: [worker002] => {"changed": false, "gid": 0, "group": "root", "mode": "0555", "owner": "root", "path": "/etc/slurm/slurm.suspend.sh", "size": 604, "state": "file", "uid": 0}
changed: [worker003] => {"changed": true, "checksum": "8377db742e981ff95e4e9f833eca530fe3248f44", "dest": "/etc/slurm/slurm.suspend.sh", "gid": 0, "group": "root", "md5sum": "1f5ceccbb484e295c67bbcdb3710ac5f", "mode": "0555", "owner": "root", "size": 604, "src": "/root/.ansible/tmp/ansible-tmp-1525234620.18-172117429807250/source", "state": "file", "uid": 0}

TASK [slurm-common : Deploy script to resume cloud instance] *******************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:106
ok: [master001] => {"changed": false, "gid": 0, "group": "root", "mode": "0555", "owner": "root", "path": "/etc/slurm/slurm.resume.sh", "size": 631, "state": "file", "uid": 0}
ok: [worker001] => {"changed": false, "gid": 0, "group": "root", "mode": "0555", "owner": "root", "path": "/etc/slurm/slurm.resume.sh", "size": 631, "state": "file", "uid": 0}
ok: [worker002] => {"changed": false, "gid": 0, "group": "root", "mode": "0555", "owner": "root", "path": "/etc/slurm/slurm.resume.sh", "size": 631, "state": "file", "uid": 0}
changed: [worker003] => {"changed": true, "checksum": "9abb021ec034c1875481fbe2e5ed2975469997aa", "dest": "/etc/slurm/slurm.resume.sh", "gid": 0, "group": "root", "md5sum": "c9c92c1d3a601152bc7c6fdaa508ac5b", "mode": "0555", "owner": "root", "size": 631, "src": "/root/.ansible/tmp/ansible-tmp-1525234620.85-226774966888366/source", "state": "file", "uid": 0}

TASK [slurm-common : Install support packages (Debian/Ubuntu)] *****************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:116
skipping: [master001] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-common : Install SLURM packages (RHEL-compatible)] *****************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:127
skipping: [master001] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-client : Install required SLURM packages (Debian/Ubuntu)] **********
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-client/tasks/main.yml:4
skipping: [master001] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-client : Install required SLURM packages (older Debian/Ubuntu)] ****
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-client/tasks/main.yml:23
skipping: [master001] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-client : service] **************************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-client/tasks/main.yml:35
skipping: [master001] => (item=slurm-llnl)  => {"changed": false, "item": "slurm-llnl", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => (item=slurm-llnl)  => {"changed": false, "item": "slurm-llnl", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item=slurm-llnl)  => {"changed": false, "item": "slurm-llnl", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => (item=slurm-llnl)  => {"changed": false, "item": "slurm-llnl", "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-client : Install SLURM packages (RHEL-compatible)] *****************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-client/tasks/main.yml:47
ok: [master001] => (item=[u'slurm', u'slurm-devel', u'slurm-perlapi', u'slurm-contribs', u'slurm-torque']) => {"changed": false, "item": ["slurm", "slurm-devel", "slurm-perlapi", "slurm-contribs", "slurm-torque"], "msg": "", "rc": 0, "results": ["slurm-17.02.7-1.el6.x86_64 providing slurm is already installed", "slurm-devel-17.02.7-1.el6.x86_64 providing slurm-devel is already installed", "slurm-perlapi-17.02.7-1.el6.x86_64 providing slurm-perlapi is already installed", "slurm-contribs-17.02.7-1.el6.x86_64 providing slurm-contribs is already installed", "slurm-torque-17.02.7-1.el6.x86_64 providing slurm-torque is already installed"]}
ok: [worker001] => (item=[u'slurm', u'slurm-devel', u'slurm-perlapi', u'slurm-contribs', u'slurm-torque']) => {"changed": false, "item": ["slurm", "slurm-devel", "slurm-perlapi", "slurm-contribs", "slurm-torque"], "msg": "", "rc": 0, "results": ["slurm-17.02.7-1.el6.x86_64 providing slurm is already installed", "slurm-devel-17.02.7-1.el6.x86_64 providing slurm-devel is already installed", "slurm-perlapi-17.02.7-1.el6.x86_64 providing slurm-perlapi is already installed", "slurm-contribs-17.02.7-1.el6.x86_64 providing slurm-contribs is already installed", "slurm-torque-17.02.7-1.el6.x86_64 providing slurm-torque is already installed"]}
ok: [worker002] => (item=[u'slurm', u'slurm-devel', u'slurm-perlapi', u'slurm-contribs', u'slurm-torque']) => {"changed": false, "item": ["slurm", "slurm-devel", "slurm-perlapi", "slurm-contribs", "slurm-torque"], "msg": "", "rc": 0, "results": ["slurm-17.02.7-1.el6.x86_64 providing slurm is already installed", "slurm-devel-17.02.7-1.el6.x86_64 providing slurm-devel is already installed", "slurm-perlapi-17.02.7-1.el6.x86_64 providing slurm-perlapi is already installed", "slurm-contribs-17.02.7-1.el6.x86_64 providing slurm-contribs is already installed", "slurm-torque-17.02.7-1.el6.x86_64 providing slurm-torque is already installed"]}
changed: [worker003] => (item=[u'slurm', u'slurm-devel', u'slurm-perlapi', u'slurm-contribs', u'slurm-torque']) => {"changed": true, "item": ["slurm", "slurm-devel", "slurm-perlapi", "slurm-contribs", "slurm-torque"], "msg": "", "rc": 0, "results": ["slurm-17.02.7-1.el6.x86_64 providing slurm is already installed", "Loaded plugins: fastestmirror, security\nSetting up Install Process\nLoading mirror speeds from cached hostfile\nResolving Dependencies\n--> Running transaction check\n---> Package slurm-contribs.x86_64 0:17.02.7-1.el6 will be installed\n---> Package slurm-devel.x86_64 0:17.02.7-1.el6 will be installed\n---> Package slurm-perlapi.x86_64 0:17.02.7-1.el6 will be installed\n---> Package slurm-torque.x86_64 0:17.02.7-1.el6 will be installed\n--> Finished Dependency Resolution\n\nDependencies Resolved\n\n================================================================================\n Package              Arch         Version              Repository         Size\n================================================================================\nInstalling:\n slurm-contribs       x86_64       17.02.7-1.el6        local-slurm        17 k\n slurm-devel          x86_64       17.02.7-1.el6        local-slurm       143 k\n slurm-perlapi        x86_64       17.02.7-1.el6        local-slurm       456 k\n slurm-torque         x86_64       17.02.7-1.el6        local-slurm        37 k\n\nTransaction Summary\n================================================================================\nInstall       4 Package(s)\n\nTotal download size: 653 k\nInstalled size: 2.5 M\nDownloading Packages:\n--------------------------------------------------------------------------------\nTotal                                            12 MB/s | 653 kB     00:00     \nRunning rpm_check_debug\nRunning Transaction Test\nTransaction Test Succeeded\nRunning Transaction\n\r  Installing : slurm-perlapi-17.02.7-1.el6.x86_64                           1/4 \n\r  Installing : slurm-torque-17.02.7-1.el6.x86_64                            2/4 \n\r  Installing : slurm-contribs-17.02.7-1.el6.x86_64                          3/4 \n\r  Installing : slurm-devel-17.02.7-1.el6.x86_64                             4/4 \n\r  Verifying  : slurm-perlapi-17.02.7-1.el6.x86_64                           1/4 \n\r  Verifying  : slurm-devel-17.02.7-1.el6.x86_64                             2/4 \n\r  Verifying  : slurm-torque-17.02.7-1.el6.x86_64                            3/4 \n\r  Verifying  : slurm-contribs-17.02.7-1.el6.x86_64                          4/4 \n\nInstalled:\n  slurm-contribs.x86_64 0:17.02.7-1.el6   slurm-devel.x86_64 0:17.02.7-1.el6   \n  slurm-perlapi.x86_64 0:17.02.7-1.el6    slurm-torque.x86_64 0:17.02.7-1.el6  \n\nComplete!\n"]}

TASK [slurm-client : Stop SLURM services (RHEL/CentOS 6.x)] ********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-client/tasks/main.yml:59
changed: [worker003] => {"changed": true, "enabled": false, "name": "slurm", "state": "stopped"}
changed: [master001] => {"changed": true, "enabled": false, "name": "slurm", "state": "stopped"}
changed: [worker001] => {"changed": true, "enabled": false, "name": "slurm", "state": "stopped"}
changed: [worker002] => {"changed": true, "enabled": false, "name": "slurm", "state": "stopped"}

TASK [slurm-client : Stop SLURM services (RHEL/CentOS 7.x)] ********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-client/tasks/main.yml:66
skipping: [master001] => (item=slurmd)  => {"changed": false, "item": "slurmd", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [master001] => (item=slurmctld)  => {"changed": false, "item": "slurmctld", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => (item=slurmd)  => {"changed": false, "item": "slurmd", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => (item=slurmctld)  => {"changed": false, "item": "slurmctld", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item=slurmd)  => {"changed": false, "item": "slurmd", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item=slurmctld)  => {"changed": false, "item": "slurmctld", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => (item=slurmd)  => {"changed": false, "item": "slurmd", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => (item=slurmctld)  => {"changed": false, "item": "slurmctld", "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-worker : Load distribution-specific parameters] ********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/main.yml:4
included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/init-RedHat.yml for master001, worker001, worker002, worker003

TASK [slurm-worker : Set SLURM worker playbook params (RHEL compatible)] *******
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/init-RedHat.yml:3
ok: [master001] => {"ansible_facts": {"slurmd_packages": ["slurm", "slurm-plugins", "slurm-sql"]}, "changed": false}
ok: [worker001] => {"ansible_facts": {"slurmd_packages": ["slurm", "slurm-plugins", "slurm-sql"]}, "changed": false}
ok: [worker002] => {"ansible_facts": {"slurmd_packages": ["slurm", "slurm-plugins", "slurm-sql"]}, "changed": false}
ok: [worker003] => {"ansible_facts": {"slurmd_packages": ["slurm", "slurm-plugins", "slurm-sql"]}, "changed": false}

TASK [slurm-worker : Set SLURM worker service name (RHEL 7.x compatible)] ******
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/init-RedHat.yml:11
skipping: [master001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-worker : Set SLURM worker service name (RHEL 6.x compatible)] ******
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/init-RedHat.yml:16
ok: [master001] => {"ansible_facts": {"slurmd_service": "slurm"}, "changed": false}
ok: [worker001] => {"ansible_facts": {"slurmd_service": "slurm"}, "changed": false}
ok: [worker002] => {"ansible_facts": {"slurmd_service": "slurm"}, "changed": false}
ok: [worker003] => {"ansible_facts": {"slurmd_service": "slurm"}, "changed": false}

TASK [slurm-worker : Deploy SLURM configuration files] *************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/main.yml:7
ok: [master001] => (item=gres.conf) => {"changed": false, "gid": 0, "group": "root", "item": "gres.conf", "mode": "0444", "owner": "root", "path": "/etc/slurm/gres.conf", "size": 297, "state": "file", "uid": 0}
ok: [worker001] => (item=gres.conf) => {"changed": false, "gid": 0, "group": "root", "item": "gres.conf", "mode": "0444", "owner": "root", "path": "/etc/slurm/gres.conf", "size": 297, "state": "file", "uid": 0}
ok: [worker002] => (item=gres.conf) => {"changed": false, "gid": 0, "group": "root", "item": "gres.conf", "mode": "0444", "owner": "root", "path": "/etc/slurm/gres.conf", "size": 297, "state": "file", "uid": 0}
changed: [worker003] => (item=gres.conf) => {"changed": true, "checksum": "afc9ac4c3247ee557700305700c9743a79f0dfd0", "dest": "/etc/slurm/gres.conf", "gid": 0, "group": "root", "item": "gres.conf", "md5sum": "a178d15a40bf70261e38f1cd729a8d56", "mode": "0444", "owner": "root", "size": 297, "src": "/root/.ansible/tmp/ansible-tmp-1525234628.55-189285197029326/source", "state": "file", "uid": 0}

TASK [slurm-worker : Deploy kernel config check script] ************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/cgroup.yml:3
skipping: [master001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-worker : Check that kernel is configured for cgroups] **************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/cgroup.yml:12
skipping: [master001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-worker : Ensure cgroup filesystems are mounted] ********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/cgroup.yml:18
skipping: [master001] => (item=blkio)  => {"changed": false, "item": "blkio", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [master001] => (item=cpuacct)  => {"changed": false, "item": "cpuacct", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => (item=blkio)  => {"changed": false, "item": "blkio", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [master001] => (item=cpuset)  => {"changed": false, "item": "cpuset", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => (item=cpuacct)  => {"changed": false, "item": "cpuacct", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item=blkio)  => {"changed": false, "item": "blkio", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => (item=cpuset)  => {"changed": false, "item": "cpuset", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [master001] => (item=devices)  => {"changed": false, "item": "devices", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item=cpuacct)  => {"changed": false, "item": "cpuacct", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [master001] => (item=freezer)  => {"changed": false, "item": "freezer", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => (item=devices)  => {"changed": false, "item": "devices", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => (item=blkio)  => {"changed": false, "item": "blkio", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item=cpuset)  => {"changed": false, "item": "cpuset", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => (item=cpuacct)  => {"changed": false, "item": "cpuacct", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => (item=freezer)  => {"changed": false, "item": "freezer", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [master001] => (item=memory)  => {"changed": false, "item": "memory", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item=devices)  => {"changed": false, "item": "devices", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => (item=cpuset)  => {"changed": false, "item": "cpuset", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => (item=memory)  => {"changed": false, "item": "memory", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item=freezer)  => {"changed": false, "item": "freezer", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => (item=devices)  => {"changed": false, "item": "devices", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item=memory)  => {"changed": false, "item": "memory", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => (item=freezer)  => {"changed": false, "item": "freezer", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => (item=memory)  => {"changed": false, "item": "memory", "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-worker : Ensure kernel is booted with swap accounting enabled] *****
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/cgroup.yml:44
skipping: [master001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-worker : Check if swap accouting is enabled (may fail!)] ***********
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/cgroup.yml:53
skipping: [master001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-worker : Reboot to enable swap accounting] *************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/cgroup.yml:59
skipping: [master001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-worker : Wait for server to come up again] *************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/cgroup.yml:72
skipping: [master001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-worker : Ensure release agent directory exists] ********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/cgroup.yml:84
skipping: [master001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-worker : Deploy cgroup-specific release agent script] **************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/cgroup.yml:93
skipping: [master001] => (item=blkio)  => {"changed": false, "item": "blkio", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [master001] => (item=cpuacct)  => {"changed": false, "item": "cpuacct", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => (item=blkio)  => {"changed": false, "item": "blkio", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [master001] => (item=cpuset)  => {"changed": false, "item": "cpuset", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => (item=cpuacct)  => {"changed": false, "item": "cpuacct", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [master001] => (item=devices)  => {"changed": false, "item": "devices", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item=blkio)  => {"changed": false, "item": "blkio", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => (item=cpuset)  => {"changed": false, "item": "cpuset", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item=cpuacct)  => {"changed": false, "item": "cpuacct", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [master001] => (item=freezer)  => {"changed": false, "item": "freezer", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => (item=blkio)  => {"changed": false, "item": "blkio", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => (item=devices)  => {"changed": false, "item": "devices", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item=cpuset)  => {"changed": false, "item": "cpuset", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => (item=cpuacct)  => {"changed": false, "item": "cpuacct", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [master001] => (item=memory)  => {"changed": false, "item": "memory", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => (item=freezer)  => {"changed": false, "item": "freezer", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item=devices)  => {"changed": false, "item": "devices", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => (item=cpuset)  => {"changed": false, "item": "cpuset", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => (item=memory)  => {"changed": false, "item": "memory", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item=freezer)  => {"changed": false, "item": "freezer", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => (item=devices)  => {"changed": false, "item": "devices", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item=memory)  => {"changed": false, "item": "memory", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => (item=freezer)  => {"changed": false, "item": "freezer", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => (item=memory)  => {"changed": false, "item": "memory", "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-worker : Deploy SLURM cgroup configuration file (main)] ************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/cgroup.yml:103
skipping: [master001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-worker : Deploy SLURM cgroup configuration file (devices)] *********
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/cgroup.yml:112
skipping: [master001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-worker : Install SLURM worker packages] ****************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/main.yml:27
ok: [master001] => (item=[u'slurm', u'slurm-plugins', u'slurm-sql']) => {"changed": false, "item": ["slurm", "slurm-plugins", "slurm-sql"], "msg": "", "rc": 0, "results": ["slurm-17.02.7-1.el6.x86_64 providing slurm is already installed", "slurm-plugins-17.02.7-1.el6.x86_64 providing slurm-plugins is already installed", "slurm-sql-17.02.7-1.el6.x86_64 providing slurm-sql is already installed"]}
ok: [worker002] => (item=[u'slurm', u'slurm-plugins', u'slurm-sql']) => {"changed": false, "item": ["slurm", "slurm-plugins", "slurm-sql"], "msg": "", "rc": 0, "results": ["slurm-17.02.7-1.el6.x86_64 providing slurm is already installed", "slurm-plugins-17.02.7-1.el6.x86_64 providing slurm-plugins is already installed", "slurm-sql-17.02.7-1.el6.x86_64 providing slurm-sql is already installed"]}
ok: [worker001] => (item=[u'slurm', u'slurm-plugins', u'slurm-sql']) => {"changed": false, "item": ["slurm", "slurm-plugins", "slurm-sql"], "msg": "", "rc": 0, "results": ["slurm-17.02.7-1.el6.x86_64 providing slurm is already installed", "slurm-plugins-17.02.7-1.el6.x86_64 providing slurm-plugins is already installed", "slurm-sql-17.02.7-1.el6.x86_64 providing slurm-sql is already installed"]}
changed: [worker003] => (item=[u'slurm', u'slurm-plugins', u'slurm-sql']) => {"changed": true, "item": ["slurm", "slurm-plugins", "slurm-sql"], "msg": "", "rc": 0, "results": ["slurm-17.02.7-1.el6.x86_64 providing slurm is already installed", "slurm-plugins-17.02.7-1.el6.x86_64 providing slurm-plugins is already installed", "Loaded plugins: fastestmirror, security\nSetting up Install Process\nLoading mirror speeds from cached hostfile\nResolving Dependencies\n--> Running transaction check\n---> Package slurm-sql.x86_64 0:17.02.7-1.el6 will be installed\n--> Finished Dependency Resolution\n\nDependencies Resolved\n\n================================================================================\n Package          Arch          Version                Repository          Size\n================================================================================\nInstalling:\n slurm-sql        x86_64        17.02.7-1.el6          local-slurm        306 k\n\nTransaction Summary\n================================================================================\nInstall       1 Package(s)\n\nTotal download size: 306 k\nInstalled size: 1.1 M\nDownloading Packages:\nRunning rpm_check_debug\nRunning Transaction Test\nTransaction Test Succeeded\nRunning Transaction\n\r  Installing : slurm-sql-17.02.7-1.el6.x86_64                               1/1 \n\r  Verifying  : slurm-sql-17.02.7-1.el6.x86_64                               1/1 \n\nInstalled:\n  slurm-sql.x86_64 0:17.02.7-1.el6                                              \n\nComplete!\n"]}

TASK [slurm-worker : Ensure SLURMd starts at boot] *****************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/main.yml:33
changed: [master001] => {"changed": true, "enabled": true, "name": "slurm"}
changed: [worker002] => {"changed": true, "enabled": true, "name": "slurm"}
changed: [worker003] => {"changed": true, "enabled": true, "name": "slurm"}
changed: [worker001] => {"changed": true, "enabled": true, "name": "slurm"}

PLAY [Setup OBS-initial mount directory and password file] *********************

TASK [setup] *******************************************************************
ok: [master001]
ok: [worker001]
ok: [worker003]
ok: [worker002]

TASK [Create the obs directory] ************************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/s3fs.yml:8
ok: [master001] => {"changed": false, "gid": 0, "group": "root", "mode": "0700", "owner": "root", "path": "/obs", "size": 0, "state": "directory", "uid": 0}
ok: [worker001] => {"changed": false, "gid": 0, "group": "root", "mode": "0700", "owner": "root", "path": "/obs", "size": 0, "state": "directory", "uid": 0}
ok: [worker002] => {"changed": false, "gid": 0, "group": "root", "mode": "0700", "owner": "root", "path": "/obs", "size": 0, "state": "directory", "uid": 0}
changed: [worker003] => {"changed": true, "gid": 0, "group": "root", "mode": "0755", "owner": "root", "path": "/obs", "size": 4096, "state": "directory", "uid": 0}

TASK [Create the obs password file] ********************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/s3fs.yml:12
changed: [master001] => {"changed": true, "dest": "/etc/passwd-obs", "gid": 0, "group": "root", "mode": "0600", "owner": "root", "size": 62, "state": "file", "uid": 0}
changed: [worker001] => {"changed": true, "dest": "/etc/passwd-obs", "gid": 0, "group": "root", "mode": "0600", "owner": "root", "size": 62, "state": "file", "uid": 0}
changed: [worker002] => {"changed": true, "dest": "/etc/passwd-obs", "gid": 0, "group": "root", "mode": "0600", "owner": "root", "size": 62, "state": "file", "uid": 0}
changed: [worker003] => {"changed": true, "dest": "/etc/passwd-obs", "gid": 0, "group": "root", "mode": "0600", "owner": "root", "size": 0, "state": "file", "uid": 0}

TASK [Setup OBS-insert into password file] *************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/s3fs.yml:17
changed: [master001] => {"changed": true, "cmd": "echo CNYVGDKK2PPBQEEMUYQG:6imAmmCVVzMqR1dm5x0AKJcL1BILA7TlOsSMhu4G > /etc/passwd-obs", "delta": "0:00:00.002760", "end": "2018-05-02 12:17:16.029870", "rc": 0, "start": "2018-05-02 12:17:16.027110", "stderr": "", "stdout": "", "stdout_lines": [], "warnings": []}
changed: [worker001] => {"changed": true, "cmd": "echo CNYVGDKK2PPBQEEMUYQG:6imAmmCVVzMqR1dm5x0AKJcL1BILA7TlOsSMhu4G > /etc/passwd-obs", "delta": "0:00:00.003355", "end": "2018-05-02 12:17:15.896613", "rc": 0, "start": "2018-05-02 12:17:15.893258", "stderr": "", "stdout": "", "stdout_lines": [], "warnings": []}
changed: [worker003] => {"changed": true, "cmd": "echo CNYVGDKK2PPBQEEMUYQG:6imAmmCVVzMqR1dm5x0AKJcL1BILA7TlOsSMhu4G > /etc/passwd-obs", "delta": "0:00:00.003250", "end": "2018-05-02 12:17:15.314298", "rc": 0, "start": "2018-05-02 12:17:15.311048", "stderr": "", "stdout": "", "stdout_lines": [], "warnings": []}
changed: [worker002] => {"changed": true, "cmd": "echo CNYVGDKK2PPBQEEMUYQG:6imAmmCVVzMqR1dm5x0AKJcL1BILA7TlOsSMhu4G > /etc/passwd-obs", "delta": "0:00:00.003852", "end": "2018-05-02 12:17:15.098264", "rc": 0, "start": "2018-05-02 12:17:15.094412", "stderr": "", "stdout": "", "stdout_lines": [], "warnings": []}

TASK [Setup OBS-umount obs] ****************************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/s3fs.yml:19
changed: [master001] => {"changed": true, "dump": "0", "fstab": "/etc/fstab", "fstype": "none", "name": "/obs", "opts": "defaults", "passno": "0", "src": "none"}
changed: [worker001] => {"changed": true, "dump": "0", "fstab": "/etc/fstab", "fstype": "none", "name": "/obs", "opts": "defaults", "passno": "0", "src": "none"}
ok: [worker003] => {"changed": false, "dump": "0", "fstab": "/etc/fstab", "fstype": "none", "name": "/obs", "opts": "defaults", "passno": "0", "src": "none"}
changed: [worker002] => {"changed": true, "dump": "0", "fstab": "/etc/fstab", "fstype": "none", "name": "/obs", "opts": "defaults", "passno": "0", "src": "none"}

TASK [Setup OBS-mount obs] *****************************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/s3fs.yml:25
changed: [master001] => {"changed": true, "cmd": "s3fs hwc-obs /obs -o url=http://obs.cn-north-1.myhwclouds.com -o endpoint=cn-north-1 -o passwd_file=/etc/passwd-obs", "delta": "0:00:00.007686", "end": "2018-05-02 12:17:16.711805", "rc": 0, "start": "2018-05-02 12:17:16.704119", "stderr": "", "stdout": "", "stdout_lines": [], "warnings": []}
changed: [worker001] => {"changed": true, "cmd": "s3fs hwc-obs /obs -o url=http://obs.cn-north-1.myhwclouds.com -o endpoint=cn-north-1 -o passwd_file=/etc/passwd-obs", "delta": "0:00:00.009349", "end": "2018-05-02 12:17:16.582449", "rc": 0, "start": "2018-05-02 12:17:16.573100", "stderr": "", "stdout": "", "stdout_lines": [], "warnings": []}
changed: [worker002] => {"changed": true, "cmd": "s3fs hwc-obs /obs -o url=http://obs.cn-north-1.myhwclouds.com -o endpoint=cn-north-1 -o passwd_file=/etc/passwd-obs", "delta": "0:00:00.009018", "end": "2018-05-02 12:17:15.788027", "rc": 0, "start": "2018-05-02 12:17:15.779009", "stderr": "", "stdout": "", "stdout_lines": [], "warnings": []}
changed: [worker003] => {"changed": true, "cmd": "s3fs hwc-obs /obs -o url=http://obs.cn-north-1.myhwclouds.com -o endpoint=cn-north-1 -o passwd_file=/etc/passwd-obs", "delta": "0:00:00.038290", "end": "2018-05-02 12:17:16.034923", "rc": 0, "start": "2018-05-02 12:17:15.996633", "stderr": "", "stdout": "", "stdout_lines": [], "warnings": []}

TASK [Remove item from /etc/fstaba] ********************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/s3fs.yml:27
changed: [master001] => {"backup": "", "changed": true, "found": 1, "msg": "1 line(s) removed"}
changed: [worker001] => {"backup": "", "changed": true, "found": 1, "msg": "1 line(s) removed"}
changed: [worker002] => {"backup": "", "changed": true, "found": 1, "msg": "1 line(s) removed"}
ok: [worker003] => {"backup": "", "changed": false, "found": 0, "msg": ""}

TASK [Add to /etc/fstab] *******************************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/s3fs.yml:33
changed: [master001] => {"changed": true, "cmd": "echo s3fs#hwc-obs /obs fuse _netdev,allow_other,use_path_request_style,url=http://obs.cn-north-1.myhwclouds.com,passwd_file=/etc/passwd-obs 0 0 >> /etc/fstab", "delta": "0:00:00.002721", "end": "2018-05-02 12:17:17.359837", "rc": 0, "start": "2018-05-02 12:17:17.357116", "stderr": "", "stdout": "", "stdout_lines": [], "warnings": []}
changed: [worker001] => {"changed": true, "cmd": "echo s3fs#hwc-obs /obs fuse _netdev,allow_other,use_path_request_style,url=http://obs.cn-north-1.myhwclouds.com,passwd_file=/etc/passwd-obs 0 0 >> /etc/fstab", "delta": "0:00:00.003407", "end": "2018-05-02 12:17:17.236439", "rc": 0, "start": "2018-05-02 12:17:17.233032", "stderr": "", "stdout": "", "stdout_lines": [], "warnings": []}
changed: [worker002] => {"changed": true, "cmd": "echo s3fs#hwc-obs /obs fuse _netdev,allow_other,use_path_request_style,url=http://obs.cn-north-1.myhwclouds.com,passwd_file=/etc/passwd-obs 0 0 >> /etc/fstab", "delta": "0:00:00.003577", "end": "2018-05-02 12:17:16.426147", "rc": 0, "start": "2018-05-02 12:17:16.422570", "stderr": "", "stdout": "", "stdout_lines": [], "warnings": []}
changed: [worker003] => {"changed": true, "cmd": "echo s3fs#hwc-obs /obs fuse _netdev,allow_other,use_path_request_style,url=http://obs.cn-north-1.myhwclouds.com,passwd_file=/etc/passwd-obs 0 0 >> /etc/fstab", "delta": "0:00:00.003283", "end": "2018-05-02 12:17:16.652369", "rc": 0, "start": "2018-05-02 12:17:16.649086", "stderr": "", "stdout": "", "stdout_lines": [], "warnings": []}

PLAY [Slurm master Playbook] ***************************************************

TASK [setup] *******************************************************************
ok: [master001]

TASK [nis : Load distribution-specific parameters] *****************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/main.yml:3
included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/init-RedHat.yml for master001

TASK [nis : Set NIS common playbook params (CentOS/RHEL)] **********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/init-RedHat.yml:3
ok: [master001] => {"ansible_facts": {"nis_client_packages": ["ypbind"], "nis_client_services": ["rpcbind", "ypbind"], "nis_common_packages": ["yp-tools"], "nis_master_packages": ["ypserv"], "nis_master_services": ["rpcbind", "yppasswdd", "ypserv"], "nis_securenets_path": "/var/yp/securenets"}, "changed": false}

TASK [nis : Pre-load debconf answer to questions (Debian/Ubuntu)] **************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/main.yml:7
skipping: [master001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [nis : Deploy additional NIS configuration (Debian/Ubuntu)] ***************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/main.yml:18
skipping: [master001] => (item=etc/default/nis)  => {"changed": false, "item": "etc/default/nis", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [master001] => (item=etc/defaultdomain)  => {"changed": false, "item": "etc/defaultdomain", "skip_reason": "Conditional check failed", "skipped": true}

TASK [nis : Set NIS domain (CentOS/RHEL)] **************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/main.yml:29
changed: [master001] => {"backup": "", "changed": true, "msg": "line added"}

TASK [nis : Install NIS common packages] ***************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/main.yml:39
ok: [master001] => (item=[u'yp-tools']) => {"changed": false, "item": ["yp-tools"], "msg": "", "rc": 0, "results": ["yp-tools-2.9-12.el6.x86_64 providing yp-tools is already installed"]}

TASK [nis : Deploy `ypserv` configuration files] *******************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml:3
ok: [master001] => {"changed": false, "gid": 0, "group": "root", "mode": "0400", "owner": "root", "path": "/var/yp/securenets", "size": 733, "state": "file", "uid": 0}

TASK [nis : Deploy `yppasswdd` configuration file (CentOS/RHEL)] ***************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml:12
ok: [master001] => {"changed": false, "checksum": "18392b4e3ecc4f9e7832c274663b5f6cee00d3f1", "dest": "/etc/sysconfig/yppasswdd", "gid": 0, "group": "root", "mode": "0444", "owner": "root", "path": "/etc/sysconfig/yppasswdd", "size": 681, "state": "file", "uid": 0}

TASK [nis : Install NIS master server packages] ********************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml:22
ok: [master001] => (item=[u'ypserv']) => {"changed": false, "item": ["ypserv"], "msg": "", "rc": 0, "results": ["ypserv-2.19-31.el6.x86_64 providing ypserv is already installed"]}

TASK [nis : Ensure `ypserv` starts at boot] ************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml:33
ok: [master001] => (item=rpcbind) => {"changed": false, "enabled": true, "item": "rpcbind", "name": "rpcbind", "state": "started"}
ok: [master001] => (item=yppasswdd) => {"changed": false, "enabled": true, "item": "yppasswdd", "name": "yppasswdd", "state": "started"}
ok: [master001] => (item=ypserv) => {"changed": false, "enabled": true, "item": "ypserv", "name": "ypserv", "state": "started"}

TASK [nis : Update NIS/YP databases (NIS master server)] ***********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml:41
changed: [master001] => {"changed": true, "cmd": ["make"], "delta": "0:00:00.026786", "end": "2018-05-02 12:17:20.315571", "rc": 0, "start": "2018-05-02 12:17:20.288785", "stderr": "", "stdout": "gmake[1]: Entering directory `/var/yp/elasticluster'\nUpdating netid.byname...\ngmake[1]: Leaving directory `/var/yp/elasticluster'", "stdout_lines": ["gmake[1]: Entering directory `/var/yp/elasticluster'", "Updating netid.byname...", "gmake[1]: Leaving directory `/var/yp/elasticluster'"], "warnings": []}

TASK [nis : Deploy `ypbind` configuration files] *******************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypbind.yml:3
skipping: [master001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [nis : Ensure NIS/YP is used as a name service] ***************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypbind.yml:12
skipping: [master001] => (item=passwd)  => {"changed": false, "item": "passwd", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [master001] => (item=group)  => {"changed": false, "item": "group", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [master001] => (item=shadow)  => {"changed": false, "item": "shadow", "skip_reason": "Conditional check failed", "skipped": true}

TASK [nis : Replace `compat` in `/etc/nsswitch.conf` with `files nis`] *********
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypbind.yml:26
skipping: [master001] => (item=passwd)  => {"changed": false, "item": "passwd", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [master001] => (item=group)  => {"changed": false, "item": "group", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [master001] => (item=shadow)  => {"changed": false, "item": "shadow", "skip_reason": "Conditional check failed", "skipped": true}

TASK [nis : Ensure `ypbind` starts at boot] ************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypbind.yml:37
skipping: [master001] => (item=rpcbind)  => {"changed": false, "item": "rpcbind", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [master001] => (item=ypbind)  => {"changed": false, "item": "ypbind", "skip_reason": "Conditional check failed", "skipped": true}

TASK [nfs-server : Load distribution-specific parameters] **********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-server/tasks/main.yml:6
included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-server/tasks/init-RedHat.yml for master001

TASK [nfs-server : Set NFS server variables (RHEL/CentOS 7.x)] *****************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-server/tasks/init-RedHat.yml:6
skipping: [master001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [nfs-server : Set NFS server variables (RHEL/CentOS 6.x)] *****************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-server/tasks/init-RedHat.yml:20
ok: [master001] => {"ansible_facts": {"_nfs_server_started_state": "started", "nfs_server_packages": ["nfs-utils"], "nfs_server_services": ["rpcbind", "nfslock", "nfs"]}, "changed": false}

TASK [nfs-server : install NFS server software] ********************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-server/tasks/main.yml:12
ok: [master001] => (item=[u'nfs-utils']) => {"changed": false, "item": ["nfs-utils"], "msg": "", "rc": 0, "results": ["nfs-utils-1:1.2.3-75.el6_9.x86_64 providing nfs-utils is already installed"]}

TASK [nfs-server : Export directories] *****************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-server/tasks/main.yml:25
changed: [master001] => (item={u'path': u'/home', u'clients': [u'worker001', u'worker002', u'worker003']}) => {"changed": true, "item": {"clients": ["worker001", "worker002", "worker003"], "path": "/home"}}

TASK [nfs-server : Ensure portmapper is running] *******************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-server/tasks/main.yml:38
ok: [master001] => {"changed": false, "enabled": true, "name": "rpcbind", "state": "started"}

TASK [nfs-server : Ensure NFS server is running (Debian 8 "jessie")] ***********
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-server/tasks/main.yml:48
skipping: [master001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [nfs-server : Ensure NFS server is running] *******************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-server/tasks/main.yml:59
ok: [master001] => (item=rpcbind) => {"changed": false, "enabled": true, "item": "rpcbind", "name": "rpcbind", "state": "started"}
ok: [master001] => (item=nfslock) => {"changed": false, "enabled": true, "item": "nfslock", "name": "nfslock", "state": "started"}
ok: [master001] => (item=nfs) => {"changed": false, "enabled": true, "item": "nfs", "name": "nfs", "state": "started"}

TASK [nfs-server : Reload NFS exports file] ************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-server/tasks/main.yml:71
changed: [master001] => {"changed": true, "cmd": ["exportfs", "-r"], "delta": "0:00:00.002874", "end": "2018-05-02 12:17:23.635099", "rc": 0, "start": "2018-05-02 12:17:23.632225", "stderr": "", "stdout": "", "stdout_lines": [], "warnings": []}

TASK [nfs-server : Restart NFS server services] ********************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-server/tasks/main.yml:79
changed: [master001] => {"changed": true, "name": "nfs", "state": "started"}

TASK [slurm-common : Load distribution-specific parameters] ********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:3
included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/init-RedHat.yml for master001

TASK [slurm-common : Set SLURM common playbook params (RHEL 7.x compatible)] ***
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/init-RedHat.yml:3
ok: [master001] => {"ansible_facts": {"slurm_pid_dir": "/var/run"}, "changed": false}

TASK [slurm-common : Create `slurm` system group] ******************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:16
ok: [master001] => {"changed": false, "gid": 498, "name": "slurm", "state": "present", "system": true}

TASK [slurm-common : Create `slurm` system user] *******************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:24
ok: [master001] => {"append": false, "changed": false, "comment": "", "group": 498, "home": "/home/slurm", "move_home": false, "name": "slurm", "shell": "/bin/bash", "state": "present", "uid": 498}

TASK [slurm-common : Create work directory {{item}}] ***************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:33
ok: [master001] => (item=/etc/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/etc/slurm", "mode": "0755", "owner": "slurm", "path": "/etc/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [master001] => (item=/var/lib/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/lib/slurm", "mode": "0755", "owner": "slurm", "path": "/var/lib/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [master001] => (item=/var/lib/slurm/checkpoint) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/lib/slurm/checkpoint", "mode": "0755", "owner": "slurm", "path": "/var/lib/slurm/checkpoint", "size": 4096, "state": "directory", "uid": 498}
ok: [master001] => (item=/var/lib/slurm/slurmctld) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/lib/slurm/slurmctld", "mode": "0755", "owner": "slurm", "path": "/var/lib/slurm/slurmctld", "size": 4096, "state": "directory", "uid": 498}
ok: [master001] => (item=/var/lib/slurm/slurmd) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/lib/slurm/slurmd", "mode": "0755", "owner": "slurm", "path": "/var/lib/slurm/slurmd", "size": 4096, "state": "directory", "uid": 498}
ok: [master001] => (item=/var/log/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/log/slurm", "mode": "0755", "owner": "slurm", "path": "/var/log/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [master001] => (item=/var/run/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/run/slurm", "mode": "0755", "owner": "slurm", "path": "/var/run/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [master001] => (item=/var/spool/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/spool/slurm", "mode": "0755", "owner": "slurm", "path": "/var/spool/slurm", "size": 4096, "state": "directory", "uid": 498}

TASK [slurm-common : Make compatibility symlinks] ******************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:56
skipping: [master001] => (item={u'to': u'/var/lib/slurm-llnl', u'from': u'/var/lib/slurm'})  => {"changed": false, "item": {"from": "/var/lib/slurm", "to": "/var/lib/slurm-llnl"}, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [master001] => (item={u'to': u'/etc/slurm-llnl', u'from': u'/etc/slurm'})  => {"changed": false, "item": {"from": "/etc/slurm", "to": "/etc/slurm-llnl"}, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [master001] => (item={u'to': u'/var/log/slurm-llnl', u'from': u'/var/log/slurm'})  => {"changed": false, "item": {"from": "/var/log/slurm", "to": "/var/log/slurm-llnl"}, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [master001] => (item={u'to': u'/var/run/slurm-llnl', u'from': u'/var/run/slurm'})  => {"changed": false, "item": {"from": "/var/run/slurm", "to": "/var/run/slurm-llnl"}, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-common : Enable Copr SLURM repo by verdurin] ***********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:73
ok: [master001] => {"changed": false, "gid": 0, "group": "root", "mode": "0444", "owner": "root", "path": "/etc/yum.repos.d/copr-slurm.repo", "size": 615, "state": "file", "uid": 0}

TASK [slurm-common : Prevent MUNGE from starting] ******************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:20
skipping: [master001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-common : Install MUNGE (Debian/Ubuntu)] ****************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:31
skipping: [master001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-common : Install MUNGE (RHEL-compatible)] **************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:43
ok: [master001] => {"changed": false, "msg": "", "rc": 0, "results": ["slurm-munge-17.02.7-1.el6.x86_64 providing slurm-munge is already installed"]}

TASK [slurm-common : Allow MUNGE to start] *************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:57
skipping: [master001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-common : Add `--syslog` option to `munged` startup] ****************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:66
skipping: [master001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-common : Reload systemd configuration] *****************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:77
skipping: [master001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-common : Add `--syslog` startup options for `munged` (Ubuntu 14.04)] ***
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:86
skipping: [master001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-common : Configure MUNGE] ******************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:101
ok: [master001] => {"changed": false, "checksum": "222afe887f907039b799b394715664ffa584fec5", "dest": "/etc/munge/munge.key", "gid": 497, "group": "munge", "mode": "0400", "owner": "munge", "path": "/etc/munge/munge.key", "size": 1024, "state": "file", "uid": 497}

TASK [slurm-common : Ensure the MUNGE service is running] **********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:114
ok: [master001] => {"changed": false, "enabled": true, "name": "munge", "state": "started"}

TASK [slurm-common : Deploy SLURM configuration file] **************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:86
ok: [master001] => {"changed": false, "gid": 0, "group": "root", "mode": "0444", "owner": "root", "path": "/etc/slurm/slurm.conf", "size": 4244, "state": "file", "uid": 0}

TASK [slurm-common : Deploy script to suspend cloud instance] ******************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:96
ok: [master001] => {"changed": false, "gid": 0, "group": "root", "mode": "0555", "owner": "root", "path": "/etc/slurm/slurm.suspend.sh", "size": 604, "state": "file", "uid": 0}

TASK [slurm-common : Deploy script to resume cloud instance] *******************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:106
ok: [master001] => {"changed": false, "gid": 0, "group": "root", "mode": "0555", "owner": "root", "path": "/etc/slurm/slurm.resume.sh", "size": 631, "state": "file", "uid": 0}

TASK [slurm-common : Install support packages (Debian/Ubuntu)] *****************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:116
skipping: [master001] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-common : Install SLURM packages (RHEL-compatible)] *****************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:127
skipping: [master001] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-client : Install required SLURM packages (Debian/Ubuntu)] **********
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-client/tasks/main.yml:4
skipping: [master001] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-client : Install required SLURM packages (older Debian/Ubuntu)] ****
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-client/tasks/main.yml:23
skipping: [master001] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-client : service] **************************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-client/tasks/main.yml:35
skipping: [master001] => (item=slurm-llnl)  => {"changed": false, "item": "slurm-llnl", "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-client : Install SLURM packages (RHEL-compatible)] *****************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-client/tasks/main.yml:47
ok: [master001] => (item=[u'slurm', u'slurm-devel', u'slurm-perlapi', u'slurm-contribs', u'slurm-torque']) => {"changed": false, "item": ["slurm", "slurm-devel", "slurm-perlapi", "slurm-contribs", "slurm-torque"], "msg": "", "rc": 0, "results": ["slurm-17.02.7-1.el6.x86_64 providing slurm is already installed", "slurm-devel-17.02.7-1.el6.x86_64 providing slurm-devel is already installed", "slurm-perlapi-17.02.7-1.el6.x86_64 providing slurm-perlapi is already installed", "slurm-contribs-17.02.7-1.el6.x86_64 providing slurm-contribs is already installed", "slurm-torque-17.02.7-1.el6.x86_64 providing slurm-torque is already installed"]}

TASK [slurm-client : Stop SLURM services (RHEL/CentOS 6.x)] ********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-client/tasks/main.yml:59
changed: [master001] => {"changed": true, "enabled": false, "name": "slurm", "state": "stopped"}

TASK [slurm-client : Stop SLURM services (RHEL/CentOS 7.x)] ********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-client/tasks/main.yml:66
skipping: [master001] => (item=slurmd)  => {"changed": false, "item": "slurmd", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [master001] => (item=slurmctld)  => {"changed": false, "item": "slurmctld", "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-master : Load distribution-specific parameters] ********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-master/tasks/main.yml:4
included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-master/tasks/init-RedHat.yml for master001

TASK [slurm-master : Set SLURM master playbook params (RHEL 7.x compatible)] ***
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-master/tasks/init-RedHat.yml:3
skipping: [master001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-master : Set SLURM playbook params (RHEL 6.x compatible)] **********
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-master/tasks/init-RedHat.yml:18
ok: [master001] => {"ansible_facts": {"slurmctld_packages": ["mailx", "slurm-plugins", "slurm"], "slurmctld_service_name": "slurm", "slurmdbd_packages": ["slurm-sql", "slurm-slurmdbd"], "slurmdbd_service_name": "slurmdbd"}, "changed": false}

TASK [slurm-master : Set variables (Debian/Ubuntu)] ****************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-master/tasks/db.yml:3
skipping: [master001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-master : Set variables (older Debian/Ubuntu)] **********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-master/tasks/db.yml:11
skipping: [master001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-master : Set variables (RHEL 7.x)] *********************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-master/tasks/db.yml:19
skipping: [master001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-master : Set variables (RHEL 6.x)] *********************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-master/tasks/db.yml:27
ok: [master001] => {"ansible_facts": {"slurm_db_python_pkg": "MySQL-python", "slurm_db_server_name": "MySQL", "slurm_db_server_pkg": "mysql-server", "slurm_db_service_name": "mysqld"}, "changed": false}

TASK [slurm-master : Install MySQL, used for SLURM accounting] *****************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-master/tasks/db.yml:36
ok: [master001] => (item=[u'mysql-server', u'MySQL-python']) => {"changed": false, "item": ["mysql-server", "MySQL-python"], "msg": "", "rc": 0, "results": ["mysql-server-5.1.73-8.el6_8.x86_64 providing mysql-server is already installed", "MySQL-python-1.2.3-0.3.c1.1.el6.x86_64 providing MySQL-python is already installed"]}

TASK [slurm-master : Ensure MySQL daemon is up] ********************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-master/tasks/db.yml:48
ok: [master001] => {"changed": false, "enabled": true, "name": "mysqld", "state": "started"}

TASK [slurm-master : Create DB for SLURMDBD] ***********************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-master/tasks/db.yml:58
ok: [master001] => {"changed": false, "db": "slurm"}

TASK [slurm-master : Create DB user for SLURMDBD] ******************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-master/tasks/db.yml:68
ok: [master001] => (item=master001) => {"changed": false, "item": "master001", "user": "slurm"}
ok: [master001] => (item=localhost) => {"changed": false, "item": "localhost", "user": "slurm"}

TASK [slurm-master : Deploy SLURMDBD configuration] ****************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-master/tasks/install-slurmdbd.yml:5
ok: [master001] => {"changed": false, "gid": 0, "group": "root", "mode": "0444", "owner": "root", "path": "/etc/slurm/slurmdbd.conf", "size": 2862, "state": "file", "uid": 0}

TASK [slurm-master : Install SLURM DBD packages] *******************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-master/tasks/install-slurmdbd.yml:19
ok: [master001] => (item=[u'slurm-sql', u'slurm-slurmdbd']) => {"changed": false, "item": ["slurm-sql", "slurm-slurmdbd"], "msg": "", "rc": 0, "results": ["slurm-sql-17.02.7-1.el6.x86_64 providing slurm-sql is already installed", "slurm-slurmdbd-17.02.7-1.el6.x86_64 providing slurm-slurmdbd is already installed"]}

TASK [slurm-master : Ensure `slurmdbd` is running] *****************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-master/tasks/install-slurmdbd.yml:26
ok: [master001] => (item=slurmdbd) => {"changed": false, "enabled": true, "item": "slurmdbd", "name": "slurmdbd", "state": "started"}

TASK [slurm-master : Install SLURM master packages] ****************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-master/tasks/install-slurmctld.yml:3
ok: [master001] => (item=[u'mailx', u'slurm-plugins', u'slurm']) => {"changed": false, "item": ["mailx", "slurm-plugins", "slurm"], "msg": "", "rc": 0, "results": ["mailx-12.4-8.el6_6.x86_64 providing mailx is already installed", "slurm-plugins-17.02.7-1.el6.x86_64 providing slurm-plugins is already installed", "slurm-17.02.7-1.el6.x86_64 providing slurm is already installed"]}

TASK [slurm-master : Create cluster in accounting database] ********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-master/tasks/main.yml:13
changed: [master001] => {"changed": true, "cmd": "sacctmgr --parsable --noheader list cluster | grep '^elasticluster|' || sacctmgr -i -Q add cluster elasticluster", "delta": "0:00:00.326278", "end": "2018-05-02 12:17:32.963666", "rc": 0, "start": "2018-05-02 12:17:32.637388", "stderr": "", "stdout": "elasticluster||0|7936|1||||||||normal||", "stdout_lines": ["elasticluster||0|7936|1||||||||normal||"], "warnings": []}

TASK [slurm-master : Create an account for default cluster] ********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-master/tasks/main.yml:20
changed: [master001] => {"changed": true, "cmd": "sacctmgr --immediate --parsable --noheader list account Cluster=elasticluster | grep '^root|' || sacctmgr -i --quiet add account root Cluster=elasticluster", "delta": "0:00:00.166450", "end": "2018-05-02 12:17:33.265493", "rc": 0, "start": "2018-05-02 12:17:33.099043", "stderr": "", "stdout": "root|default root account|root|", "stdout_lines": ["root|default root account|root|"], "warnings": []}

TASK [slurm-master : Add default user to cluster] ******************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-master/tasks/main.yml:27
changed: [master001] => (item=root) => {"changed": true, "cmd": "sacctmgr --immediate --parsable --noheader list user Account=root | grep '^root|' || sacctmgr --immediate --quiet add user 'root' DefaultAccount=root", "delta": "0:00:00.167995", "end": "2018-05-02 12:17:33.575633", "item": "root", "rc": 0, "start": "2018-05-02 12:17:33.407638", "stderr": "", "stdout": "root|root|Administrator|", "stdout_lines": ["root|root|Administrator|"], "warnings": []}

TASK [slurm-master : Ensure `slurmctld` is running] ****************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-master/tasks/main.yml:35
changed: [master001] => (item=slurm) => {"changed": true, "enabled": true, "item": "slurm", "name": "slurm", "state": "started"}

PLAY [Slurm worker nodes Playbook] *********************************************

TASK [setup] *******************************************************************
ok: [worker001]
ok: [worker002]
ok: [worker003]

TASK [nis : Load distribution-specific parameters] *****************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/main.yml:3
included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/init-RedHat.yml for worker001, worker002, worker003

TASK [nis : Set NIS common playbook params (CentOS/RHEL)] **********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/init-RedHat.yml:3
ok: [worker001] => {"ansible_facts": {"nis_client_packages": ["ypbind"], "nis_client_services": ["rpcbind", "ypbind"], "nis_common_packages": ["yp-tools"], "nis_master_packages": ["ypserv"], "nis_master_services": ["rpcbind", "yppasswdd", "ypserv"], "nis_securenets_path": "/var/yp/securenets"}, "changed": false}
ok: [worker002] => {"ansible_facts": {"nis_client_packages": ["ypbind"], "nis_client_services": ["rpcbind", "ypbind"], "nis_common_packages": ["yp-tools"], "nis_master_packages": ["ypserv"], "nis_master_services": ["rpcbind", "yppasswdd", "ypserv"], "nis_securenets_path": "/var/yp/securenets"}, "changed": false}
ok: [worker003] => {"ansible_facts": {"nis_client_packages": ["ypbind"], "nis_client_services": ["rpcbind", "ypbind"], "nis_common_packages": ["yp-tools"], "nis_master_packages": ["ypserv"], "nis_master_services": ["rpcbind", "yppasswdd", "ypserv"], "nis_securenets_path": "/var/yp/securenets"}, "changed": false}

TASK [nis : Pre-load debconf answer to questions (Debian/Ubuntu)] **************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/main.yml:7
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [nis : Deploy additional NIS configuration (Debian/Ubuntu)] ***************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/main.yml:18
skipping: [worker001] => (item=etc/default/nis)  => {"changed": false, "item": "etc/default/nis", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => (item=etc/defaultdomain)  => {"changed": false, "item": "etc/defaultdomain", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item=etc/default/nis)  => {"changed": false, "item": "etc/default/nis", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item=etc/defaultdomain)  => {"changed": false, "item": "etc/defaultdomain", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => (item=etc/default/nis)  => {"changed": false, "item": "etc/default/nis", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => (item=etc/defaultdomain)  => {"changed": false, "item": "etc/defaultdomain", "skip_reason": "Conditional check failed", "skipped": true}

TASK [nis : Set NIS domain (CentOS/RHEL)] **************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/main.yml:29
changed: [worker001] => {"backup": "", "changed": true, "msg": "line added"}
changed: [worker002] => {"backup": "", "changed": true, "msg": "line added"}
changed: [worker003] => {"backup": "", "changed": true, "msg": "line added"}

TASK [nis : Install NIS common packages] ***************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/main.yml:39
ok: [worker001] => (item=[u'yp-tools']) => {"changed": false, "item": ["yp-tools"], "msg": "", "rc": 0, "results": ["yp-tools-2.9-12.el6.x86_64 providing yp-tools is already installed"]}
ok: [worker002] => (item=[u'yp-tools']) => {"changed": false, "item": ["yp-tools"], "msg": "", "rc": 0, "results": ["yp-tools-2.9-12.el6.x86_64 providing yp-tools is already installed"]}
ok: [worker003] => (item=[u'yp-tools']) => {"changed": false, "item": ["yp-tools"], "msg": "", "rc": 0, "results": ["yp-tools-2.9-12.el6.x86_64 providing yp-tools is already installed"]}

TASK [nis : Deploy `ypserv` configuration files] *******************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml:3
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [nis : Deploy `yppasswdd` configuration file (CentOS/RHEL)] ***************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml:12
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [nis : Install NIS master server packages] ********************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml:22
skipping: [worker001] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}

TASK [nis : Ensure `ypserv` starts at boot] ************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml:33
skipping: [worker001] => (item=rpcbind)  => {"changed": false, "item": "rpcbind", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item=rpcbind)  => {"changed": false, "item": "rpcbind", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => (item=yppasswdd)  => {"changed": false, "item": "yppasswdd", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item=yppasswdd)  => {"changed": false, "item": "yppasswdd", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => (item=rpcbind)  => {"changed": false, "item": "rpcbind", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => (item=ypserv)  => {"changed": false, "item": "ypserv", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => (item=yppasswdd)  => {"changed": false, "item": "yppasswdd", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item=ypserv)  => {"changed": false, "item": "ypserv", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => (item=ypserv)  => {"changed": false, "item": "ypserv", "skip_reason": "Conditional check failed", "skipped": true}

TASK [nis : Update NIS/YP databases (NIS master server)] ***********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypserv.yml:41
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [nis : Deploy `ypbind` configuration files] *******************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypbind.yml:3
ok: [worker001] => {"changed": false, "gid": 0, "group": "root", "mode": "0400", "owner": "root", "path": "/etc/yp.conf", "size": 723, "state": "file", "uid": 0}
ok: [worker002] => {"changed": false, "gid": 0, "group": "root", "mode": "0400", "owner": "root", "path": "/etc/yp.conf", "size": 723, "state": "file", "uid": 0}
ok: [worker003] => {"changed": false, "gid": 0, "group": "root", "mode": "0400", "owner": "root", "path": "/etc/yp.conf", "size": 723, "state": "file", "uid": 0}

TASK [nis : Ensure NIS/YP is used as a name service] ***************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypbind.yml:12
ok: [worker001] => (item=passwd) => {"backup": "", "changed": false, "item": "passwd", "msg": ""}
ok: [worker002] => (item=passwd) => {"backup": "", "changed": false, "item": "passwd", "msg": ""}
ok: [worker003] => (item=passwd) => {"backup": "", "changed": false, "item": "passwd", "msg": ""}
ok: [worker001] => (item=group) => {"backup": "", "changed": false, "item": "group", "msg": ""}
ok: [worker002] => (item=group) => {"backup": "", "changed": false, "item": "group", "msg": ""}
ok: [worker003] => (item=group) => {"backup": "", "changed": false, "item": "group", "msg": ""}
ok: [worker001] => (item=shadow) => {"backup": "", "changed": false, "item": "shadow", "msg": ""}
ok: [worker002] => (item=shadow) => {"backup": "", "changed": false, "item": "shadow", "msg": ""}
ok: [worker003] => (item=shadow) => {"backup": "", "changed": false, "item": "shadow", "msg": ""}

TASK [nis : Replace `compat` in `/etc/nsswitch.conf` with `files nis`] *********
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypbind.yml:26
ok: [worker001] => (item=passwd) => {"changed": false, "item": "passwd", "msg": ""}
ok: [worker002] => (item=passwd) => {"changed": false, "item": "passwd", "msg": ""}
ok: [worker003] => (item=passwd) => {"changed": false, "item": "passwd", "msg": ""}
ok: [worker001] => (item=group) => {"changed": false, "item": "group", "msg": ""}
ok: [worker003] => (item=group) => {"changed": false, "item": "group", "msg": ""}
ok: [worker002] => (item=group) => {"changed": false, "item": "group", "msg": ""}
ok: [worker001] => (item=shadow) => {"changed": false, "item": "shadow", "msg": ""}
ok: [worker003] => (item=shadow) => {"changed": false, "item": "shadow", "msg": ""}
ok: [worker002] => (item=shadow) => {"changed": false, "item": "shadow", "msg": ""}

TASK [nis : Ensure `ypbind` starts at boot] ************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nis/tasks/ypbind.yml:37
ok: [worker001] => (item=rpcbind) => {"changed": false, "enabled": true, "item": "rpcbind", "name": "rpcbind", "state": "started"}
ok: [worker002] => (item=rpcbind) => {"changed": false, "enabled": true, "item": "rpcbind", "name": "rpcbind", "state": "started"}
ok: [worker003] => (item=rpcbind) => {"changed": false, "enabled": true, "item": "rpcbind", "name": "rpcbind", "state": "started"}
ok: [worker001] => (item=ypbind) => {"changed": false, "enabled": true, "item": "ypbind", "name": "ypbind", "state": "started"}
ok: [worker003] => (item=ypbind) => {"changed": false, "enabled": true, "item": "ypbind", "name": "ypbind", "state": "started"}
ok: [worker002] => (item=ypbind) => {"changed": false, "enabled": true, "item": "ypbind", "name": "ypbind", "state": "started"}

TASK [nfs-client : install NFS client software (Debian/Ubuntu)] ****************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-client/tasks/main.yml:3
skipping: [worker001] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}

TASK [nfs-client : install NFS client software (RHEL-compatible)] **************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-client/tasks/main.yml:15
ok: [worker001] => (item=[u'nfs-utils', u'nfs4-acl-tools']) => {"changed": false, "item": ["nfs-utils", "nfs4-acl-tools"], "msg": "", "rc": 0, "results": ["nfs-utils-1:1.2.3-75.el6_9.x86_64 providing nfs-utils is already installed", "nfs4-acl-tools-0.3.3-8.el6.x86_64 providing nfs4-acl-tools is already installed"]}
ok: [worker002] => (item=[u'nfs-utils', u'nfs4-acl-tools']) => {"changed": false, "item": ["nfs-utils", "nfs4-acl-tools"], "msg": "", "rc": 0, "results": ["nfs-utils-1:1.2.3-75.el6_9.x86_64 providing nfs-utils is already installed", "nfs4-acl-tools-0.3.3-8.el6.x86_64 providing nfs4-acl-tools is already installed"]}
ok: [worker003] => (item=[u'nfs-utils', u'nfs4-acl-tools']) => {"changed": false, "item": ["nfs-utils", "nfs4-acl-tools"], "msg": "", "rc": 0, "results": ["nfs-utils-1:1.2.3-75.el6_9.x86_64 providing nfs-utils is already installed", "nfs4-acl-tools-0.3.3-8.el6.x86_64 providing nfs4-acl-tools is already installed"]}

TASK [nfs-client : Ensure `rpcbind` is running (Debian)] ***********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-client/tasks/main.yml:28
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [nfs-client : Ensure `rpcbind` is running (RHEL-compatible)] **************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-client/tasks/main.yml:35
ok: [worker001] => {"changed": false, "enabled": true, "name": "rpcbind", "state": "started"}
ok: [worker002] => {"changed": false, "enabled": true, "name": "rpcbind", "state": "started"}
ok: [worker003] => {"changed": false, "enabled": true, "name": "rpcbind", "state": "started"}

TASK [nfs-client : Ensure `portmap` is running (Ubuntu prior to 14.04)] ********
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-client/tasks/main.yml:42
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [nfs-client : Ensure `rpcbind` is running (Ubuntu 14.04 or newer)] ********
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-client/tasks/main.yml:49
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [nfs-client : Mount NFS filesystems] **************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-client/tasks/main.yml:57
included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-client/tasks/nfsmount.yml for worker001, worker002, worker003

TASK [nfs-client : ensure /home directory exists] ******************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-client/tasks/nfsmount.yml:3
ok: [worker001] => {"changed": false, "gid": 0, "group": "root", "mode": "0755", "owner": "root", "path": "/home", "size": 4096, "state": "directory", "uid": 0}
ok: [worker002] => {"changed": false, "gid": 0, "group": "root", "mode": "0755", "owner": "root", "path": "/home", "size": 4096, "state": "directory", "uid": 0}
ok: [worker003] => {"changed": false, "gid": 0, "group": "root", "mode": "0755", "owner": "root", "path": "/home", "size": 4096, "state": "directory", "uid": 0}

TASK [nfs-client : add to /etc/fstab] ******************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/nfs-client/tasks/nfsmount.yml:8
ok: [worker001] => {"changed": false, "dump": "0", "fstab": "/etc/fstab", "fstype": "nfs", "name": "/home", "opts": "rw,async", "passno": "0", "src": "master001:/home"}
ok: [worker002] => {"changed": false, "dump": "0", "fstab": "/etc/fstab", "fstype": "nfs", "name": "/home", "opts": "rw,async", "passno": "0", "src": "master001:/home"}
changed: [worker003] => {"changed": true, "dump": "0", "fstab": "/etc/fstab", "fstype": "nfs", "name": "/home", "opts": "rw,async", "passno": "0", "src": "master001:/home"}

TASK [autofs : Load distribution-specific parameters] **************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/autofs/tasks/main.yml:8
included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/autofs/tasks/init-RedHat.yml for worker001, worker002, worker003

TASK [autofs : Provide RHEL-specific values] ***********************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/autofs/tasks/init-RedHat.yml:6
ok: [worker001] => {"ansible_facts": {"autofs_packages": ["autofs"], "autofs_service": "autofs", "autofs_username": "root"}, "changed": false}
ok: [worker002] => {"ansible_facts": {"autofs_packages": ["autofs"], "autofs_service": "autofs", "autofs_username": "root"}, "changed": false}
ok: [worker003] => {"ansible_facts": {"autofs_packages": ["autofs"], "autofs_service": "autofs", "autofs_username": "root"}, "changed": false}

TASK [autofs : Deploy autofs configuration] ************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/autofs/tasks/main.yml:12
ok: [worker001] => (item=etc/auto.master) => {"changed": false, "gid": 0, "group": "root", "item": "etc/auto.master", "mode": "0444", "owner": "root", "path": "/etc/auto.master", "size": 540, "state": "file", "uid": 0}
ok: [worker003] => (item=etc/auto.master) => {"changed": false, "gid": 0, "group": "root", "item": "etc/auto.master", "mode": "0444", "owner": "root", "path": "/etc/auto.master", "size": 540, "state": "file", "uid": 0}
ok: [worker002] => (item=etc/auto.master) => {"changed": false, "gid": 0, "group": "root", "item": "etc/auto.master", "mode": "0444", "owner": "root", "path": "/etc/auto.master", "size": 540, "state": "file", "uid": 0}
ok: [worker001] => (item=etc/auto.home) => {"changed": false, "gid": 0, "group": "root", "item": "etc/auto.home", "mode": "0444", "owner": "root", "path": "/etc/auto.home", "size": 852, "state": "file", "uid": 0}
ok: [worker003] => (item=etc/auto.home) => {"changed": false, "gid": 0, "group": "root", "item": "etc/auto.home", "mode": "0444", "owner": "root", "path": "/etc/auto.home", "size": 852, "state": "file", "uid": 0}
ok: [worker002] => (item=etc/auto.home) => {"changed": false, "gid": 0, "group": "root", "item": "etc/auto.home", "mode": "0444", "owner": "root", "path": "/etc/auto.home", "size": 852, "state": "file", "uid": 0}

TASK [autofs : Deploy autofs mount script for NFSv4 (Debian/Ubuntu)] ***********
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/autofs/tasks/main.yml:24
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [autofs : Install Autofs] *************************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/autofs/tasks/main.yml:34
ok: [worker002] => (item=[u'autofs']) => {"changed": false, "item": ["autofs"], "msg": "", "rc": 0, "results": ["autofs-1:5.0.5-133.el6_9.x86_64 providing autofs is already installed"]}
ok: [worker001] => (item=[u'autofs']) => {"changed": false, "item": ["autofs"], "msg": "", "rc": 0, "results": ["autofs-1:5.0.5-133.el6_9.x86_64 providing autofs is already installed"]}
ok: [worker003] => (item=[u'autofs']) => {"changed": false, "item": ["autofs"], "msg": "", "rc": 0, "results": ["autofs-1:5.0.5-133.el6_9.x86_64 providing autofs is already installed"]}

TASK [autofs : Ensure autofs is running and starts at boot] ********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/autofs/tasks/main.yml:41
ok: [worker002] => {"changed": false, "enabled": true, "name": "autofs", "state": "started"}
ok: [worker003] => {"changed": false, "enabled": true, "name": "autofs", "state": "started"}
ok: [worker001] => {"changed": false, "enabled": true, "name": "autofs", "state": "started"}

TASK [slurm-common : Load distribution-specific parameters] ********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:3
included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/init-RedHat.yml for worker001, worker002, worker003

TASK [slurm-common : Set SLURM common playbook params (RHEL 7.x compatible)] ***
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/init-RedHat.yml:3
ok: [worker001] => {"ansible_facts": {"slurm_pid_dir": "/var/run"}, "changed": false}
ok: [worker002] => {"ansible_facts": {"slurm_pid_dir": "/var/run"}, "changed": false}
ok: [worker003] => {"ansible_facts": {"slurm_pid_dir": "/var/run"}, "changed": false}

TASK [slurm-common : Create `slurm` system group] ******************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:16
ok: [worker002] => {"changed": false, "gid": 498, "name": "slurm", "state": "present", "system": true}
ok: [worker001] => {"changed": false, "gid": 498, "name": "slurm", "state": "present", "system": true}
ok: [worker003] => {"changed": false, "gid": 498, "name": "slurm", "state": "present", "system": true}

TASK [slurm-common : Create `slurm` system user] *******************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:24
ok: [worker001] => {"append": false, "changed": false, "comment": "", "group": 498, "home": "/home/slurm", "move_home": false, "name": "slurm", "shell": "/bin/bash", "state": "present", "uid": 498}
ok: [worker002] => {"append": false, "changed": false, "comment": "", "group": 498, "home": "/home/slurm", "move_home": false, "name": "slurm", "shell": "/bin/bash", "state": "present", "uid": 498}
ok: [worker003] => {"append": false, "changed": false, "comment": "", "group": 498, "home": "/home/slurm", "move_home": false, "name": "slurm", "shell": "/bin/bash", "state": "present", "uid": 498}

TASK [slurm-common : Create work directory {{item}}] ***************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:33
ok: [worker001] => (item=/etc/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/etc/slurm", "mode": "0755", "owner": "slurm", "path": "/etc/slurm", "size": 4096, "state": "directory", "uid": 498}
changed: [worker003] => (item=/etc/slurm) => {"changed": true, "gid": 498, "group": "slurm", "item": "/etc/slurm", "mode": "0755", "owner": "slurm", "path": "/etc/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [worker002] => (item=/etc/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/etc/slurm", "mode": "0755", "owner": "slurm", "path": "/etc/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [worker001] => (item=/var/lib/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/lib/slurm", "mode": "0755", "owner": "slurm", "path": "/var/lib/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [worker003] => (item=/var/lib/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/lib/slurm", "mode": "0755", "owner": "slurm", "path": "/var/lib/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [worker002] => (item=/var/lib/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/lib/slurm", "mode": "0755", "owner": "slurm", "path": "/var/lib/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [worker001] => (item=/var/lib/slurm/checkpoint) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/lib/slurm/checkpoint", "mode": "0755", "owner": "slurm", "path": "/var/lib/slurm/checkpoint", "size": 4096, "state": "directory", "uid": 498}
ok: [worker003] => (item=/var/lib/slurm/checkpoint) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/lib/slurm/checkpoint", "mode": "0755", "owner": "slurm", "path": "/var/lib/slurm/checkpoint", "size": 4096, "state": "directory", "uid": 498}
ok: [worker002] => (item=/var/lib/slurm/checkpoint) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/lib/slurm/checkpoint", "mode": "0755", "owner": "slurm", "path": "/var/lib/slurm/checkpoint", "size": 4096, "state": "directory", "uid": 498}
ok: [worker001] => (item=/var/lib/slurm/slurmctld) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/lib/slurm/slurmctld", "mode": "0755", "owner": "slurm", "path": "/var/lib/slurm/slurmctld", "size": 4096, "state": "directory", "uid": 498}
ok: [worker003] => (item=/var/lib/slurm/slurmctld) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/lib/slurm/slurmctld", "mode": "0755", "owner": "slurm", "path": "/var/lib/slurm/slurmctld", "size": 4096, "state": "directory", "uid": 498}
ok: [worker002] => (item=/var/lib/slurm/slurmctld) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/lib/slurm/slurmctld", "mode": "0755", "owner": "slurm", "path": "/var/lib/slurm/slurmctld", "size": 4096, "state": "directory", "uid": 498}
ok: [worker001] => (item=/var/lib/slurm/slurmd) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/lib/slurm/slurmd", "mode": "0755", "owner": "slurm", "path": "/var/lib/slurm/slurmd", "size": 4096, "state": "directory", "uid": 498}
ok: [worker002] => (item=/var/lib/slurm/slurmd) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/lib/slurm/slurmd", "mode": "0755", "owner": "slurm", "path": "/var/lib/slurm/slurmd", "size": 4096, "state": "directory", "uid": 498}
ok: [worker003] => (item=/var/lib/slurm/slurmd) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/lib/slurm/slurmd", "mode": "0755", "owner": "slurm", "path": "/var/lib/slurm/slurmd", "size": 4096, "state": "directory", "uid": 498}
ok: [worker001] => (item=/var/log/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/log/slurm", "mode": "0755", "owner": "slurm", "path": "/var/log/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [worker003] => (item=/var/log/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/log/slurm", "mode": "0755", "owner": "slurm", "path": "/var/log/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [worker002] => (item=/var/log/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/log/slurm", "mode": "0755", "owner": "slurm", "path": "/var/log/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [worker001] => (item=/var/run/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/run/slurm", "mode": "0755", "owner": "slurm", "path": "/var/run/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [worker003] => (item=/var/run/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/run/slurm", "mode": "0755", "owner": "slurm", "path": "/var/run/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [worker002] => (item=/var/run/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/run/slurm", "mode": "0755", "owner": "slurm", "path": "/var/run/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [worker001] => (item=/var/spool/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/spool/slurm", "mode": "0755", "owner": "slurm", "path": "/var/spool/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [worker003] => (item=/var/spool/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/spool/slurm", "mode": "0755", "owner": "slurm", "path": "/var/spool/slurm", "size": 4096, "state": "directory", "uid": 498}
ok: [worker002] => (item=/var/spool/slurm) => {"changed": false, "gid": 498, "group": "slurm", "item": "/var/spool/slurm", "mode": "0755", "owner": "slurm", "path": "/var/spool/slurm", "size": 4096, "state": "directory", "uid": 498}

TASK [slurm-common : Make compatibility symlinks] ******************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:56
skipping: [worker001] => (item={u'to': u'/etc/slurm-llnl', u'from': u'/etc/slurm'})  => {"changed": false, "item": {"from": "/etc/slurm", "to": "/etc/slurm-llnl"}, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => (item={u'to': u'/var/lib/slurm-llnl', u'from': u'/var/lib/slurm'})  => {"changed": false, "item": {"from": "/var/lib/slurm", "to": "/var/lib/slurm-llnl"}, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item={u'to': u'/etc/slurm-llnl', u'from': u'/etc/slurm'})  => {"changed": false, "item": {"from": "/etc/slurm", "to": "/etc/slurm-llnl"}, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => (item={u'to': u'/var/log/slurm-llnl', u'from': u'/var/log/slurm'})  => {"changed": false, "item": {"from": "/var/log/slurm", "to": "/var/log/slurm-llnl"}, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item={u'to': u'/var/lib/slurm-llnl', u'from': u'/var/lib/slurm'})  => {"changed": false, "item": {"from": "/var/lib/slurm", "to": "/var/lib/slurm-llnl"}, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => (item={u'to': u'/var/run/slurm-llnl', u'from': u'/var/run/slurm'})  => {"changed": false, "item": {"from": "/var/run/slurm", "to": "/var/run/slurm-llnl"}, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item={u'to': u'/var/log/slurm-llnl', u'from': u'/var/log/slurm'})  => {"changed": false, "item": {"from": "/var/log/slurm", "to": "/var/log/slurm-llnl"}, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => (item={u'to': u'/etc/slurm-llnl', u'from': u'/etc/slurm'})  => {"changed": false, "item": {"from": "/etc/slurm", "to": "/etc/slurm-llnl"}, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => (item={u'to': u'/var/lib/slurm-llnl', u'from': u'/var/lib/slurm'})  => {"changed": false, "item": {"from": "/var/lib/slurm", "to": "/var/lib/slurm-llnl"}, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item={u'to': u'/var/run/slurm-llnl', u'from': u'/var/run/slurm'})  => {"changed": false, "item": {"from": "/var/run/slurm", "to": "/var/run/slurm-llnl"}, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => (item={u'to': u'/var/log/slurm-llnl', u'from': u'/var/log/slurm'})  => {"changed": false, "item": {"from": "/var/log/slurm", "to": "/var/log/slurm-llnl"}, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => (item={u'to': u'/var/run/slurm-llnl', u'from': u'/var/run/slurm'})  => {"changed": false, "item": {"from": "/var/run/slurm", "to": "/var/run/slurm-llnl"}, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-common : Enable Copr SLURM repo by verdurin] ***********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:73
ok: [worker002] => {"changed": false, "gid": 0, "group": "root", "mode": "0444", "owner": "root", "path": "/etc/yum.repos.d/copr-slurm.repo", "size": 615, "state": "file", "uid": 0}
ok: [worker003] => {"changed": false, "gid": 0, "group": "root", "mode": "0444", "owner": "root", "path": "/etc/yum.repos.d/copr-slurm.repo", "size": 615, "state": "file", "uid": 0}
ok: [worker001] => {"changed": false, "gid": 0, "group": "root", "mode": "0444", "owner": "root", "path": "/etc/yum.repos.d/copr-slurm.repo", "size": 615, "state": "file", "uid": 0}

TASK [slurm-common : Prevent MUNGE from starting] ******************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:20
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-common : Install MUNGE (Debian/Ubuntu)] ****************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:31
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-common : Install MUNGE (RHEL-compatible)] **************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:43
ok: [worker002] => {"changed": false, "msg": "", "rc": 0, "results": ["slurm-munge-17.02.7-1.el6.x86_64 providing slurm-munge is already installed"]}
ok: [worker001] => {"changed": false, "msg": "", "rc": 0, "results": ["slurm-munge-17.02.7-1.el6.x86_64 providing slurm-munge is already installed"]}
ok: [worker003] => {"changed": false, "msg": "", "rc": 0, "results": ["slurm-munge-17.02.7-1.el6.x86_64 providing slurm-munge is already installed"]}

TASK [slurm-common : Allow MUNGE to start] *************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:57
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-common : Add `--syslog` option to `munged` startup] ****************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:66
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-common : Reload systemd configuration] *****************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:77
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-common : Add `--syslog` startup options for `munged` (Ubuntu 14.04)] ***
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:86
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-common : Configure MUNGE] ******************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:101
ok: [worker001] => {"changed": false, "checksum": "222afe887f907039b799b394715664ffa584fec5", "dest": "/etc/munge/munge.key", "gid": 497, "group": "munge", "mode": "0400", "owner": "munge", "path": "/etc/munge/munge.key", "size": 1024, "state": "file", "uid": 497}
ok: [worker003] => {"changed": false, "checksum": "222afe887f907039b799b394715664ffa584fec5", "dest": "/etc/munge/munge.key", "gid": 497, "group": "munge", "mode": "0400", "owner": "munge", "path": "/etc/munge/munge.key", "size": 1024, "state": "file", "uid": 497}
ok: [worker002] => {"changed": false, "checksum": "222afe887f907039b799b394715664ffa584fec5", "dest": "/etc/munge/munge.key", "gid": 497, "group": "munge", "mode": "0400", "owner": "munge", "path": "/etc/munge/munge.key", "size": 1024, "state": "file", "uid": 497}

TASK [slurm-common : Ensure the MUNGE service is running] **********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/munge.yml:114
ok: [worker002] => {"changed": false, "enabled": true, "name": "munge", "state": "started"}
ok: [worker003] => {"changed": false, "enabled": true, "name": "munge", "state": "started"}
ok: [worker001] => {"changed": false, "enabled": true, "name": "munge", "state": "started"}

TASK [slurm-common : Deploy SLURM configuration file] **************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:86
ok: [worker002] => {"changed": false, "gid": 0, "group": "root", "mode": "0444", "owner": "root", "path": "/etc/slurm/slurm.conf", "size": 4244, "state": "file", "uid": 0}
ok: [worker003] => {"changed": false, "gid": 0, "group": "root", "mode": "0444", "owner": "root", "path": "/etc/slurm/slurm.conf", "size": 4244, "state": "file", "uid": 0}
ok: [worker001] => {"changed": false, "gid": 0, "group": "root", "mode": "0444", "owner": "root", "path": "/etc/slurm/slurm.conf", "size": 4244, "state": "file", "uid": 0}

TASK [slurm-common : Deploy script to suspend cloud instance] ******************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:96
ok: [worker002] => {"changed": false, "gid": 0, "group": "root", "mode": "0555", "owner": "root", "path": "/etc/slurm/slurm.suspend.sh", "size": 604, "state": "file", "uid": 0}
ok: [worker001] => {"changed": false, "gid": 0, "group": "root", "mode": "0555", "owner": "root", "path": "/etc/slurm/slurm.suspend.sh", "size": 604, "state": "file", "uid": 0}
ok: [worker003] => {"changed": false, "gid": 0, "group": "root", "mode": "0555", "owner": "root", "path": "/etc/slurm/slurm.suspend.sh", "size": 604, "state": "file", "uid": 0}

TASK [slurm-common : Deploy script to resume cloud instance] *******************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:106
ok: [worker001] => {"changed": false, "gid": 0, "group": "root", "mode": "0555", "owner": "root", "path": "/etc/slurm/slurm.resume.sh", "size": 631, "state": "file", "uid": 0}
ok: [worker003] => {"changed": false, "gid": 0, "group": "root", "mode": "0555", "owner": "root", "path": "/etc/slurm/slurm.resume.sh", "size": 631, "state": "file", "uid": 0}
ok: [worker002] => {"changed": false, "gid": 0, "group": "root", "mode": "0555", "owner": "root", "path": "/etc/slurm/slurm.resume.sh", "size": 631, "state": "file", "uid": 0}

TASK [slurm-common : Install support packages (Debian/Ubuntu)] *****************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:116
skipping: [worker001] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-common : Install SLURM packages (RHEL-compatible)] *****************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-common/tasks/main.yml:127
skipping: [worker001] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-client : Install required SLURM packages (Debian/Ubuntu)] **********
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-client/tasks/main.yml:4
skipping: [worker001] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-client : Install required SLURM packages (older Debian/Ubuntu)] ****
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-client/tasks/main.yml:23
skipping: [worker001] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => (item=[])  => {"changed": false, "item": [], "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-client : service] **************************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-client/tasks/main.yml:35
skipping: [worker001] => (item=slurm-llnl)  => {"changed": false, "item": "slurm-llnl", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item=slurm-llnl)  => {"changed": false, "item": "slurm-llnl", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => (item=slurm-llnl)  => {"changed": false, "item": "slurm-llnl", "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-client : Install SLURM packages (RHEL-compatible)] *****************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-client/tasks/main.yml:47
ok: [worker001] => (item=[u'slurm', u'slurm-devel', u'slurm-perlapi', u'slurm-contribs', u'slurm-torque']) => {"changed": false, "item": ["slurm", "slurm-devel", "slurm-perlapi", "slurm-contribs", "slurm-torque"], "msg": "", "rc": 0, "results": ["slurm-17.02.7-1.el6.x86_64 providing slurm is already installed", "slurm-devel-17.02.7-1.el6.x86_64 providing slurm-devel is already installed", "slurm-perlapi-17.02.7-1.el6.x86_64 providing slurm-perlapi is already installed", "slurm-contribs-17.02.7-1.el6.x86_64 providing slurm-contribs is already installed", "slurm-torque-17.02.7-1.el6.x86_64 providing slurm-torque is already installed"]}
ok: [worker003] => (item=[u'slurm', u'slurm-devel', u'slurm-perlapi', u'slurm-contribs', u'slurm-torque']) => {"changed": false, "item": ["slurm", "slurm-devel", "slurm-perlapi", "slurm-contribs", "slurm-torque"], "msg": "", "rc": 0, "results": ["slurm-17.02.7-1.el6.x86_64 providing slurm is already installed", "slurm-devel-17.02.7-1.el6.x86_64 providing slurm-devel is already installed", "slurm-perlapi-17.02.7-1.el6.x86_64 providing slurm-perlapi is already installed", "slurm-contribs-17.02.7-1.el6.x86_64 providing slurm-contribs is already installed", "slurm-torque-17.02.7-1.el6.x86_64 providing slurm-torque is already installed"]}
ok: [worker002] => (item=[u'slurm', u'slurm-devel', u'slurm-perlapi', u'slurm-contribs', u'slurm-torque']) => {"changed": false, "item": ["slurm", "slurm-devel", "slurm-perlapi", "slurm-contribs", "slurm-torque"], "msg": "", "rc": 0, "results": ["slurm-17.02.7-1.el6.x86_64 providing slurm is already installed", "slurm-devel-17.02.7-1.el6.x86_64 providing slurm-devel is already installed", "slurm-perlapi-17.02.7-1.el6.x86_64 providing slurm-perlapi is already installed", "slurm-contribs-17.02.7-1.el6.x86_64 providing slurm-contribs is already installed", "slurm-torque-17.02.7-1.el6.x86_64 providing slurm-torque is already installed"]}

TASK [slurm-client : Stop SLURM services (RHEL/CentOS 6.x)] ********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-client/tasks/main.yml:59
changed: [worker001] => {"changed": true, "enabled": false, "name": "slurm", "state": "stopped"}
changed: [worker002] => {"changed": true, "enabled": false, "name": "slurm", "state": "stopped"}
changed: [worker003] => {"changed": true, "enabled": false, "name": "slurm", "state": "stopped"}

TASK [slurm-client : Stop SLURM services (RHEL/CentOS 7.x)] ********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-client/tasks/main.yml:66
skipping: [worker001] => (item=slurmd)  => {"changed": false, "item": "slurmd", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => (item=slurmctld)  => {"changed": false, "item": "slurmctld", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item=slurmd)  => {"changed": false, "item": "slurmd", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item=slurmctld)  => {"changed": false, "item": "slurmctld", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => (item=slurmd)  => {"changed": false, "item": "slurmd", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => (item=slurmctld)  => {"changed": false, "item": "slurmctld", "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-worker : Load distribution-specific parameters] ********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/main.yml:4
included: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/init-RedHat.yml for worker001, worker002, worker003

TASK [slurm-worker : Set SLURM worker playbook params (RHEL compatible)] *******
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/init-RedHat.yml:3
ok: [worker001] => {"ansible_facts": {"slurmd_packages": ["slurm", "slurm-plugins", "slurm-sql"]}, "changed": false}
ok: [worker002] => {"ansible_facts": {"slurmd_packages": ["slurm", "slurm-plugins", "slurm-sql"]}, "changed": false}
ok: [worker003] => {"ansible_facts": {"slurmd_packages": ["slurm", "slurm-plugins", "slurm-sql"]}, "changed": false}

TASK [slurm-worker : Set SLURM worker service name (RHEL 7.x compatible)] ******
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/init-RedHat.yml:11
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-worker : Set SLURM worker service name (RHEL 6.x compatible)] ******
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/init-RedHat.yml:16
ok: [worker001] => {"ansible_facts": {"slurmd_service": "slurm"}, "changed": false}
ok: [worker002] => {"ansible_facts": {"slurmd_service": "slurm"}, "changed": false}
ok: [worker003] => {"ansible_facts": {"slurmd_service": "slurm"}, "changed": false}

TASK [slurm-worker : Deploy SLURM configuration files] *************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/main.yml:7
ok: [worker002] => (item=gres.conf) => {"changed": false, "gid": 0, "group": "root", "item": "gres.conf", "mode": "0444", "owner": "root", "path": "/etc/slurm/gres.conf", "size": 297, "state": "file", "uid": 0}
ok: [worker001] => (item=gres.conf) => {"changed": false, "gid": 0, "group": "root", "item": "gres.conf", "mode": "0444", "owner": "root", "path": "/etc/slurm/gres.conf", "size": 297, "state": "file", "uid": 0}
ok: [worker003] => (item=gres.conf) => {"changed": false, "gid": 0, "group": "root", "item": "gres.conf", "mode": "0444", "owner": "root", "path": "/etc/slurm/gres.conf", "size": 297, "state": "file", "uid": 0}

TASK [slurm-worker : Deploy kernel config check script] ************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/cgroup.yml:3
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-worker : Check that kernel is configured for cgroups] **************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/cgroup.yml:12
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-worker : Ensure cgroup filesystems are mounted] ********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/cgroup.yml:18
skipping: [worker001] => (item=blkio)  => {"changed": false, "item": "blkio", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => (item=cpuacct)  => {"changed": false, "item": "cpuacct", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item=blkio)  => {"changed": false, "item": "blkio", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => (item=cpuset)  => {"changed": false, "item": "cpuset", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item=cpuacct)  => {"changed": false, "item": "cpuacct", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => (item=blkio)  => {"changed": false, "item": "blkio", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => (item=devices)  => {"changed": false, "item": "devices", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item=cpuset)  => {"changed": false, "item": "cpuset", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => (item=cpuacct)  => {"changed": false, "item": "cpuacct", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => (item=freezer)  => {"changed": false, "item": "freezer", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item=devices)  => {"changed": false, "item": "devices", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => (item=cpuset)  => {"changed": false, "item": "cpuset", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => (item=memory)  => {"changed": false, "item": "memory", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item=freezer)  => {"changed": false, "item": "freezer", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => (item=devices)  => {"changed": false, "item": "devices", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item=memory)  => {"changed": false, "item": "memory", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => (item=freezer)  => {"changed": false, "item": "freezer", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => (item=memory)  => {"changed": false, "item": "memory", "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-worker : Ensure kernel is booted with swap accounting enabled] *****
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/cgroup.yml:44
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-worker : Check if swap accouting is enabled (may fail!)] ***********
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/cgroup.yml:53
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-worker : Reboot to enable swap accounting] *************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/cgroup.yml:59
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-worker : Wait for server to come up again] *************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/cgroup.yml:72
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-worker : Ensure release agent directory exists] ********************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/cgroup.yml:84
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-worker : Deploy cgroup-specific release agent script] **************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/cgroup.yml:93
skipping: [worker001] => (item=blkio)  => {"changed": false, "item": "blkio", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => (item=cpuacct)  => {"changed": false, "item": "cpuacct", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item=blkio)  => {"changed": false, "item": "blkio", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => (item=cpuset)  => {"changed": false, "item": "cpuset", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item=cpuacct)  => {"changed": false, "item": "cpuacct", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => (item=blkio)  => {"changed": false, "item": "blkio", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => (item=devices)  => {"changed": false, "item": "devices", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item=cpuset)  => {"changed": false, "item": "cpuset", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => (item=cpuacct)  => {"changed": false, "item": "cpuacct", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => (item=freezer)  => {"changed": false, "item": "freezer", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item=devices)  => {"changed": false, "item": "devices", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => (item=cpuset)  => {"changed": false, "item": "cpuset", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item=freezer)  => {"changed": false, "item": "freezer", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker001] => (item=memory)  => {"changed": false, "item": "memory", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => (item=devices)  => {"changed": false, "item": "devices", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => (item=memory)  => {"changed": false, "item": "memory", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => (item=freezer)  => {"changed": false, "item": "freezer", "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => (item=memory)  => {"changed": false, "item": "memory", "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-worker : Deploy SLURM cgroup configuration file (main)] ************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/cgroup.yml:103
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-worker : Deploy SLURM cgroup configuration file (devices)] *********
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/cgroup.yml:112
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [slurm-worker : Install SLURM worker packages] ****************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/main.yml:27
ok: [worker002] => (item=[u'slurm', u'slurm-plugins', u'slurm-sql']) => {"changed": false, "item": ["slurm", "slurm-plugins", "slurm-sql"], "msg": "", "rc": 0, "results": ["slurm-17.02.7-1.el6.x86_64 providing slurm is already installed", "slurm-plugins-17.02.7-1.el6.x86_64 providing slurm-plugins is already installed", "slurm-sql-17.02.7-1.el6.x86_64 providing slurm-sql is already installed"]}
ok: [worker003] => (item=[u'slurm', u'slurm-plugins', u'slurm-sql']) => {"changed": false, "item": ["slurm", "slurm-plugins", "slurm-sql"], "msg": "", "rc": 0, "results": ["slurm-17.02.7-1.el6.x86_64 providing slurm is already installed", "slurm-plugins-17.02.7-1.el6.x86_64 providing slurm-plugins is already installed", "slurm-sql-17.02.7-1.el6.x86_64 providing slurm-sql is already installed"]}
ok: [worker001] => (item=[u'slurm', u'slurm-plugins', u'slurm-sql']) => {"changed": false, "item": ["slurm", "slurm-plugins", "slurm-sql"], "msg": "", "rc": 0, "results": ["slurm-17.02.7-1.el6.x86_64 providing slurm is already installed", "slurm-plugins-17.02.7-1.el6.x86_64 providing slurm-plugins is already installed", "slurm-sql-17.02.7-1.el6.x86_64 providing slurm-sql is already installed"]}

TASK [slurm-worker : Ensure SLURMd starts at boot] *****************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm-worker/tasks/main.yml:33
changed: [worker001] => {"changed": true, "enabled": true, "name": "slurm"}
changed: [worker002] => {"changed": true, "enabled": true, "name": "slurm"}
changed: [worker003] => {"changed": true, "enabled": true, "name": "slurm"}

PLAY [Restart SLURMd after all config is done] *********************************

TASK [setup] *******************************************************************
ok: [worker001]
ok: [worker002]
ok: [worker003]

TASK [service] *****************************************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm.yml:47
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [service] *****************************************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm.yml:51
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [service] *****************************************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm.yml:55
skipping: [worker001] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker002] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}
skipping: [worker003] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true}

TASK [service] *****************************************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/roles/slurm.yml:59
changed: [worker001] => {"changed": true, "name": "slurm", "state": "started"}
changed: [worker003] => {"changed": true, "name": "slurm", "state": "started"}
changed: [worker002] => {"changed": true, "name": "slurm", "state": "started"}

PLAY [Apply local customizations (after)] **************************************

TASK [setup] *******************************************************************
ok: [master001]
ok: [worker001]
ok: [worker003]
ok: [worker002]

PLAY [Report success on cluster creation] **************************************

TASK [Mark host as successfully configured] ************************************
task path: /root/.cache/Python-Eggs/elasticluster-1.3.dev1-py2.7.egg-tmp/elasticluster/share/playbooks/site.yml:79
changed: [master001 -> localhost] => {"backup": "", "changed": true, "msg": "line added"}
changed: [worker001 -> localhost] => {"backup": "", "changed": true, "msg": "line added"}
changed: [worker002 -> localhost] => {"backup": "", "changed": true, "msg": "line added"}
changed: [worker003 -> localhost] => {"backup": "", "changed": true, "msg": "line added"}

PLAY RECAP *********************************************************************
master001                  : ok=130  changed=34   unreachable=0    failed=0   
worker001                  : ok=122  changed=25   unreachable=0    failed=0   
worker002                  : ok=122  changed=25   unreachable=0    failed=0   
worker003                  : ok=123  changed=41   unreachable=0    failed=0   

2018-05-02 12:17:57 hwc hwc[18431] INFO Cluster correctly configured.
Adding 1 worker node(s) to the cluster
Reconfiguring the cluster.

Cluster name:     slurm
Cluster template: slurm
Default ssh to node: master001
- worker nodes: 3
- master nodes: 1

To login on the frontend node, run the command:

    elasticluster ssh slurm

To upload or download files to the cluster, use the command:

    elasticluster sftp slurm

